{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_knrNHXGl91k"
      },
      "source": [
        "### Experiment 5: Experiment with 3 depth stacks vs 2D\n",
        "\n",
        "The goal of this experiment is to investigate whether the model would able to learn transformation given information of following and preceding depths.\n",
        "\n",
        "To decide: how will output be determined, whether it will be a 3D or 1D image (of middle depth)\n",
        "\n",
        "**Methods**: generate 3D patches of 3 consecutive depths.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCz6M7fo1Bdm",
        "outputId": "2928a746-6dd3-44a4-a1c6-c211583cc032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (1.0.18)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import_ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import_ipynb) (0.2.5)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.11.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (2.16.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.9.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import_ipynb) (3.8.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install tifffile\n",
        "# !pip install sklearn\n",
        "# !pip install scikit-image\n",
        "!pip install import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gL_e5tv648r"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMUT7spjl35_"
      },
      "outputs": [],
      "source": [
        "# importing dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "import skimage\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import keras.backend as K\n",
        "from datetime import datetime\n",
        "import fractions\n",
        "import itertools\n",
        "import tqdm\n",
        "from keras.utils.conv_utils import normalize_tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-8XBBzanaNG",
        "outputId": "c9ecb6a9-68dc-48ae-8a15-721e5c775330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok5TnqXZU4_0"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH = '/cluster/tufts/georgakoudi_lab01/tdinh02/npz/Cervix_all_data_original.npz'\n",
        "# LABEL_PATH = '/cluster/tufts/georgakoudi_lab01/tdinh02/npz/Cervix_all_data_labels_original.npz'\\\n",
        "# MAIN_PATH = '/cluster/tufts/georgakoudi_lab01/tdinh02/objective_transform/'\n",
        "\n",
        "DATA_PATH = '/npz/Cervix_all_data_3depths_3D.npz'\n",
        "LABEL_PATH = '/npz/Cervix_all_data_labels_3depths_3D.npz'\n",
        "MAIN_PATH= r\"/objective_transfer/deep_learning\" # Artem's Drive\n",
        "now = datetime.now() # current date and time\n",
        "CURR_DATE = now.strftime(\"%m-%d-%Y\")\n",
        "\n",
        "def config(patch_size, depth):\n",
        "  return {\n",
        "        'img_size': 512,\n",
        "        'learning_rate': 1e-5,\n",
        "        'batch_size': 16,\n",
        "        'alpha': 0.84,\n",
        "        'patch_size':patch_size,\n",
        "        'input_shape': [patch_size, patch_size,3],\n",
        "        'kern_size':3,\n",
        "        'n_depth': depth,\n",
        "        'first_depth': 32,\n",
        "        'dropout': 0,\n",
        "        'epoch':1,\n",
        "        'lr_decay_factor':0.97,\n",
        "        'lr_decay_patience':10,\n",
        "  }\n",
        "\n",
        "MODEL_ROOT_NAME = \"CARE_patch_depth_tune_0726\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWOPU7Kl6QC7",
        "outputId": "d1cee229-222e-475d-ec45-0ba4c035930c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from CARE_util.ipynb\n"
          ]
        }
      ],
      "source": [
        "# importing yaml config file and resetting working direcgory\n",
        "os.chdir(MAIN_PATH+'/CAREstd/')\n",
        "import import_ipynb\n",
        "from CARE_util import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW-FYFCH579U"
      },
      "outputs": [],
      "source": [
        "# def ssim(y_true, y_pred):\n",
        "#     return (tf.image.ssim(y_true, y_pred,1,k2=0.05)) # sliding Gaussian window as mentioned in Wikipedia\n",
        "# smaller filter size, should avoid blurring\n",
        "def ssim(y_true, y_pred):\n",
        "    return tf.image.ssim(y_true, y_pred, 1, filter_size=3, filter_sigma=0.5, k2=0.05)\n",
        "\n",
        "w = (0.0448, 0.2856, 0.3001, 0.2363, 0.1333)\n",
        "def mssim(y_true, y_pred):\n",
        "   return tf.image.ssim_multiscale(y_true, y_pred, 1, filter_size=11,power_factors=w, filter_sigma=1.5, k2=0.05)\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    '''\n",
        "    Computs the peak signal-to-noise ratio between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    '''\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def SSIM_loss(y_true, y_pred):\n",
        "    return 1-((ssim(y_true, y_pred)+1)*0.5) # chanfe to 2\n",
        "\n",
        "from scipy import signal\n",
        "def cw_ssim(y_true, y_pred, width=30):\n",
        "        \"\"\"Compute the complex wavelet SSIM (CW-SSIM) value from the reference\n",
        "        image to the target image.\n",
        "        Args:\n",
        "          target (str or PIL.Image): Input image to compare the reference image\n",
        "          to. This may be a PIL Image object or, to save time, an SSIMImage\n",
        "          object (e.g. the img member of another SSIM object).\n",
        "          width: width for the wavelet convolution (default: 30)\n",
        "        Returns:\n",
        "          Computed CW-SSIM float value.\n",
        "        \"\"\"\n",
        "        k = 0.01 #k (float): CW-SSIM configuration variable (default 0.01)\n",
        "        # Define a width for the wavelet convolution\n",
        "        widths = np.arange(1, width+1)\n",
        "\n",
        "        # Use the image data as arrays\n",
        "        sig1 = y_pred.numpy().flatten()\n",
        "        sig2 = y_true.numpy().flatten()\n",
        "\n",
        "        # Convolution\n",
        "        cwtmatr1 = signal.cwt(sig1, signal.ricker, widths)\n",
        "        cwtmatr2 = signal.cwt(sig2, signal.ricker, widths)\n",
        "\n",
        "        # Compute the first term\n",
        "        c1c2 = np.multiply(abs(cwtmatr1), abs(cwtmatr2))\n",
        "        c1_2 = np.square(abs(cwtmatr1))\n",
        "        c2_2 = np.square(abs(cwtmatr2))\n",
        "        num_ssim_1 = 2 * np.sum(c1c2, axis=0) + k\n",
        "        den_ssim_1 = np.sum(c1_2, axis=0) + np.sum(c2_2, axis=0) + k\n",
        "\n",
        "        # Compute the second term\n",
        "        c1c2_conj = np.multiply(cwtmatr1, np.conjugate(cwtmatr2))\n",
        "        num_ssim_2 = 2 * np.abs(np.sum(c1c2_conj, axis=0)) + k\n",
        "        den_ssim_2 = 2 * np.sum(np.abs(c1c2_conj), axis=0) + k\n",
        "\n",
        "        # Construct the result\n",
        "        ssim_map = (num_ssim_1 / den_ssim_1) * (num_ssim_2 / den_ssim_2)\n",
        "\n",
        "        # Average the per pixel results\n",
        "        index = np.average(ssim_map)\n",
        "        return index\n",
        "\n",
        "def SSIML1_loss(y_true, y_pred, alpha=0.84):\n",
        "  # alpha = 0.84\n",
        "  ssim_partial = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "  mae_partial = tf.keras.losses.mae(\n",
        "        *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "  return alpha*ssim_partial  + (1-alpha)*mae_partial\n",
        "\n",
        "def mov_var(image):\n",
        "  dtype = tf.float32\n",
        "  img_height = tf.shape(image)[1]\n",
        "  img_width = tf.shape(image)[2]\n",
        "  mean_filter = tf.ones((3,3),dtype) / 9\n",
        "  img_mean = tf.nn.conv2d(image[:,:,:,:],\n",
        "                          mean_filter[:,:,tf.newaxis,tf.newaxis],\n",
        "                          [1,1,1,1],'VALID')\n",
        "  img_clip = image[:, 1:-1, 1:-1,:]\n",
        "  # Difference between pixel intensity and its block mean\n",
        "  x_diff = tf.math.squared_difference(img_clip, img_mean) / 8\n",
        "  return x_diff\n",
        "\n",
        "def genSSIML1_loss(alpha=0.84):\n",
        "  def SSIM_L1_loss(y_true, y_pred):\n",
        "    ssim_partial = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "    mae_partial = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "    return alpha*ssim_partial  + (1-alpha)*mae_partial\n",
        "  return SSIM_L1_loss\n",
        "\n",
        "def genSSIMVar_loss(alpha=0.84):\n",
        "  def SSIMVar_loss(y_true, y_pred):\n",
        "      SSIM = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "      MAE = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [mov_var(y_true), mov_var(y_pred)]])\n",
        "      return alpha * SSIM + (1-alpha) * MAE\n",
        "  return SSIMVar_loss\n",
        "\n",
        "def genSSIMVarL1_loss(alpha=0.84):\n",
        "  def SSIMVarL1_loss(y_true, y_pred):\n",
        "      SSIM = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "      MAE = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [mov_var(y_true), mov_var(y_pred)]])\n",
        "      MAE2 = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "      return alpha * SSIM + ((1-alpha)/4) * MAE + (3*(1-alpha)/4) * MAE2\n",
        "  return SSIMVarL1_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U23b4LDwl38K"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def create_patches(img, patch_shape, slide, depth=3):\n",
        "    # returns stack of patches and number of patches\n",
        "    patch_img = skimage.util.view_as_windows(img, (patch_shape,patch_shape,depth), step=patch_shape-slide)\n",
        "    patch = patch_img.reshape(patch_img.shape[0]*patch_img.shape[1],patch_shape,patch_shape,depth) # more time efficient\n",
        "    return patch\n",
        "\n",
        "def patchify(input, patch_shape, slide, depth=3):\n",
        "    # getting number of input images\n",
        "    len_to_allocate = int(np.shape(input)[0]*((slide-np.shape(input)[1]) / (slide-patch_shape))**2)\n",
        "    data = np.zeros((len_to_allocate,patch_shape,patch_shape,depth))\n",
        "    count = 0\n",
        "    for i in range(np.shape(input)[0]):\n",
        "      A = create_patches(input[i], patch_shape, slide)\n",
        "      # print(\"[:,:,:]A\", np.amax(A[:,:,:]))\n",
        "      data[count:count+len(A),:,:] = A[:,:,:]\n",
        "      # print(\"data[count:count+len(A),:,:]\", np.amax(data[count:count+len(A),:,:]))\n",
        "      count = count + len(A)\n",
        "    print(\"      [PATCHIFYING COMPLETED] output shape, slide: \",np.shape(data),slide,\"; number of images: \", np.shape(input)[0], \", number of patches: \", np.shape(data)[0])\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_data(path, expand=False, patch=None):\n",
        "    # Loading preprocessed image patches and adding 4th arbitrary dimension\n",
        "    b = np.load(path)\n",
        "    training_data = b['t']\n",
        "    val_data = b['v']\n",
        "\n",
        "    # patchify if patch size is passed\n",
        "    if patch != None:\n",
        "      slide = int(patch/2)\n",
        "      # slide = 0\n",
        "      training_data = patchify(training_data, patch, slide)\n",
        "      val_data = patchify(val_data, patch, slide)\n",
        "\n",
        "    # if len(np.shape(training_data))>3:\n",
        "    #   # for 3D only, changing dimensions\n",
        "    #   training_data = np.transpose(training_data, (0, 3, 1, 2,4))\n",
        "    #   val_data = np.transpose(val_data, (0, 3, 1, 2,4))\n",
        "    if expand == True:\n",
        "      training_data = expandLastDim(training_data)\n",
        "      val_data = expandLastDim(val_data)\n",
        "    res = [training_data, val_data]\n",
        "    return res\n",
        "\n",
        "def expandLastDim(data):\n",
        "    return np.expand_dims(data, -1)\n",
        "\n",
        "# add arbitrary channel dimension and augments data on flow, also splits data into batches\n",
        "class DataGenerator:\n",
        "    '''\n",
        "    Generates batches of image pairs with real-time data augmentation.\n",
        "    Parameters\n",
        "    ----------\n",
        "    shape: tuple of int\n",
        "        Shape of batch images (excluding the channel dimension).\n",
        "    batch_size: int\n",
        "        Batch size.\n",
        "    transform_function: str or callable or None\n",
        "        Function used for data augmentation. Typically you will set\n",
        "        ``transform_function='rotate_and_flip'`` to apply combination of\n",
        "        randomly selected image rotation and flipping.  Alternatively, you can\n",
        "        specify an arbitrary transformation function which takes two input\n",
        "        images (source and target) and returns transformed images. If\n",
        "        ``transform_function=None``, no augmentation will be performed.\n",
        "    intensity_threshold: float\n",
        "        If ``intensity_threshold > 0``, pixels whose intensities are greater\n",
        "        than this threshold will be considered as foreground.\n",
        "    area_ratio_threshold: float between 0 and 1\n",
        "        If ``intensity_threshold > 0``, the generator calculates the ratio of\n",
        "        foreground pixels in a target patch, and rejects the patch if the ratio\n",
        "        is smaller than this threshold.\n",
        "    scale_factor: int != 0\n",
        "        Scale factor for the target patch size. Positive and negative values\n",
        "        mean up- and down-scaling respectively.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 shape,\n",
        "                 batch_size,\n",
        "                 transform_function='rotate_and_flip',\n",
        "                 intensity_threshold=0.0,\n",
        "                 area_ratio_threshold=0.0,\n",
        "                 scale_factor=1):\n",
        "        def rotate_and_flip(x, y, dim):\n",
        "            if dim == 2:\n",
        "                k = np.random.randint(0, 4)\n",
        "                x, y = [np.rot90(v, k=k) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    x, y = [np.fliplr(v) for v in (x, y)]\n",
        "                return x, y\n",
        "            elif dim == 3:\n",
        "                k = np.random.randint(0, 4)\n",
        "                x, y = [np.rot90(v, k=k, axes=(1, 2)) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    x, y = [np.flip(v, axis=1) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    x, y = [np.flip(v, axis=0) for v in (x, y)]\n",
        "                return x, y\n",
        "            else:\n",
        "                raise ValueError('Unsupported dimension')\n",
        "\n",
        "        self._shape = tuple(shape)\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        dim = len(self._shape)\n",
        "\n",
        "        if transform_function == 'rotate_and_flip':\n",
        "            if shape[-2] != shape[-1]:\n",
        "                raise ValueError(\n",
        "                    'Patch shape must be square when using `rotate_and_flip`; '\n",
        "                    f'Received shape: {shape}')\n",
        "            self._transform_function = lambda x, y: rotate_and_flip(x, y, dim)\n",
        "        elif callable(transform_function):\n",
        "            self._transform_function = transform_function\n",
        "        elif transform_function is None:\n",
        "            self._transform_function = lambda x, y: (x, y)\n",
        "        else:\n",
        "            raise ValueError('Invalid transform function')\n",
        "\n",
        "        self._intensity_threshold = intensity_threshold\n",
        "\n",
        "        if not 0 <= area_ratio_threshold <= 1:\n",
        "            raise ValueError('\"area_ratio_threshold\" must be between 0 and 1')\n",
        "        self._area_threshold = area_ratio_threshold * np.prod(shape)\n",
        "\n",
        "        self._scale_factor = normalize_tuple(scale_factor, dim, 'scale_factor')\n",
        "        if any(not isinstance(f, int) or f == 0 for f in self._scale_factor):\n",
        "            raise ValueError('\"scale_factor\" must be nonzero integer')\n",
        "\n",
        "    class _Sequence(tf.keras.utils.Sequence):\n",
        "        def _scale(self, shape):\n",
        "            return tuple(\n",
        "                s * f if f > 0 else s // -f\n",
        "                for s, f in zip(shape, self._scale_factor))\n",
        "\n",
        "        def __init__(self,\n",
        "                     x,\n",
        "                     y,\n",
        "                     batch_size,\n",
        "                     shape,\n",
        "                     transform_function,\n",
        "                     intensity_threshold,\n",
        "                     area_threshold,\n",
        "                     scale_factor):\n",
        "            self._batch_size = batch_size\n",
        "            self._transform_function = transform_function\n",
        "            self._intensity_threshold = intensity_threshold\n",
        "            self._area_threshold = area_threshold\n",
        "            self._scale_factor = scale_factor\n",
        "\n",
        "            for s, f, in zip(shape, self._scale_factor):\n",
        "                if f < 0 and s % -f != 0:\n",
        "                    raise ValueError(\n",
        "                        'When downsampling, all elements in `shape` must be '\n",
        "                        'divisible by the scale factor; '\n",
        "                        f'Received shape: {shape}, '\n",
        "                        f'scale factor: {self._scale_factor}')\n",
        "\n",
        "            self._x, self._y = [\n",
        "                list(m) if isinstance(m, (list, tuple)) else [m]\n",
        "                for m in [x, y]]\n",
        "            self._x = np.moveaxis(self._x,0,-1)\n",
        "            self._y = np.moveaxis(self._y,0,-1)\n",
        "            if len(self._x) != len(self._y):\n",
        "                raise ValueError(\n",
        "                    'Different number of images are given: '\n",
        "                    f'{len(self._x)} vs. {len(self._y)}')\n",
        "\n",
        "            if len({m.dtype for m in self._x}) != 1:\n",
        "                raise ValueError('All source images must be the same type')\n",
        "            if len({m.dtype for m in self._y}) != 1:\n",
        "                raise ValueError('All target images must be the same type')\n",
        "            print(len(self._x))\n",
        "            for i in range(len(self._x)):\n",
        "                if len(self._x[i].shape) == len(shape):\n",
        "                    self._x[i] = self._x[i][..., np.newaxis]\n",
        "\n",
        "                if len(self._y[i].shape) == len(shape):\n",
        "                    self._y[i] = self._y[i][..., np.newaxis]\n",
        "\n",
        "                if len(self._x[i].shape) != len(shape) + 1:\n",
        "                    raise ValueError(f'Source image must be {len(shape)}D')\n",
        "\n",
        "                if len(self._y[i].shape) != len(shape) + 1:\n",
        "                    raise ValueError(f'Target image must be {len(shape)}D')\n",
        "                if self._x[i].shape[:-1] < shape:\n",
        "                    raise ValueError(\n",
        "                        'Source image must be larger than the patch size')\n",
        "\n",
        "                expected_y_image_size = self._scale(self._x[i].shape[:-1])\n",
        "                if self._y[i].shape[:-1] != expected_y_image_size:\n",
        "                    raise ValueError('Invalid target image size: '\n",
        "                                     f'expected {expected_y_image_size}, '\n",
        "                                     f'but received {self._y[i].shape[:-1]}')\n",
        "\n",
        "            if len({m.shape[-1] for m in self._x}) != 1:\n",
        "                raise ValueError(\n",
        "                    'All source images must have the same number of channels')\n",
        "            if len({m.shape[-1] for m in self._y}) != 1:\n",
        "                raise ValueError(\n",
        "                    'All target images must have the same number of channels')\n",
        "            self._batch_x = np.zeros(\n",
        "                (batch_size, *shape, self._x[0].shape[-1]),\n",
        "                dtype=self._x[0].dtype)\n",
        "            self._batch_y = np.zeros(\n",
        "                (batch_size, *self._scale(shape),self._y[0].shape[-1]),\n",
        "                dtype=self._y[0].dtype)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self._x) // self._batch_size # return a dummy value\n",
        "\n",
        "        def __next__(self):\n",
        "            return self.__getitem__(0)\n",
        "\n",
        "        def __getitem__(self, _):\n",
        "            for i in range(self._batch_size):\n",
        "                for _ in range(139):\n",
        "                    j = np.random.randint(0, len(self._x))\n",
        "\n",
        "                    tl = [np.random.randint(0, a - b + 1)\n",
        "                          for a, b in zip(\n",
        "                              self._x[j].shape, self._batch_x.shape[1:])]\n",
        "\n",
        "                    x = np.copy(self._x[j][tuple(\n",
        "                        [slice(a, a + b) for a, b in zip(\n",
        "                            tl, self._batch_x.shape[1:])])])\n",
        "                    y = np.copy(self._y[j][tuple(\n",
        "                        [slice(a, a + b) for a, b in zip(\n",
        "                            self._scale(tl), self._batch_y.shape[1:])])])\n",
        "\n",
        "                    if (self._intensity_threshold <= 0.0 or\n",
        "                            np.count_nonzero(y > self._intensity_threshold)\n",
        "                            >= self._area_threshold):\n",
        "                        break\n",
        "                else:\n",
        "                    import warnings\n",
        "                    warnings.warn(\n",
        "                        'Failed to sample a valid patch',\n",
        "                        RuntimeWarning,\n",
        "                        stacklevel=3)\n",
        "\n",
        "\n",
        "                self._batch_x[i], self._batch_y[i] = \\\n",
        "                    self._transform_function(x, y)\n",
        "            return self._batch_x, self._batch_y\n",
        "\n",
        "    def flow(self, x, y):\n",
        "        '''\n",
        "        Returns a `keras.utils.Sequence` object which generates batches\n",
        "        infinitely. It can be used as an input generator for\n",
        "        `keras.models.Model.fit_generator()`.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: array_like or list of array_like\n",
        "            Source image(s).\n",
        "        y: array_like or list of array_like\n",
        "            Target image(s).\n",
        "        Returns\n",
        "        -------\n",
        "        keras.utils.Sequence\n",
        "            `keras.utils.Sequence` object which generates tuples of source and\n",
        "            target image patches.\n",
        "        '''\n",
        "        return self._Sequence(x,\n",
        "                              y,\n",
        "                              self._batch_size,\n",
        "                              self._shape,\n",
        "                              self._transform_function,\n",
        "                              self._intensity_threshold,\n",
        "                              self._area_threshold,\n",
        "                              self._scale_factor)\n",
        "\n",
        "# CALLBACKS\n",
        "# tensorboard callback\n",
        "tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1,write_images=True,write_steps_per_second=True,)\n",
        "\n",
        "\n",
        "def build_model(n_dim=3, n_depth=2, kern_size=5, dropout= 0, n_first=32, n_channel_out=3, last_activation='relu', batch_norm=False,shape=(None,None,3), residual=True):\n",
        "    model = common_unet(n_dim,n_depth,n_first=n_first,kern_size=kern_size,n_channel_out=n_channel_out, dropout=dropout,last_activation=last_activation,batch_norm=batch_norm,residual=residual)(shape)\n",
        "    model.summary()\n",
        "    model.save_weights('model.h5')\n",
        "    return model\n",
        "\n",
        "# Compile model for training\n",
        "def compile_model(model, lr, loss=SSIML1_loss, metric=[{'psnr': psnr, 'ssim': ssim}]):\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr),\n",
        "                loss=loss,\n",
        "                # loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "                metrics = metric)\n",
        "                # metrics = [{'mae': tf.keras.losses.mae}])\n",
        "    return model\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "def save_weight_callback(checkpoint_path, batch_size, epoch_freq=10):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=epoch_freq*batch_size)\n",
        "\n",
        "def display_training_stats(history, savepath, stat, axis_name):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.plot(history.history[stat], label=stat)\n",
        "    plt.plot(history.history['val_'+stat], label = 'val_'+stat)\n",
        "    plt.xlabel(axis_name)\n",
        "    plt.ylabel(stat)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.savefig(savepath+'_'+stat+'.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB7XDna0zi33"
      },
      "outputs": [],
      "source": [
        "def _get_gaussian_kernel(dim, size, sigma):\n",
        "    import tensorflow_probability as tfp\n",
        "    k = size // 2\n",
        "    normal = tfp.distributions.Normal(0.0, sigma)\n",
        "    p = normal.prob(tf.range(-k, size - k, dtype=tf.float32))\n",
        "\n",
        "    indices = [chr(i) for i in range(105, 105 + dim)]\n",
        "    eq = ','.join(indices) + '->' + ''.join(indices)\n",
        "    kernel = tf.einsum(eq, *([p] * dim))\n",
        "    kernel /= tf.reduce_sum(kernel)\n",
        "    kernel = kernel[..., tf.newaxis, tf.newaxis]\n",
        "\n",
        "    return kernel\n",
        "\n",
        "# RCAN ssim\n",
        "import keras.backend as K\n",
        "def ssim_rcan(y_true, y_pred):\n",
        "    '''\n",
        "    Computes the structural similarity index between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    References\n",
        "    ----------\n",
        "    Image Quality Assessment: From Error Visibility to Structural Similarity\n",
        "    https://doi.org/10.1109/TIP.2003.819861\n",
        "    '''\n",
        "\n",
        "    c1 = 0.01 ** 2\n",
        "    c2 = 0.03 ** 2\n",
        "\n",
        "    dim = K.ndim(y_pred) - 2\n",
        "    if dim not in (2, 3):\n",
        "        raise NotImplementedError(f'{dim}D SSIM is not suported')\n",
        "\n",
        "    num_channels = K.int_shape(y_pred)[-1]\n",
        "\n",
        "    kernel = _get_gaussian_kernel(dim, 11, 1.5)\n",
        "    conv = K.conv2d if dim == 2 else K.conv3d\n",
        "\n",
        "    def average(x):\n",
        "        # channel-wise weighted average using the Gaussian kernel\n",
        "        return tf.concat(\n",
        "            [conv(y, kernel) for y in tf.split(x, num_channels, axis=-1)],\n",
        "            axis=-1)\n",
        "\n",
        "    ux = average(y_true)\n",
        "    uy = average(y_pred)\n",
        "\n",
        "    a = ux * uy\n",
        "    b = K.square(ux) + K.square(uy)\n",
        "    c = average(y_true * y_pred)\n",
        "    d = average(K.square(y_true) + K.square(y_pred))\n",
        "\n",
        "    lum = (2 * a + c1) / (b + c1)\n",
        "    cs = (2 * (c - a) + c2) / (d - b + c2)\n",
        "\n",
        "    return K.mean(K.batch_flatten(lum * cs), axis=-1)\n",
        "\n",
        "def revert_img(img,original_size, patch_shape, slide):\n",
        "  # reverts original image and removes overlaps by splitting overlap over 2 images\n",
        "  step = int(patch_shape-slide)\n",
        "  reconstructed_arr = np.zeros((original_size,original_size))\n",
        "  for x in range(img.shape[0]):\n",
        "    for y in range(img.shape[1]):\n",
        "      start_x = int(slide/2)\n",
        "      start_y = int(slide/2)\n",
        "      end_x = 0\n",
        "      end_y = 0\n",
        "      if x == 0:\n",
        "        start_x = 0\n",
        "        end_x = int(slide/2)\n",
        "      if y == 0:\n",
        "        start_y = 0\n",
        "        end_y = int(slide/2)\n",
        "      if x == img.shape[0]-1: end_x = int(slide/2)\n",
        "      if y == img.shape[1]-1: end_y = int(slide/2)\n",
        "      x_pos, y_pos = x * step + start_x, y * step + start_y\n",
        "      reconstructed_arr[x_pos : x_pos + step + end_x, y_pos : y_pos + step + end_y] = img[x, y, start_x:start_x+step+end_x, start_y:start_y+step+end_y]\n",
        "  return reconstructed_arr\n",
        "\n",
        "def merge_patches(img, original_size, patch_shape, slide):\n",
        "  #  merging patches, img is a 3D array of stacked patches\n",
        "  row_len = int(math.sqrt(img.shape[0]))\n",
        "  patches = np.zeros((row_len,row_len,patch_shape,patch_shape))\n",
        "  for r in range(row_len):\n",
        "      patches[r,:,:,:] = img[r*row_len:r*row_len+row_len,:,:]\n",
        "  return revert_img(patches,original_size,patch_shape, slide)\n",
        "\n",
        "def normalize0to1(img):\n",
        "  # Normalizing images between 0 and 1 and preserving distribution\n",
        "  img_norm = (img - np.amin(img))/( np.amax(img)- np.amin(img))\n",
        "  return img_norm\n",
        "\n",
        "def mergeAndPredict(model,raw,gt,start, full_size, patch_size, overlap, merge=True, normalize=True, clip=False):\n",
        "  end = start+int((full_size - overlap) / (patch_size - overlap))**2\n",
        "  # outputs a 3D list of 3 images in order raw, predicted, ground truth\n",
        "  pred = model.predict(raw[start:end])\n",
        "  print(\"predicted results shape:  \",np.shape(pred))\n",
        "  if len(np.shape(raw)) > 3:\n",
        "    if merge:\n",
        "      result = [merge_patches(raw[start:end,:,:,1],full_size,patch_size,overlap), merge_patches(pred[0:end-start,:,:,0],full_size,patch_size,overlap), merge_patches(gt[start:end,:,:,1],full_size,patch_size,overlap)]\n",
        "    else: result = [raw[start:end,:,:,1], pred[0:end-start,:,:,0],gt[start:end,:,:,1]]\n",
        "  # else:\n",
        "  #   if merge:\n",
        "  #     result = [merge_patches(raw[start:end,:,:],full_size,patch_size,overlap), merge_patches(pred[start:end,:,:,0],full_size,patch_size,overlap), merge_patches(gt[start:end,:,:],full_size,patch_size,overlap)]\n",
        "  #   else: result = [raw[start:end,:,:], pred[start:end,:,:,0], gt[start:end,:,:]]\n",
        "  if normalize: result = [normalize0to1(m) for m in result]\n",
        "  if clip: result = [np.clip(255 * m, 0, 255).astype('uint8') for m in result]\n",
        "  plot(np.concatenate((np.concatenate((result[0],result[1]),axis=1), result[2]),axis=1), \"raw, predicted, gt image\", size=(30,30))\n",
        "  return result\n",
        "\n",
        "def evalSSIM(result):\n",
        "  result1 = [np.expand_dims(m,-1) for m in result]\n",
        "  raw = tf.convert_to_tensor(np.expand_dims(result1[0],0),dtype=np.float32)\n",
        "  rest = tf.convert_to_tensor(np.expand_dims(result1[1],0),dtype=np.float32)\n",
        "  gt = tf.convert_to_tensor(np.expand_dims(result1[2],0),dtype=np.float32)\n",
        "  # Metric evaluation\n",
        "  print('raw vs gt=======================')\n",
        "  print(\"Our SSIM between raw and ground truth: \",ssim(gt,raw).numpy())\n",
        "  print(\"RCAN paper SSIM between raw and ground truth: \",ssim_rcan(gt, raw).numpy())\n",
        "  print('predicted vs gt=======================')\n",
        "  print(\"Our SSIM between predicted and ground truth: \",ssim(gt, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and ground truth: \",ssim_rcan(gt,rest).numpy())\n",
        "  print('predicted vs raw=======================')\n",
        "  print(\"Our SSIM between predicted and raw: \",ssim(raw, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and raw: \",ssim_rcan(raw, rest).numpy())\n",
        "\n",
        "\n",
        "\n",
        "def plot(img, label, hist=False, size=(10,10)):\n",
        "  plt.figure(figsize = size)\n",
        "  if hist==True:\n",
        "    plt.hist(img.flatten(), bins=120)\n",
        "  else: plt.imshow(img,cmap=\"gray\")\n",
        "  plt.title(label)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK27J3hGCzZJ"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.shape_base import expand_dims\n",
        "### Experiment 5: Experiment with 3 depth stacks vs 2D\n",
        "\n",
        "# The goal of this experiment is to investigate whether the model would able to learn transformation given information of following and preceding depths.\n",
        "\n",
        "# To decide: how will output be determined, whether it will be a 3D or 1D image (of middle depth)\n",
        "def mssim(y_true, y_pred):\n",
        "   return tf.image.ssim_multiscale(y_true, y_pred, 1, filter_size=3,power_factors=w, filter_sigma=0.5, k2=0.05)\n",
        "# **Methods**: generate 3D patches of 3 consecutive depths.\n",
        "def genSSIML1_3D_loss(alpha=0.84):\n",
        "  # this loss takes in 3D patch of dimension (n,n,3) and calculate loss on middle patch only\n",
        "  def SSIM_L1_loss(y_true, y_pred):\n",
        "    y_true =tf.expand_dims(y_true[...,1], -1)\n",
        "    ssim_partial = 1-((mssim(y_true, y_pred)+1)*0.5)\n",
        "    mae_partial = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "    # print(\"partials ssim l1: \",ssim_partial,mae_partial)\n",
        "\n",
        "    # adding l2 regulizer\n",
        "    l2_norms = [tf.nn.l2_loss(v) for v in model.trainable_variables]\n",
        "    l2_norm = tf.reduce_sum(l2_norms)\n",
        "    lambda_ = 0.1\n",
        "    return alpha*ssim_partial  + (1-alpha)*mae_partial + lambda_*l2_norm\n",
        "  return SSIM_L1_loss\n",
        "\n",
        "def genSSIMVar_3D_loss(alpha=0.84):\n",
        "  # this loss takes in 3D patch of dimension (n,n,3) and calculate loss on middle patch only\n",
        "  def SSIMVar_loss(y_true, y_pred):\n",
        "    y_true =tf.expand_dims(y_true[...,1], -1)\n",
        "    SSIM = 1-((mssim(y_true, y_pred)+1)*0.5)\n",
        "    MAE = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [mov_var(y_true), mov_var(y_pred)]])\n",
        "    return alpha * SSIM + (1-alpha) * MAE\n",
        "  return SSIMVar_loss\n",
        "\n",
        "# def genSSIMVar_loss(alpha=0.84):\n",
        "#   def SSIMVar_loss(y_true, y_pred):\n",
        "#       SSIM = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "#       MAE = tf.keras.losses.mae(\n",
        "#           *[tf.keras.backend.batch_flatten(y) for y in [mov_var(y_true), mov_var(y_pred)]])\n",
        "#       return alpha * SSIM + (1-alpha) * MAE\n",
        "#   return SSIMVar_loss\n",
        "\n",
        "def ssim_3d_metric(y_true, y_pred):\n",
        "  y_true =tf.expand_dims(y_true[...,1], -1)\n",
        "  return ssim(y_true, y_pred)\n",
        "def psnr_3d_metric(y_true, y_pred):\n",
        "  y_true =tf.expand_dims(y_true[...,1], -1)\n",
        "  return psnr(y_true, y_pred)\n",
        "def mae_3d_metric(y_true, y_pred):\n",
        "  y_true =tf.expand_dims(y_true[...,1], -1)\n",
        "  return tf.keras.losses.mae(y_true, y_pred)\n",
        "\n",
        "def config(patch_size, depth):\n",
        "  # return {\n",
        "  #       'img_size': 512,\n",
        "  #       'learning_rate': 1e-3,\n",
        "  #       'batch_size': 16,\n",
        "  #       'alpha': 0.84,\n",
        "  #       'patch_size':patch_size,\n",
        "  #       'input_shape': [patch_size, patch_size],\n",
        "  #       'kern_size':3,\n",
        "  #       'n_depth': depth,\n",
        "  #       'first_depth': 32,\n",
        "  #       'dropout': 0,\n",
        "  #       'epoch':100,\n",
        "  #       'lr_decay_factor':0.5,\n",
        "  #       'lr_decay_patience':3,\n",
        "  # }\n",
        "  # return {\n",
        "  #     'img_size': 512,\n",
        "  #     'learning_rate': 1e-4,\n",
        "  #     'batch_size': 16,\n",
        "  #     'alpha': 0.9,\n",
        "  #     'patch_size':patch_size,\n",
        "  #     'input_shape': [patch_size, patch_size],\n",
        "  #     'kern_size':3,\n",
        "  #     'kern_sigma': 0.99,\n",
        "  #     'n_depth': depth,\n",
        "  #     'first_depth': 32,\n",
        "  #     'dropout': 0,\n",
        "  #     'epoch':200,\n",
        "  #     'lr_decay_factor':0.5,\n",
        "  #     'lr_decay_patience':10,\n",
        "  #     'loss': genSSIML1_3D_loss\n",
        "  # }\n",
        "  return {\n",
        "      'img_size': 512,\n",
        "      'learning_rate': 1e-4,\n",
        "      'batch_size': 16,\n",
        "      'alpha': 0.6,\n",
        "      'patch_size':patch_size,\n",
        "      'input_shape': [patch_size, patch_size],\n",
        "      'kern_size':3,\n",
        "      'kern_sigma': 0.99,\n",
        "      'n_depth': depth,\n",
        "      'first_depth': 32,\n",
        "      'dropout': 0,\n",
        "      'epoch':200,\n",
        "      'lr_decay_factor':0.99,\n",
        "      'lr_decay_patience':5,\n",
        "      'loss': genSSIMVar_3D_loss\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqhfFdNVe7vj",
        "outputId": "c4b156e0-637f-4e8e-e8e2-2274313b4873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      [PATCHIFYING COMPLETED] output shape, slide:  (2610, 256, 256, 3) 128 ; number of images:  290 , number of patches:  2610\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (378, 256, 256, 3) 128 ; number of images:  42 , number of patches:  378\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (2610, 256, 256, 3) 128 ; number of images:  290 , number of patches:  2610\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (378, 256, 256, 3) 128 ; number of images:  42 , number of patches:  378\n"
          ]
        }
      ],
      "source": [
        "config_train = config(256, 6)\n",
        "\n",
        "# generating data\n",
        "# data_gen = DataGenerator(\n",
        "#     config_train['input_shape'],\n",
        "#     config_train['batch_size'],\n",
        "#     transform_function=None)\n",
        "[training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=config_train['patch_size'])\n",
        "[training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=config_train['patch_size'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pwl3oGYXsUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c10095e-726b-4816-fa04-952edb1ada84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUSTOM UNET INPUT SHAPE:  (256, 256, 3)\n",
            "n_dim 2\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " down_level_0_no_0 (Conv2D)     (None, 256, 256, 32  896         ['input[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " down_level_0_no_1 (Conv2D)     (None, 256, 256, 32  9248        ['down_level_0_no_0[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_0 (MaxPooling2D)           (None, 128, 128, 32  0           ['down_level_0_no_1[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " down_level_1_no_0 (Conv2D)     (None, 128, 128, 64  18496       ['max_0[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " down_level_1_no_1 (Conv2D)     (None, 128, 128, 64  36928       ['down_level_1_no_0[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_1 (MaxPooling2D)           (None, 64, 64, 64)   0           ['down_level_1_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " down_level_2_no_0 (Conv2D)     (None, 64, 64, 128)  73856       ['max_1[0][0]']                  \n",
            "                                                                                                  \n",
            " down_level_2_no_1 (Conv2D)     (None, 64, 64, 128)  147584      ['down_level_2_no_0[0][0]']      \n",
            "                                                                                                  \n",
            " max_2 (MaxPooling2D)           (None, 32, 32, 128)  0           ['down_level_2_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " down_level_3_no_0 (Conv2D)     (None, 32, 32, 256)  295168      ['max_2[0][0]']                  \n",
            "                                                                                                  \n",
            " down_level_3_no_1 (Conv2D)     (None, 32, 32, 256)  590080      ['down_level_3_no_0[0][0]']      \n",
            "                                                                                                  \n",
            " max_3 (MaxPooling2D)           (None, 16, 16, 256)  0           ['down_level_3_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " down_level_4_no_0 (Conv2D)     (None, 16, 16, 512)  1180160     ['max_3[0][0]']                  \n",
            "                                                                                                  \n",
            " down_level_4_no_1 (Conv2D)     (None, 16, 16, 512)  2359808     ['down_level_4_no_0[0][0]']      \n",
            "                                                                                                  \n",
            " max_4 (MaxPooling2D)           (None, 8, 8, 512)    0           ['down_level_4_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " down_level_5_no_0 (Conv2D)     (None, 8, 8, 1024)   4719616     ['max_4[0][0]']                  \n",
            "                                                                                                  \n",
            " down_level_5_no_1 (Conv2D)     (None, 8, 8, 1024)   9438208     ['down_level_5_no_0[0][0]']      \n",
            "                                                                                                  \n",
            " max_5 (MaxPooling2D)           (None, 4, 4, 1024)   0           ['down_level_5_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " middle_0 (Conv2D)              (None, 4, 4, 2048)   18876416    ['max_5[0][0]']                  \n",
            "                                                                                                  \n",
            " middle_2 (Conv2D)              (None, 4, 4, 1024)   18875392    ['middle_0[0][0]']               \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2D)   (None, 8, 8, 1024)   0           ['middle_2[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 8, 8, 2048)   0           ['up_sampling2d[0][0]',          \n",
            "                                                                  'down_level_5_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_5_no_0 (Conv2D)       (None, 8, 8, 1024)   18875392    ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " up_level_5_no_2 (Conv2D)       (None, 8, 8, 512)    4719104     ['up_level_5_no_0[0][0]']        \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 512)  0          ['up_level_5_no_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 16, 16, 1024  0           ['up_sampling2d_1[0][0]',        \n",
            "                                )                                 'down_level_4_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_4_no_0 (Conv2D)       (None, 16, 16, 512)  4719104     ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " up_level_4_no_2 (Conv2D)       (None, 16, 16, 256)  1179904     ['up_level_4_no_0[0][0]']        \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 256)  0          ['up_level_4_no_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 32, 32, 512)  0           ['up_sampling2d_2[0][0]',        \n",
            "                                                                  'down_level_3_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_3_no_0 (Conv2D)       (None, 32, 32, 256)  1179904     ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " up_level_3_no_2 (Conv2D)       (None, 32, 32, 128)  295040      ['up_level_3_no_0[0][0]']        \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 128)  0          ['up_level_3_no_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 64, 64, 256)  0           ['up_sampling2d_3[0][0]',        \n",
            "                                                                  'down_level_2_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_2_no_0 (Conv2D)       (None, 64, 64, 128)  295040      ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " up_level_2_no_2 (Conv2D)       (None, 64, 64, 64)   73792       ['up_level_2_no_0[0][0]']        \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 64  0          ['up_level_2_no_2[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 128, 128, 12  0           ['up_sampling2d_4[0][0]',        \n",
            "                                8)                                'down_level_1_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_1_no_0 (Conv2D)       (None, 128, 128, 64  73792       ['concatenate_4[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " up_level_1_no_2 (Conv2D)       (None, 128, 128, 32  18464       ['up_level_1_no_0[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSampling2D)  (None, 256, 256, 32  0          ['up_level_1_no_2[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 256, 256, 64  0           ['up_sampling2d_5[0][0]',        \n",
            "                                )                                 'down_level_0_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_0_no_0 (Conv2D)       (None, 256, 256, 32  18464       ['concatenate_5[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " up_level_0_no_2 (Conv2D)       (None, 256, 256, 32  9248        ['up_level_0_no_0[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 256, 256, 1)  33          ['up_level_0_no_2[0][0]']        \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 256, 256, 1)  0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 88,079,137\n",
            "Trainable params: 88,079,137\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# tdata = data_gen.flow(*list(zip([training_data[:1000],training_data_labels[:1000]])))\n",
        "# vdata = data_gen.flow(*list(zip([val_data[:100],val_data_labels[:100]])))\n",
        "\n",
        "curr_loss = genSSIML1_3D_loss(alpha = config_train['alpha'])\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',verbose=True,factor=config_train['lr_decay_factor'],min_delta=0,patience=config_train['lr_decay_patience'],)\n",
        "\n",
        "# TRAINING\n",
        "model = build_model(kern_size=config_train['kern_size'],n_dim=2,n_first=config_train['first_depth'],n_channel_out = 1, n_depth=config_train['n_depth'],shape=(config_train['input_shape'][0],config_train['input_shape'][0],3),residual=False, batch_norm=False)\n",
        "model = compile_model(model,config_train['learning_rate'], loss=curr_loss,metric=[{'psnr': psnr_3d_metric, 'ssim': ssim_3d_metric,'mae': mae_3d_metric}])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "                    training_data[:,:, :, :], training_data_labels[:,:, :, :],\n",
        "                    epochs = config_train['epoch'],\n",
        "                    batch_size=config_train['batch_size'],\n",
        "                    validation_data = [val_data,val_data_labels]\n",
        "                    , callbacks=[reduce_lr,tb_callback])\n"
      ],
      "metadata": {
        "id": "43dDgkkbmkYQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c58d361e-8e93-4886-d248-13e06ff193fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "164/164 [==============================] - 67s 268ms/step - loss: 511.0459 - mae_3d_metric: 0.1123 - psnr_3d_metric: 16.8418 - ssim_3d_metric: 0.3244 - val_loss: 222.8467 - val_mae_3d_metric: 0.1216 - val_psnr_3d_metric: 16.1896 - val_ssim_3d_metric: 0.2404 - lr: 1.0000e-04\n",
            "Epoch 2/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 129.5197 - mae_3d_metric: 0.1055 - psnr_3d_metric: 17.3234 - ssim_3d_metric: 0.3573 - val_loss: 70.5866 - val_mae_3d_metric: 0.1190 - val_psnr_3d_metric: 16.3771 - val_ssim_3d_metric: 0.2525 - lr: 1.0000e-04\n",
            "Epoch 3/200\n",
            "164/164 [==============================] - 38s 235ms/step - loss: 46.8081 - mae_3d_metric: 0.1044 - psnr_3d_metric: 17.4012 - ssim_3d_metric: 0.3676 - val_loss: 30.2209 - val_mae_3d_metric: 0.1183 - val_psnr_3d_metric: 16.3926 - val_ssim_3d_metric: 0.2585 - lr: 1.0000e-04\n",
            "Epoch 4/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 22.0164 - mae_3d_metric: 0.1040 - psnr_3d_metric: 17.4305 - ssim_3d_metric: 0.3715 - val_loss: 15.8001 - val_mae_3d_metric: 0.1182 - val_psnr_3d_metric: 16.3691 - val_ssim_3d_metric: 0.2609 - lr: 1.0000e-04\n",
            "Epoch 5/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 12.2240 - mae_3d_metric: 0.1039 - psnr_3d_metric: 17.4420 - ssim_3d_metric: 0.3730 - val_loss: 9.3696 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4299 - val_ssim_3d_metric: 0.2615 - lr: 1.0000e-04\n",
            "Epoch 6/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 7.5564 - mae_3d_metric: 0.1039 - psnr_3d_metric: 17.4429 - ssim_3d_metric: 0.3734 - val_loss: 6.0668 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4366 - val_ssim_3d_metric: 0.2616 - lr: 1.0000e-04\n",
            "Epoch 7/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 5.0512 - mae_3d_metric: 0.1038 - psnr_3d_metric: 17.4464 - ssim_3d_metric: 0.3734 - val_loss: 4.2056 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4300 - val_ssim_3d_metric: 0.2616 - lr: 1.0000e-04\n",
            "Epoch 8/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 3.5939 - mae_3d_metric: 0.1039 - psnr_3d_metric: 17.4487 - ssim_3d_metric: 0.3732 - val_loss: 3.0848 - val_mae_3d_metric: 0.1190 - val_psnr_3d_metric: 16.2709 - val_ssim_3d_metric: 0.2611 - lr: 1.0000e-04\n",
            "Epoch 9/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 2.6937 - mae_3d_metric: 0.1041 - psnr_3d_metric: 17.4305 - ssim_3d_metric: 0.3731 - val_loss: 2.3715 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4332 - val_ssim_3d_metric: 0.2613 - lr: 1.0000e-04\n",
            "Epoch 10/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 2.1093 - mae_3d_metric: 0.1040 - psnr_3d_metric: 17.4344 - ssim_3d_metric: 0.3730 - val_loss: 1.8977 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4344 - val_ssim_3d_metric: 0.2612 - lr: 1.0000e-04\n",
            "Epoch 11/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 1.7131 - mae_3d_metric: 0.1040 - psnr_3d_metric: 17.4399 - ssim_3d_metric: 0.3727 - val_loss: 1.5693 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4077 - val_ssim_3d_metric: 0.2611 - lr: 1.0000e-04\n",
            "Epoch 12/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 1.4331 - mae_3d_metric: 0.1040 - psnr_3d_metric: 17.4342 - ssim_3d_metric: 0.3723 - val_loss: 1.3322 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.4020 - val_ssim_3d_metric: 0.2610 - lr: 1.0000e-04\n",
            "Epoch 13/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 1.2273 - mae_3d_metric: 0.1040 - psnr_3d_metric: 17.4371 - ssim_3d_metric: 0.3719 - val_loss: 1.1545 - val_mae_3d_metric: 0.1182 - val_psnr_3d_metric: 16.3531 - val_ssim_3d_metric: 0.2609 - lr: 1.0000e-04\n",
            "Epoch 14/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 1.0702 - mae_3d_metric: 0.1041 - psnr_3d_metric: 17.4307 - ssim_3d_metric: 0.3713 - val_loss: 1.0160 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3401 - val_ssim_3d_metric: 0.2607 - lr: 1.0000e-04\n",
            "Epoch 15/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.9458 - mae_3d_metric: 0.1042 - psnr_3d_metric: 17.4232 - ssim_3d_metric: 0.3699 - val_loss: 0.9042 - val_mae_3d_metric: 0.1176 - val_psnr_3d_metric: 16.4647 - val_ssim_3d_metric: 0.2608 - lr: 1.0000e-04\n",
            "Epoch 16/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.8442 - mae_3d_metric: 0.1042 - psnr_3d_metric: 17.4241 - ssim_3d_metric: 0.3694 - val_loss: 0.8118 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.3967 - val_ssim_3d_metric: 0.2608 - lr: 1.0000e-04\n",
            "Epoch 17/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.7588 - mae_3d_metric: 0.1042 - psnr_3d_metric: 17.4246 - ssim_3d_metric: 0.3694 - val_loss: 0.7332 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4110 - val_ssim_3d_metric: 0.2606 - lr: 1.0000e-04\n",
            "Epoch 18/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.6858 - mae_3d_metric: 0.1043 - psnr_3d_metric: 17.4176 - ssim_3d_metric: 0.3687 - val_loss: 0.6652 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4280 - val_ssim_3d_metric: 0.2607 - lr: 1.0000e-04\n",
            "Epoch 19/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.6222 - mae_3d_metric: 0.1043 - psnr_3d_metric: 17.4178 - ssim_3d_metric: 0.3678 - val_loss: 0.6058 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3833 - val_ssim_3d_metric: 0.2605 - lr: 1.0000e-04\n",
            "Epoch 20/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.5660 - mae_3d_metric: 0.1044 - psnr_3d_metric: 17.4148 - ssim_3d_metric: 0.3671 - val_loss: 0.5531 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3843 - val_ssim_3d_metric: 0.2605 - lr: 1.0000e-04\n",
            "Epoch 21/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.5163 - mae_3d_metric: 0.1045 - psnr_3d_metric: 17.4102 - ssim_3d_metric: 0.3664 - val_loss: 0.5065 - val_mae_3d_metric: 0.1186 - val_psnr_3d_metric: 16.3126 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 22/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.4719 - mae_3d_metric: 0.1044 - psnr_3d_metric: 17.4100 - ssim_3d_metric: 0.3666 - val_loss: 0.4642 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4236 - val_ssim_3d_metric: 0.2605 - lr: 1.0000e-04\n",
            "Epoch 23/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.4323 - mae_3d_metric: 0.1045 - psnr_3d_metric: 17.4108 - ssim_3d_metric: 0.3662 - val_loss: 0.4271 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3392 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 24/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.3969 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4080 - ssim_3d_metric: 0.3659 - val_loss: 0.3937 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3397 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 25/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.3653 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4052 - ssim_3d_metric: 0.3659 - val_loss: 0.3638 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3855 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 26/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.3373 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4051 - ssim_3d_metric: 0.3657 - val_loss: 0.3377 - val_mae_3d_metric: 0.1188 - val_psnr_3d_metric: 16.2969 - val_ssim_3d_metric: 0.2602 - lr: 1.0000e-04\n",
            "Epoch 27/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.3124 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4029 - ssim_3d_metric: 0.3657 - val_loss: 0.3140 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.4033 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 28/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.2904 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4018 - ssim_3d_metric: 0.3656 - val_loss: 0.2934 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3827 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 29/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.2711 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4041 - ssim_3d_metric: 0.3655 - val_loss: 0.2754 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3415 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 30/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.2541 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4080 - ssim_3d_metric: 0.3656 - val_loss: 0.2600 - val_mae_3d_metric: 0.1193 - val_psnr_3d_metric: 16.2432 - val_ssim_3d_metric: 0.2600 - lr: 1.0000e-04\n",
            "Epoch 31/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.2393 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3970 - ssim_3d_metric: 0.3653 - val_loss: 0.2458 - val_mae_3d_metric: 0.1182 - val_psnr_3d_metric: 16.3598 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 32/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.2264 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4041 - ssim_3d_metric: 0.3653 - val_loss: 0.2339 - val_mae_3d_metric: 0.1183 - val_psnr_3d_metric: 16.3472 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 33/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.2154 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3986 - ssim_3d_metric: 0.3655 - val_loss: 0.2234 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.4001 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 34/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.2058 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4015 - ssim_3d_metric: 0.3655 - val_loss: 0.2149 - val_mae_3d_metric: 0.1187 - val_psnr_3d_metric: 16.3030 - val_ssim_3d_metric: 0.2602 - lr: 1.0000e-04\n",
            "Epoch 35/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1976 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4065 - ssim_3d_metric: 0.3655 - val_loss: 0.2077 - val_mae_3d_metric: 0.1192 - val_psnr_3d_metric: 16.2530 - val_ssim_3d_metric: 0.2600 - lr: 1.0000e-04\n",
            "Epoch 36/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1907 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4028 - ssim_3d_metric: 0.3654 - val_loss: 0.2008 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3914 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 37/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1849 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4093 - ssim_3d_metric: 0.3654 - val_loss: 0.1958 - val_mae_3d_metric: 0.1186 - val_psnr_3d_metric: 16.3121 - val_ssim_3d_metric: 0.2602 - lr: 1.0000e-04\n",
            "Epoch 38/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1801 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.3992 - ssim_3d_metric: 0.3656 - val_loss: 0.1913 - val_mae_3d_metric: 0.1185 - val_psnr_3d_metric: 16.3248 - val_ssim_3d_metric: 0.2602 - lr: 1.0000e-04\n",
            "Epoch 39/200\n",
            "164/164 [==============================] - 39s 238ms/step - loss: 0.1760 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4030 - ssim_3d_metric: 0.3654 - val_loss: 0.1874 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.3970 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 40/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1727 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4030 - ssim_3d_metric: 0.3655 - val_loss: 0.1848 - val_mae_3d_metric: 0.1188 - val_psnr_3d_metric: 16.2969 - val_ssim_3d_metric: 0.2602 - lr: 1.0000e-04\n",
            "Epoch 41/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1700 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4024 - ssim_3d_metric: 0.3655 - val_loss: 0.1821 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3675 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 42/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1679 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4013 - ssim_3d_metric: 0.3656 - val_loss: 0.1800 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4265 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 43/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1661 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4081 - ssim_3d_metric: 0.3654 - val_loss: 0.1790 - val_mae_3d_metric: 0.1190 - val_psnr_3d_metric: 16.2708 - val_ssim_3d_metric: 0.2601 - lr: 1.0000e-04\n",
            "Epoch 44/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1647 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.3988 - ssim_3d_metric: 0.3656 - val_loss: 0.1774 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3846 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 45/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1637 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4027 - ssim_3d_metric: 0.3656 - val_loss: 0.1766 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3395 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 46/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1629 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4013 - ssim_3d_metric: 0.3656 - val_loss: 0.1757 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.3971 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 47/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1622 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4039 - ssim_3d_metric: 0.3655 - val_loss: 0.1752 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.3994 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 48/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1617 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4050 - ssim_3d_metric: 0.3655 - val_loss: 0.1752 - val_mae_3d_metric: 0.1189 - val_psnr_3d_metric: 16.2795 - val_ssim_3d_metric: 0.2601 - lr: 1.0000e-04\n",
            "Epoch 49/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1614 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4001 - ssim_3d_metric: 0.3656 - val_loss: 0.1746 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3795 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 50/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1612 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3926 - ssim_3d_metric: 0.3655 - val_loss: 0.1743 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3794 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 51/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1610 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4006 - ssim_3d_metric: 0.3656 - val_loss: 0.1742 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3710 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 52/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1609 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4012 - ssim_3d_metric: 0.3655 - val_loss: 0.1742 - val_mae_3d_metric: 0.1183 - val_psnr_3d_metric: 16.3521 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 53/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1608 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4038 - ssim_3d_metric: 0.3655 - val_loss: 0.1740 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3849 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 54/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1608 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4023 - ssim_3d_metric: 0.3655 - val_loss: 0.1739 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4151 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 55/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1608 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3993 - ssim_3d_metric: 0.3654 - val_loss: 0.1739 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4477 - val_ssim_3d_metric: 0.2605 - lr: 1.0000e-04\n",
            "Epoch 56/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.3999 - ssim_3d_metric: 0.3655 - val_loss: 0.1740 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3782 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 57/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4057 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4166 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 58/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1607 - mae_3d_metric: 0.1045 - psnr_3d_metric: 17.4079 - ssim_3d_metric: 0.3656 - val_loss: 0.1738 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4332 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 59/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3922 - ssim_3d_metric: 0.3655 - val_loss: 0.1740 - val_mae_3d_metric: 0.1183 - val_psnr_3d_metric: 16.3539 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 60/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1608 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3959 - ssim_3d_metric: 0.3653 - val_loss: 0.1741 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3363 - val_ssim_3d_metric: 0.2603 - lr: 1.0000e-04\n",
            "Epoch 61/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.3993 - ssim_3d_metric: 0.3655 - val_loss: 0.1739 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3841 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 62/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4046 - ssim_3d_metric: 0.3654 - val_loss: 0.1742 - val_mae_3d_metric: 0.1187 - val_psnr_3d_metric: 16.3049 - val_ssim_3d_metric: 0.2602 - lr: 1.0000e-04\n",
            "Epoch 63/200\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.4000 - ssim_3d_metric: 0.3655\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.4000 - ssim_3d_metric: 0.3655 - val_loss: 0.1739 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.4048 - val_ssim_3d_metric: 0.2604 - lr: 1.0000e-04\n",
            "Epoch 64/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3981 - ssim_3d_metric: 0.3655 - val_loss: 0.1740 - val_mae_3d_metric: 0.1182 - val_psnr_3d_metric: 16.3673 - val_ssim_3d_metric: 0.2604 - lr: 9.9000e-05\n",
            "Epoch 65/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3982 - ssim_3d_metric: 0.3654 - val_loss: 0.1739 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3839 - val_ssim_3d_metric: 0.2604 - lr: 9.9000e-05\n",
            "Epoch 66/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4020 - ssim_3d_metric: 0.3654 - val_loss: 0.1740 - val_mae_3d_metric: 0.1183 - val_psnr_3d_metric: 16.3510 - val_ssim_3d_metric: 0.2603 - lr: 9.9000e-05\n",
            "Epoch 67/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4007 - ssim_3d_metric: 0.3655 - val_loss: 0.1742 - val_mae_3d_metric: 0.1187 - val_psnr_3d_metric: 16.3010 - val_ssim_3d_metric: 0.2602 - lr: 9.9000e-05\n",
            "Epoch 68/200\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4034 - ssim_3d_metric: 0.3656\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4034 - ssim_3d_metric: 0.3656 - val_loss: 0.1740 - val_mae_3d_metric: 0.1183 - val_psnr_3d_metric: 16.3447 - val_ssim_3d_metric: 0.2603 - lr: 9.9000e-05\n",
            "Epoch 69/200\n",
            "164/164 [==============================] - 38s 234ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3951 - ssim_3d_metric: 0.3653 - val_loss: 0.1740 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3743 - val_ssim_3d_metric: 0.2604 - lr: 9.8010e-05\n",
            "Epoch 70/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1045 - psnr_3d_metric: 17.4056 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4223 - val_ssim_3d_metric: 0.2604 - lr: 9.8010e-05\n",
            "Epoch 71/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4031 - ssim_3d_metric: 0.3654 - val_loss: 0.1740 - val_mae_3d_metric: 0.1182 - val_psnr_3d_metric: 16.3656 - val_ssim_3d_metric: 0.2604 - lr: 9.8010e-05\n",
            "Epoch 72/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4043 - ssim_3d_metric: 0.3654 - val_loss: 0.1741 - val_mae_3d_metric: 0.1185 - val_psnr_3d_metric: 16.3279 - val_ssim_3d_metric: 0.2603 - lr: 9.8010e-05\n",
            "Epoch 73/200\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4054 - ssim_3d_metric: 0.3656\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4054 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.4084 - val_ssim_3d_metric: 0.2604 - lr: 9.8010e-05\n",
            "Epoch 74/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.3983 - ssim_3d_metric: 0.3657 - val_loss: 0.1739 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3875 - val_ssim_3d_metric: 0.2604 - lr: 9.7030e-05\n",
            "Epoch 75/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3982 - ssim_3d_metric: 0.3653 - val_loss: 0.1740 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3772 - val_ssim_3d_metric: 0.2604 - lr: 9.7030e-05\n",
            "Epoch 76/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3997 - ssim_3d_metric: 0.3656 - val_loss: 0.1741 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3435 - val_ssim_3d_metric: 0.2603 - lr: 9.7030e-05\n",
            "Epoch 77/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4062 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.4076 - val_ssim_3d_metric: 0.2604 - lr: 9.7030e-05\n",
            "Epoch 78/200\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4006 - ssim_3d_metric: 0.3656\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4006 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4330 - val_ssim_3d_metric: 0.2604 - lr: 9.7030e-05\n",
            "Epoch 79/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4014 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4219 - val_ssim_3d_metric: 0.2604 - lr: 9.6060e-05\n",
            "Epoch 80/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4080 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1179 - val_psnr_3d_metric: 16.4088 - val_ssim_3d_metric: 0.2604 - lr: 9.6060e-05\n",
            "Epoch 81/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4078 - ssim_3d_metric: 0.3656 - val_loss: 0.1740 - val_mae_3d_metric: 0.1182 - val_psnr_3d_metric: 16.3581 - val_ssim_3d_metric: 0.2603 - lr: 9.6060e-05\n",
            "Epoch 82/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4018 - ssim_3d_metric: 0.3656 - val_loss: 0.1741 - val_mae_3d_metric: 0.1185 - val_psnr_3d_metric: 16.3203 - val_ssim_3d_metric: 0.2602 - lr: 9.6060e-05\n",
            "Epoch 83/200\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4016 - ssim_3d_metric: 0.3655\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4016 - ssim_3d_metric: 0.3655 - val_loss: 0.1739 - val_mae_3d_metric: 0.1178 - val_psnr_3d_metric: 16.4196 - val_ssim_3d_metric: 0.2604 - lr: 9.6060e-05\n",
            "Epoch 84/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4018 - ssim_3d_metric: 0.3656 - val_loss: 0.1741 - val_mae_3d_metric: 0.1184 - val_psnr_3d_metric: 16.3423 - val_ssim_3d_metric: 0.2603 - lr: 9.5099e-05\n",
            "Epoch 85/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4030 - ssim_3d_metric: 0.3656 - val_loss: 0.1740 - val_mae_3d_metric: 0.1181 - val_psnr_3d_metric: 16.3696 - val_ssim_3d_metric: 0.2604 - lr: 9.5099e-05\n",
            "Epoch 86/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4030 - ssim_3d_metric: 0.3655 - val_loss: 0.1739 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3947 - val_ssim_3d_metric: 0.2604 - lr: 9.5099e-05\n",
            "Epoch 87/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4031 - ssim_3d_metric: 0.3657 - val_loss: 0.1739 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3891 - val_ssim_3d_metric: 0.2604 - lr: 9.5099e-05\n",
            "Epoch 88/200\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4054 - ssim_3d_metric: 0.3654\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.4054 - ssim_3d_metric: 0.3654 - val_loss: 0.1745 - val_mae_3d_metric: 0.1193 - val_psnr_3d_metric: 16.2464 - val_ssim_3d_metric: 0.2600 - lr: 9.5099e-05\n",
            "Epoch 89/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1047 - psnr_3d_metric: 17.3965 - ssim_3d_metric: 0.3655 - val_loss: 0.1738 - val_mae_3d_metric: 0.1177 - val_psnr_3d_metric: 16.4388 - val_ssim_3d_metric: 0.2605 - lr: 9.4148e-05\n",
            "Epoch 90/200\n",
            "164/164 [==============================] - 38s 233ms/step - loss: 0.1607 - mae_3d_metric: 0.1046 - psnr_3d_metric: 17.3999 - ssim_3d_metric: 0.3656 - val_loss: 0.1739 - val_mae_3d_metric: 0.1180 - val_psnr_3d_metric: 16.3861 - val_ssim_3d_metric: 0.2604 - lr: 9.4148e-05\n",
            "Epoch 91/200\n",
            " 36/164 [=====>........................] - ETA: 28s - loss: 0.1622 - mae_3d_metric: 0.1057 - psnr_3d_metric: 17.3276 - ssim_3d_metric: 0.3628"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a44a3a2a6c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     , callbacks=[reduce_lr,tb_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m       \u001b[0mbatch_run_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m       tf.summary.scalar(\n\u001b[0;32m-> 2488\u001b[0;31m           'batch_steps_per_second', 1. / batch_run_time, step=self._train_step)\n\u001b[0m\u001b[1;32m   2489\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/scalar/summary_v2.py\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(name, data, step, description)\u001b[0m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msummary_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scalar_summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         return tf.summary.write(\n\u001b[1;32m     90\u001b[0m             \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/check_ops.py\u001b[0m in \u001b[0;36massert_scalar_v2\u001b[0;34m(tensor, message, name)\u001b[0m\n\u001b[1;32m   2247\u001b[0m       \u001b[0munknown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m   \"\"\"\n\u001b[0;32m-> 2249\u001b[0;31m   \u001b[0massert_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/check_ops.py\u001b[0m in \u001b[0;36massert_scalar\u001b[0;34m(tensor, name, message)\u001b[0m\n\u001b[1;32m   2273\u001b[0m   \"\"\"\n\u001b[1;32m   2274\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'assert_scalar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2275\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2276\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2277\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_message_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \"\"\"\n\u001b[1;32m    267\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 268\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1No5sRYtNUul"
      },
      "outputs": [],
      "source": [
        "# evaluating SSIM on merged images\n",
        "result = mergeAndPredict(model,val_data,val_data_labels,0, config_train['img_size'], config_train['patch_size'], config_train['patch_size']/2)\n",
        "evalSSIM(result)\n",
        "result = mergeAndPredict(model,training_data,training_data_labels,0, config_train['img_size'], config_train['patch_size'], config_train['patch_size']/2)\n",
        "evalSSIM(result)\n",
        "print(\"END TRIAL =========================================\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# LEARNING RATE SEARCH EXPERIMENT\n",
        "class MetricsCheckpoint(Callback):\n",
        "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
        "    def __init__(self, savepath):\n",
        "        super(MetricsCheckpoint, self).__init__()\n",
        "        self.savepath = savepath\n",
        "        self.history = {}\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        np.save(self.savepath, self.history)\n",
        "\n",
        "def plotKerasLearningCurve():\n",
        "    plt.figure(figsize=(10,5))\n",
        "    metrics = np.load('logs.npy')[()]\n",
        "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
        "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
        "        l = np.array(metrics[k])\n",
        "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
        "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
        "        y = l[x]\n",
        "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
        "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')\n",
        "    plt.legend(loc=4)\n",
        "    plt.axis([0, None, None, None]);\n",
        "    plt.grid()\n",
        "    plt.xlabel('Number of epochs')\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "dict_characters = {0: '0', 1: '1', 2: '2',\n",
        "        3: '3', 4: '4', 5: '5', 6: '6', 7:'7',\n",
        "        8: '8', 9: '9'}\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from keras.callbacks import LambdaCallback\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "class LRFinder:\n",
        "    \"\"\"\n",
        "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
        "    See for details:\n",
        "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if math.isnan(loss) or loss > self.best_loss * 4:\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
        "        num_batches = epochs * x_train.shape[0] / batch_size\n",
        "        self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
        "\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs,\n",
        "                        callbacks=[callback])\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale('log')\n",
        "        for i in range(len(self.lrs)):\n",
        "          print('lrs, losses',self.lrs,self.losses)\n",
        "\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change of the loss function.\n",
        "        Parameters:\n",
        "            sma - number of batches for simple moving average to smooth out the curve.\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "            y_lim - limits for the y axis.\n",
        "        \"\"\"\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
        "            derivatives.append(derivative)\n",
        "\n",
        "        plt.ylabel(\"rate of loss change\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale('log')\n",
        "        plt.ylim(y_lim)\n",
        "        for i in range(len(self.lrs)):\n",
        "          print('lrs, losses',self.lrs,self.losses)"
      ],
      "metadata": {
        "id": "esKJHSqfn11B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_finder = LRFinder(model)\n",
        "lr_finder.find(training_data, training_data_labels, start_lr=0.0000001, end_lr=100, batch_size=16, epochs=5)\n"
      ],
      "metadata": {
        "id": "BH9kF-p3oDq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efemu0MMETsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOs2ecqFX2JT"
      },
      "outputs": [],
      "source": [
        "print(np.shape(expandLastDim(training_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3FFTpKUxdnM"
      },
      "outputs": [],
      "source": [
        "print(np.shape(training_data),np.shape(training_data_labels), np.shape(val_data), np.shape(val_data_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBE8nETFO6B2"
      },
      "outputs": [],
      "source": [
        "# # print(np.amax(val_data[0]))\n",
        "# plot(val_data[0],'test',hist=True)\n",
        "# [training_dataa, val_dataa] = load_data(MAIN_PATH+DATA_PATH, patch=32)\n",
        "# print(\"TEST AFTER\", np.amax(training_dataa))\n",
        "# test = normalize(merge_patches(val_dataa[0:961,:,:],512,32,16))\n",
        "# plot(test,'test',hist=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD9AhngwMLFs"
      },
      "outputs": [],
      "source": [
        "# result = predictAndPlot(model,val_data,val_data_labels, 0, 961, isPlot=True, merge=True, normalize=False, clip=False)\n",
        "# evalSSIM(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO-rSGKhMVvl"
      },
      "outputs": [],
      "source": [
        "def revert_img(img,original_size, patch_shape, slide):\n",
        "  # reverts original image and removes overlaps by splitting overlap over 2 images\n",
        "  step = int(patch_shape-slide)\n",
        "  reconstructed_arr = np.zeros((original_size,original_size))\n",
        "  for x in range(img.shape[0]):\n",
        "    for y in range(img.shape[1]):\n",
        "      start_x = int(slide/2)\n",
        "      start_y = int(slide/2)\n",
        "      end_x = 0\n",
        "      end_y = 0\n",
        "      if x == 0:\n",
        "        start_x = 0\n",
        "        end_x = int(slide/2)\n",
        "      if y == 0:\n",
        "        start_y = 0\n",
        "        end_y = int(slide/2)\n",
        "      if x == img.shape[0]-1: end_x = int(slide/2)\n",
        "      if y == img.shape[1]-1: end_y = int(slide/2)\n",
        "      x_pos, y_pos = x * step + start_x, y * step + start_y\n",
        "      # print('x_pos, start_x: ',x_pos, start_x)\n",
        "      # print('x_pos + step + end_x, start_x+step+end_x: ',x_pos + step + end_x, start_x+step+end_x)\n",
        "      # print('y_pos, start_y: ',y_pos, start_y)\n",
        "      # print('y_pos + step + end_y,  start_y+step+end_y: ',y_pos + step + end_y,  start_y+step+end_y)\n",
        "      # print(\"end===============x,y: \",x,y)\n",
        "      reconstructed_arr[x_pos : x_pos + step + end_x, y_pos : y_pos + step + end_y] = img[x, y, start_x:start_x+step+end_x, start_y:start_y+step+end_y]\n",
        "  return reconstructed_arr\n",
        "\n",
        "def merge_patches(img, original_size, patch_shape, slide):\n",
        "  #  merging patches, img is a 3D array of stacked patches\n",
        "  print(\"merging patches, img shape: \", img.shape)\n",
        "  row_len = int(math.sqrt(img.shape[0]))\n",
        "  patches = np.zeros((row_len,row_len,patch_shape,patch_shape))\n",
        "  print(img.shape)\n",
        "  print(patches.shape)\n",
        "  for r in range(row_len):\n",
        "      patches[r,:,:,:] = img[r*row_len:r*row_len+row_len,:,:]\n",
        "  # plt.figure(5)\n",
        "  # plt.imshow(patches[1,1,:,:])\n",
        "  return revert_img(patches,original_size,patch_shape, slide)\n",
        "\n",
        "\n",
        "def MSE(img1, img2):\n",
        "  # comparing one processed and preprocessed image\n",
        "  squared_diff = (img1 -img2) ** 2\n",
        "  summed = np.sum(squared_diff)\n",
        "  num_pix = img1.shape[0] * img1.shape[1] #img1 and 2 should have same shape\n",
        "  err = summed / num_pix\n",
        "  return err\n",
        "\n",
        "def normalize(img):\n",
        "  # Normalizing images between 0 and 1 and preserving distribution\n",
        "  img_norm = (img - np.amin(img))/( np.amax(img)- np.amin(img))\n",
        "  return img_norm\n",
        "\n",
        "def plot(img, label, hist=False, size=(10,10)):\n",
        "  plt.figure(figsize = size)\n",
        "  if hist==True:\n",
        "    plt.hist(img.flatten(), bins=120)\n",
        "  else: plt.imshow(img,cmap=\"gray\")\n",
        "  plt.title(label)\n",
        "\n",
        "def normalize_between_zero_and_one(m):\n",
        "    max_val, min_val = m.max(), m.min()\n",
        "    diff = max_val - min_val\n",
        "    return (m - min_val) / diff if diff > 0 else np.zeros_like(m)\n",
        "\n",
        "def normalize_percentile(img, pmin=0.1, pmax=99.9, clip = True):\n",
        "  eps=1e-20 # avoid zero division\n",
        "  mi = np.percentile(img,pmin,axis=None,keepdims=True)\n",
        "  # mi = np.amin(img)\n",
        "  # print(\"mi\",mi)\n",
        "  ma = np.percentile(img,pmax,axis=None,keepdims=True)\n",
        "  if clip == True: return np.clip((img - mi) / ( ma - mi + eps ), 0, 1)\n",
        "  return (img - mi) / ( ma - mi + eps )\n",
        "\n",
        "# def ssim(y_true, y_pred):\n",
        "#     '''\n",
        "#     Computes the structural similarity index between two images. Note that the\n",
        "#     maximum signal value is assumed to be 1.\n",
        "#     '''\n",
        "\n",
        "#     return ssim2(y_true, y_pred,1,k2=0.05)\n",
        "# our SSIM loss\n",
        "from tensorflow.image import ssim as ssim2\n",
        "def ssim_our(y_true, y_pred):\n",
        "  return ssim2(y_true, y_pred,1,k2=0.05)\n",
        "\n",
        "def _get_gaussian_kernel(dim, size, sigma):\n",
        "    k = size // 2\n",
        "    normal = tfp.distributions.Normal(0.0, sigma)\n",
        "    p = normal.prob(tf.range(-k, size - k, dtype=tf.float32))\n",
        "\n",
        "    indices = [chr(i) for i in range(105, 105 + dim)]\n",
        "    eq = ','.join(indices) + '->' + ''.join(indices)\n",
        "    kernel = tf.einsum(eq, *([p] * dim))\n",
        "    kernel /= tf.reduce_sum(kernel)\n",
        "    kernel = kernel[..., tf.newaxis, tf.newaxis]\n",
        "\n",
        "    return kernel\n",
        "\n",
        "# RCAN ssim\n",
        "import keras.backend as K\n",
        "def ssim_rcan(y_true, y_pred):\n",
        "    '''\n",
        "    Computes the structural similarity index between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    References\n",
        "    ----------\n",
        "    Image Quality Assessment: From Error Visibility to Structural Similarity\n",
        "    https://doi.org/10.1109/TIP.2003.819861\n",
        "    '''\n",
        "\n",
        "    c1 = 0.01 ** 2\n",
        "    c2 = 0.03 ** 2\n",
        "\n",
        "    dim = K.ndim(y_pred) - 2\n",
        "    if dim not in (2, 3):\n",
        "        raise NotImplementedError(f'{dim}D SSIM is not suported')\n",
        "\n",
        "    num_channels = K.int_shape(y_pred)[-1]\n",
        "\n",
        "    kernel = _get_gaussian_kernel(dim, 11, 1.5)\n",
        "    conv = K.conv2d if dim == 2 else K.conv3d\n",
        "\n",
        "    def average(x):\n",
        "        # channel-wise weighted average using the Gaussian kernel\n",
        "        return tf.concat(\n",
        "            [conv(y, kernel) for y in tf.split(x, num_channels, axis=-1)],\n",
        "            axis=-1)\n",
        "\n",
        "    ux = average(y_true)\n",
        "    uy = average(y_pred)\n",
        "\n",
        "    a = ux * uy\n",
        "    b = K.square(ux) + K.square(uy)\n",
        "    c = average(y_true * y_pred)\n",
        "    d = average(K.square(y_true) + K.square(y_pred))\n",
        "\n",
        "    lum = (2 * a + c1) / (b + c1)\n",
        "    cs = (2 * (c - a) + c2) / (d - b + c2)\n",
        "\n",
        "    return K.mean(K.batch_flatten(lum * cs), axis=-1)\n",
        "\n",
        "def predictAndPlot(model,input,val, start, end, isPlot=True, merge=True, normalize=False, clip=False):\n",
        "  pred = model.predict(input[start:end])\n",
        "  print(\"shape of predicted image: \", np.shape(pred))\n",
        "  if len(np.shape(input)) > 3:\n",
        "    if merge:\n",
        "      result = [merge_patches(input[start:end,:,:,0],512,64,0), merge_patches(pred[start:end,:,:,0],512,64,0), merge_patches(val[start:end,:,:,0],512,64,0)]\n",
        "    else: result = [input[start:end,:,:,0], pred[start:end,:,:,0],val[start:end,:,:,0]]\n",
        "  else:\n",
        "    if merge:\n",
        "      result = [merge_patches(input[start:end,:,:],512,64,0), merge_patches(pred[start:end,:,:,0],512,64,0), merge_patches(val[start:end,:,:],512,64,0)]\n",
        "    else: result = [input[start:end,:,:], pred[start:end,:,:,0], val[start:end,:,:]]\n",
        "  if normalize: result = [normalize_percentile(m) for m in result]\n",
        "  if clip: result = [np.clip(255 * m, 0, 255).astype('uint8') for m in result]\n",
        "  res_img =  np.concatenate((np.concatenate((result[0],result[1]),axis=1),result[2]),axis=1)\n",
        "  plot(res_img, \"input, restored, gt\", size=(30,30))\n",
        "  # plot(result[0],'input 25x')\n",
        "  # plot(result[1],'restored')\n",
        "  # plot(result[2],'gt 40x')\n",
        "  if isPlot:\n",
        "    plot(result[0],'hist input 25x',hist=True)\n",
        "    plot(result[1],'hist restored',hist=True)\n",
        "    plot(result[2],'hist gt 40x',hist=True)\n",
        "  return result\n",
        "\n",
        "def evalSSIM(result):\n",
        "  result1 = [np.expand_dims(m,-1) for m in result]\n",
        "  raw = tf.convert_to_tensor(np.expand_dims(result1[0],0),dtype=np.float32)\n",
        "  rest = tf.convert_to_tensor(np.expand_dims(result1[1],0),dtype=np.float32)\n",
        "  gt = tf.convert_to_tensor(np.expand_dims(result1[2],0),dtype=np.float32)\n",
        "  # Metric evaluation\n",
        "  print('raw vs gt=======================')\n",
        "  print(\"Our SSIM between raw and ground truth: \",ssim_our(gt,raw).numpy())\n",
        "  print(\"RCAN paper SSIM between raw and ground truth: \",ssim_rcan(gt, raw).numpy())\n",
        "  print('predicted vs gt=======================')\n",
        "  print(\"Our SSIM between predicted and ground truth: \",ssim_our(gt, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and ground truth: \",ssim_rcan(gt,rest).numpy())\n",
        "  print('predicted vs raw=======================')\n",
        "  print(\"Our SSIM between predicted and raw: \",ssim_our(raw, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and raw: \",ssim_rcan(raw, rest).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxNKJi3L6D_F"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability as tfp\n",
        "result = predictAndPlot(model,expandLastDim(val_data), expandLastDim(val_data_labels), 0, 64, isPlot=True, merge=True, normalize=False, clip=False)\n",
        "evalSSIM(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vNyttLT6YFy"
      },
      "outputs": [],
      "source": [
        "print(np.shape(val_data))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "gpu2",
      "language": "python",
      "name": "gpu2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
