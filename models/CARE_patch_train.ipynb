{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_knrNHXGl91k"
      },
      "source": [
        "Upscaling 25x to 40x images of NADH type only.\n",
        "Importing CARE model from another module.\n",
        "\n",
        "Last updated: 06.29\n",
        "\n",
        "Change: Testing premade CARE package\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCz6M7fo1Bdm",
        "outputId": "e83bfab8-91e5-473e-ad7a-c7e81ef08315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (5.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import_ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import_ipynb) (1.15.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.11.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (2.16.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.9.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import_ipynb) (3.8.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install tifffile\n",
        "# !pip install sklearn\n",
        "# !pip install scikit-image\n",
        "!pip install import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gL_e5tv648r"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMUT7spjl35_"
      },
      "outputs": [],
      "source": [
        "# importing dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "import skimage\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import keras.backend as K\n",
        "from datetime import datetime\n",
        "import fractions\n",
        "import itertools\n",
        "import tqdm\n",
        "from keras.utils.conv_utils import normalize_tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-8XBBzanaNG",
        "outputId": "73ac4086-f5e3-431f-ccae-d9d788535c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok5TnqXZU4_0"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH = '/cluster/tufts/georgakoudi_lab01/tdinh02/npz/Cervix_all_data_original.npz'\n",
        "# LABEL_PATH = '/cluster/tufts/georgakoudi_lab01/tdinh02/npz/Cervix_all_data_labels_original_3D.npz'\\\n",
        "# MAIN_PATH = '/cluster/tufts/georgakoudi_lab01/tdinh02/objective_transform/'\n",
        "\n",
        "DATA_PATH = '/npz/Cervix_all_data_original.npz'\n",
        "LABEL_PATH = '/npz/Cervix_all_data_labels_original.npz'\n",
        "# MAIN_PATH= r\"/content/drive/MyDrive/Research and Grad/Diamond Program/objective_transfer/deep_learning\" # Artem's Drive\n",
        "MAIN_PATH = \"/content/drive/MyDrive\" # Filip's drive\n",
        "now = datetime.now() # current date and time\n",
        "CURR_DATE = now.strftime(\"%m-%d-%Y\")\n",
        "\n",
        "def config(patch_size, depth):\n",
        "  return {\n",
        "        'img_size': 256,\n",
        "        'learning_rate': 1e-5,\n",
        "        'batch_size': 16,\n",
        "        'alpha': 0.84,\n",
        "        'patch_size':patch_size,\n",
        "        'input_shape': [patch_size, patch_size],\n",
        "        'kern_size':3,\n",
        "        'n_depth': depth,\n",
        "        'first_depth': 32,\n",
        "        'dropout': 0,\n",
        "        'epoch':1,\n",
        "        'lr_decay_factor':0.97,\n",
        "        'lr_decay_patience':10,\n",
        "  }\n",
        "\n",
        "MODEL_ROOT_NAME = \"CARE_patch_depth_tune_0726\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWOPU7Kl6QC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10e7e41-1b18-4656-fdbc-21af08b4847a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from CARE_util.ipynb\n"
          ]
        }
      ],
      "source": [
        "# importing yaml config file and resetting working direcgory\n",
        "os.chdir(MAIN_PATH+'/CAREstd/')\n",
        "import import_ipynb\n",
        "from CARE_util import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW-FYFCH579U"
      },
      "outputs": [],
      "source": [
        "# def ssim(y_true, y_pred):\n",
        "#     return (tf.image.ssim(y_true, y_pred,1,k2=0.05)) # sliding Gaussian window as mentioned in Wikipedia\n",
        "# smaller filter size, should avoid blurring\n",
        "def ssim(y_true, y_pred):\n",
        "    return tf.image.ssim(y_true, y_pred, 1, filter_size=3, filter_sigma=0.5, k2=0.05)\n",
        "\n",
        "w =  (0.0448, 0.2856, 0.3001)\n",
        "def mssim(y_true, y_pred):\n",
        "   return tf.image.ssim_multiscale(y_true, y_pred, 1, filter_size=11,power_factors=w, filter_sigma=1.5, k2=0.05)\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    '''\n",
        "    Computs the peak signal-to-noise ratio between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    '''\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def SSIM_loss(y_true, y_pred):\n",
        "    return 1-((ssim(y_true, y_pred)+1)*0.5) # chanfe to 2\n",
        "\n",
        "# Use cw_ssim\n",
        "#\n",
        "# from scipy import signal\n",
        "# def cw_ssim(y_true, y_pred, width=30):\n",
        "#         \"\"\"Compute the complex wavelet SSIM (CW-SSIM) value from the reference\n",
        "#         image to the target image.\n",
        "#         Args:\n",
        "#           target (str or PIL.Image): Input image to compare the reference image\n",
        "#           to. This may be a PIL Image object or, to save time, an SSIMImage\n",
        "#           object (e.g. the img member of another SSIM object).\n",
        "#           width: width for the wavelet convolution (default: 30)\n",
        "#         Returns:\n",
        "#           Computed CW-SSIM float value.\n",
        "#         \"\"\"\n",
        "#         k = 0.01 #k (float): CW-SSIM configuration variable (default 0.01)\n",
        "#         # Define a width for the wavelet convolution\n",
        "#         widths = np.arange(1, width+1)\n",
        "\n",
        "#         # Use the image data as arrays\n",
        "#         sig1 = y_pred.numpy().flatten()\n",
        "#         sig2 = y_true.numpy().flatten()\n",
        "\n",
        "#         # Convolution\n",
        "#         cwtmatr1 = signal.cwt(sig1, signal.ricker, widths)\n",
        "#         cwtmatr2 = signal.cwt(sig2, signal.ricker, widths)\n",
        "\n",
        "#         # Compute the first term\n",
        "#         c1c2 = np.multiply(abs(cwtmatr1), abs(cwtmatr2))\n",
        "#         c1_2 = np.square(abs(cwtmatr1))\n",
        "#         c2_2 = np.square(abs(cwtmatr2))\n",
        "#         num_ssim_1 = 2 * np.sum(c1c2, axis=0) + k\n",
        "#         den_ssim_1 = np.sum(c1_2, axis=0) + np.sum(c2_2, axis=0) + k\n",
        "\n",
        "#         # Compute the second term\n",
        "#         c1c2_conj = np.multiply(cwtmatr1, np.conjugate(cwtmatr2))\n",
        "#         num_ssim_2 = 2 * np.abs(np.sum(c1c2_conj, axis=0)) + k\n",
        "#         den_ssim_2 = 2 * np.sum(np.abs(c1c2_conj), axis=0) + k\n",
        "\n",
        "#         # Construct the result\n",
        "#         ssim_map = (num_ssim_1 / den_ssim_1) * (num_ssim_2 / den_ssim_2)\n",
        "\n",
        "#         # Average the per pixel results\n",
        "#         index = np.average(ssim_map)\n",
        "#         return index\n",
        "\n",
        "def SSIML1_loss(y_true, y_pred, alpha=0.84):\n",
        "  # alpha = 0.84\n",
        "  ssim_partial = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "  mae_partial = tf.keras.losses.mae(\n",
        "        *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "  return alpha*ssim_partial  + (1-alpha)*mae_partial\n",
        "\n",
        "def mov_var(image):\n",
        "  dtype = tf.float32\n",
        "  img_height = tf.shape(image)[1]\n",
        "  img_width = tf.shape(image)[2]\n",
        "  mean_filter = tf.ones((3,3),dtype) / 9\n",
        "  img_mean = tf.nn.conv2d(image[:,:,:,:],\n",
        "                          mean_filter[:,:,tf.newaxis,tf.newaxis],\n",
        "                          [1,1,1,1],'VALID')\n",
        "  img_clip = image[:, 1:-1, 1:-1,:]\n",
        "  # Difference between pixel intensity and its block mean\n",
        "  x_diff = tf.math.squared_difference(img_clip, img_mean) / 8\n",
        "  return x_diff\n",
        "\n",
        "def genSSIML1_loss(alpha=0.84):\n",
        "  def SSIM_L1_loss(y_true, y_pred):\n",
        "    ssim_partial = 1-((cw_ssim(y_true, y_pred)+1)*0.5)\n",
        "    mae_partial = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "    return alpha*ssim_partial  + (1-alpha)*mae_partial\n",
        "  return SSIM_L1_loss\n",
        "\n",
        "def genSSIMVar_loss(alpha=0.84):\n",
        "  def SSIMVar_loss(y_true, y_pred):\n",
        "      SSIM = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "      MAE = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [mov_var(y_true), mov_var(y_pred)]])\n",
        "      return alpha * SSIM + (1-alpha) * MAE\n",
        "  return SSIMVar_loss\n",
        "\n",
        "def genSSIMVarL1_loss(alpha=0.84):\n",
        "  def SSIMVarL1_loss(y_true, y_pred):\n",
        "      SSIM = 1-((ssim(y_true, y_pred)+1)*0.5)\n",
        "      MAE = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [mov_var(y_true), mov_var(y_pred)]])\n",
        "      MAE2 = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "      return alpha * SSIM + ((1-alpha)/4) * MAE + (3*(1-alpha)/4) * MAE2\n",
        "  return SSIMVarL1_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U23b4LDwl38K"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def create_patches(img, patch_shape, slide):\n",
        "    # returns stack of patches and number of patches\n",
        "    patch_img = skimage.util.view_as_windows(img, (patch_shape,patch_shape), step=patch_shape-slide)\n",
        "    patch = patch_img.reshape(patch_img.shape[0]*patch_img.shape[1],patch_shape,patch_shape) # more time efficient\n",
        "    return patch\n",
        "\n",
        "def patchify(input, patch_shape, slide):\n",
        "    # getting number of input images\n",
        "    len_to_allocate = int(np.shape(input)[0]*((slide-np.shape(input)[1]) / (slide-patch_shape))**2)\n",
        "    data = np.zeros((len_to_allocate,patch_shape,patch_shape))\n",
        "    count = 0\n",
        "    for i in range(np.shape(input)[0]):\n",
        "      A = create_patches(input[i], patch_shape, slide)\n",
        "      # print(\"[:,:,:]A\", np.amax(A[:,:,:]))\n",
        "      data[count:count+len(A),:,:] = A[:,:,:]\n",
        "      # print(\"data[count:count+len(A),:,:]\", np.amax(data[count:count+len(A),:,:]))\n",
        "      count = count + len(A)\n",
        "    print(\"      [PATCHIFYING COMPLETED] output shape, slide: \",np.shape(data),slide,\"; number of images: \", np.shape(input)[0], \", number of patches: \", np.shape(data)[0])\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_data(path, expand=False, patch=None):\n",
        "    # Loading preprocessed image patches and adding 4th arbitrary dimension\n",
        "    b = np.load(path)\n",
        "    training_data = b['t']\n",
        "    val_data = b['v']\n",
        "\n",
        "    # not supporting 3D stacks at the moment\n",
        "    if len(np.shape(training_data)) > 3:\n",
        "      print(\"3D STACKS ARE NOT SUPPORTED\")\n",
        "      return []\n",
        "    # patchify if patch size is passed\n",
        "    if patch != None:\n",
        "      slide = int(patch/2)\n",
        "      # slide = 0\n",
        "      training_data = patchify(training_data, patch, slide)\n",
        "      val_data = patchify(val_data, patch, slide)\n",
        "\n",
        "    # if len(np.shape(training_data))>3:\n",
        "    #   # for 3D only, changing dimensions\n",
        "    #   training_data = np.transpose(training_data, (0, 3, 1, 2,4))\n",
        "    #   val_data = np.transpose(val_data, (0, 3, 1, 2,4))\n",
        "    if expand == True:\n",
        "      training_data = expandLastDim(training_data)\n",
        "      val_data = expandLastDim(val_data)\n",
        "    res = [training_data, val_data]\n",
        "    return res\n",
        "\n",
        "def expandLastDim(data):\n",
        "    return np.expand_dims(data, -1)\n",
        "\n",
        "# add arbitrary channel dimension and augments data on flow, also splits data into batches\n",
        "class DataGenerator:\n",
        "    '''\n",
        "    Generates batches of image pairs with real-time data augmentation.\n",
        "    Parameters\n",
        "    ----------\n",
        "    shape: tuple of int\n",
        "        Shape of batch images (excluding the channel dimension).\n",
        "    batch_size: int\n",
        "        Batch size.\n",
        "    transform_function: str or callable or None\n",
        "        Function used for data augmentation. Typically you will set\n",
        "        ``transform_function='rotate_and_flip'`` to apply combination of\n",
        "        randomly selected image rotation and flipping.  Alternatively, you can\n",
        "        specify an arbitrary transformation function which takes two input\n",
        "        images (source and target) and returns transformed images. If\n",
        "        ``transform_function=None``, no augmentation will be performed.\n",
        "    intensity_threshold: float\n",
        "        If ``intensity_threshold > 0``, pixels whose intensities are greater\n",
        "        than this threshold will be considered as foreground.\n",
        "    area_ratio_threshold: float between 0 and 1\n",
        "        If ``intensity_threshold > 0``, the generator calculates the ratio of\n",
        "        foreground pixels in a target patch, and rejects the patch if the ratio\n",
        "        is smaller than this threshold.\n",
        "    scale_factor: int != 0\n",
        "        Scale factor for the target patch size. Positive and negative values\n",
        "        mean up- and down-scaling respectively.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 shape,\n",
        "                 batch_size,\n",
        "                 transform_function='rotate_and_flip',\n",
        "                 intensity_threshold=0.0,\n",
        "                 area_ratio_threshold=0.0,\n",
        "                 scale_factor=1):\n",
        "        def rotate_and_flip(x, y, dim):\n",
        "            if dim == 2:\n",
        "                k = np.random.randint(0, 4)\n",
        "                x, y = [np.rot90(v, k=k) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    x, y = [np.fliplr(v) for v in (x, y)]\n",
        "                return x, y\n",
        "            elif dim == 3:\n",
        "                k = np.random.randint(0, 4)\n",
        "                x, y = [np.rot90(v, k=k, axes=(1, 2)) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    x, y = [np.flip(v, axis=1) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    x, y = [np.flip(v, axis=0) for v in (x, y)]\n",
        "                return x, y\n",
        "            else:\n",
        "                raise ValueError('Unsupported dimension')\n",
        "\n",
        "        self._shape = tuple(shape)\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        dim = len(self._shape)\n",
        "\n",
        "        if transform_function == 'rotate_and_flip':\n",
        "            if shape[-2] != shape[-1]:\n",
        "                raise ValueError(\n",
        "                    'Patch shape must be square when using `rotate_and_flip`; '\n",
        "                    f'Received shape: {shape}')\n",
        "            self._transform_function = lambda x, y: rotate_and_flip(x, y, dim)\n",
        "        elif callable(transform_function):\n",
        "            self._transform_function = transform_function\n",
        "        elif transform_function is None:\n",
        "            self._transform_function = lambda x, y: (x, y)\n",
        "        else:\n",
        "            raise ValueError('Invalid transform function')\n",
        "\n",
        "        self._intensity_threshold = intensity_threshold\n",
        "\n",
        "        if not 0 <= area_ratio_threshold <= 1:\n",
        "            raise ValueError('\"area_ratio_threshold\" must be between 0 and 1')\n",
        "        self._area_threshold = area_ratio_threshold * np.prod(shape)\n",
        "\n",
        "        self._scale_factor = normalize_tuple(scale_factor, dim, 'scale_factor')\n",
        "        if any(not isinstance(f, int) or f == 0 for f in self._scale_factor):\n",
        "            raise ValueError('\"scale_factor\" must be nonzero integer')\n",
        "\n",
        "    class _Sequence(tf.keras.utils.Sequence):\n",
        "        def _scale(self, shape):\n",
        "            return tuple(\n",
        "                s * f if f > 0 else s // -f\n",
        "                for s, f in zip(shape, self._scale_factor))\n",
        "\n",
        "        def __init__(self,\n",
        "                     x,\n",
        "                     y,\n",
        "                     batch_size,\n",
        "                     shape,\n",
        "                     transform_function,\n",
        "                     intensity_threshold,\n",
        "                     area_threshold,\n",
        "                     scale_factor):\n",
        "            self._batch_size = batch_size\n",
        "            self._transform_function = transform_function\n",
        "            self._intensity_threshold = intensity_threshold\n",
        "            self._area_threshold = area_threshold\n",
        "            self._scale_factor = scale_factor\n",
        "\n",
        "            for s, f, in zip(shape, self._scale_factor):\n",
        "                if f < 0 and s % -f != 0:\n",
        "                    raise ValueError(\n",
        "                        'When downsampling, all elements in `shape` must be '\n",
        "                        'divisible by the scale factor; '\n",
        "                        f'Received shape: {shape}, '\n",
        "                        f'scale factor: {self._scale_factor}')\n",
        "\n",
        "            self._x, self._y = [\n",
        "                list(m) if isinstance(m, (list, tuple)) else [m]\n",
        "                for m in [x, y]]\n",
        "            self._x = np.moveaxis(self._x,0,-1)\n",
        "            self._y = np.moveaxis(self._y,0,-1)\n",
        "            if len(self._x) != len(self._y):\n",
        "                raise ValueError(\n",
        "                    'Different number of images are given: '\n",
        "                    f'{len(self._x)} vs. {len(self._y)}')\n",
        "\n",
        "            if len({m.dtype for m in self._x}) != 1:\n",
        "                raise ValueError('All source images must be the same type')\n",
        "            if len({m.dtype for m in self._y}) != 1:\n",
        "                raise ValueError('All target images must be the same type')\n",
        "            print(len(self._x))\n",
        "            for i in range(len(self._x)):\n",
        "                if len(self._x[i].shape) == len(shape):\n",
        "                    self._x[i] = self._x[i][..., np.newaxis]\n",
        "\n",
        "                if len(self._y[i].shape) == len(shape):\n",
        "                    self._y[i] = self._y[i][..., np.newaxis]\n",
        "\n",
        "                if len(self._x[i].shape) != len(shape) + 1:\n",
        "                    raise ValueError(f'Source image must be {len(shape)}D')\n",
        "\n",
        "                if len(self._y[i].shape) != len(shape) + 1:\n",
        "                    raise ValueError(f'Target image must be {len(shape)}D')\n",
        "                if self._x[i].shape[:-1] < shape:\n",
        "                    raise ValueError(\n",
        "                        'Source image must be larger than the patch size')\n",
        "\n",
        "                expected_y_image_size = self._scale(self._x[i].shape[:-1])\n",
        "                if self._y[i].shape[:-1] != expected_y_image_size:\n",
        "                    raise ValueError('Invalid target image size: '\n",
        "                                     f'expected {expected_y_image_size}, '\n",
        "                                     f'but received {self._y[i].shape[:-1]}')\n",
        "\n",
        "            if len({m.shape[-1] for m in self._x}) != 1:\n",
        "                raise ValueError(\n",
        "                    'All source images must have the same number of channels')\n",
        "            if len({m.shape[-1] for m in self._y}) != 1:\n",
        "                raise ValueError(\n",
        "                    'All target images must have the same number of channels')\n",
        "            self._batch_x = np.zeros(\n",
        "                (batch_size, *shape, self._x[0].shape[-1]),\n",
        "                dtype=self._x[0].dtype)\n",
        "            self._batch_y = np.zeros(\n",
        "                (batch_size, *self._scale(shape),self._y[0].shape[-1]),\n",
        "                dtype=self._y[0].dtype)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self._x) // self._batch_size # return a dummy value\n",
        "\n",
        "        def __next__(self):\n",
        "            return self.__getitem__(0)\n",
        "\n",
        "        def __getitem__(self, _):\n",
        "            for i in range(self._batch_size):\n",
        "                for _ in range(139):\n",
        "                    j = np.random.randint(0, len(self._x))\n",
        "\n",
        "                    tl = [np.random.randint(0, a - b + 1)\n",
        "                          for a, b in zip(\n",
        "                              self._x[j].shape, self._batch_x.shape[1:])]\n",
        "\n",
        "                    x = np.copy(self._x[j][tuple(\n",
        "                        [slice(a, a + b) for a, b in zip(\n",
        "                            tl, self._batch_x.shape[1:])])])\n",
        "                    y = np.copy(self._y[j][tuple(\n",
        "                        [slice(a, a + b) for a, b in zip(\n",
        "                            self._scale(tl), self._batch_y.shape[1:])])])\n",
        "\n",
        "                    if (self._intensity_threshold <= 0.0 or\n",
        "                            np.count_nonzero(y > self._intensity_threshold)\n",
        "                            >= self._area_threshold):\n",
        "                        break\n",
        "                else:\n",
        "                    import warnings\n",
        "                    warnings.warn(\n",
        "                        'Failed to sample a valid patch',\n",
        "                        RuntimeWarning,\n",
        "                        stacklevel=3)\n",
        "\n",
        "\n",
        "                self._batch_x[i], self._batch_y[i] = \\\n",
        "                    self._transform_function(x, y)\n",
        "            return self._batch_x, self._batch_y\n",
        "\n",
        "    def flow(self, x, y):\n",
        "        '''\n",
        "        Returns a `keras.utils.Sequence` object which generates batches\n",
        "        infinitely. It can be used as an input generator for\n",
        "        `keras.models.Model.fit_generator()`.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: array_like or list of array_like\n",
        "            Source image(s).\n",
        "        y: array_like or list of array_like\n",
        "            Target image(s).\n",
        "        Returns\n",
        "        -------\n",
        "        keras.utils.Sequence\n",
        "            `keras.utils.Sequence` object which generates tuples of source and\n",
        "            target image patches.\n",
        "        '''\n",
        "        return self._Sequence(x,\n",
        "                              y,\n",
        "                              self._batch_size,\n",
        "                              self._shape,\n",
        "                              self._transform_function,\n",
        "                              self._intensity_threshold,\n",
        "                              self._area_threshold,\n",
        "                              self._scale_factor)\n",
        "\n",
        "# CALLBACKS\n",
        "# tensorboard callback\n",
        "tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1,write_images=True,write_steps_per_second=True,)\n",
        "\n",
        "\n",
        "def build_model(n_dim=2, n_depth=2, kern_size=5, dropout= 0, n_first=32, n_channel_out=1, last_activation='relu', batch_norm=False,shape=(None,None,1)):\n",
        "    model = common_unet(n_dim,n_depth,n_first=n_first,kern_size=kern_size, dropout=dropout,last_activation=last_activation,batch_norm=batch_norm)(shape)\n",
        "    model.summary()\n",
        "    model.save_weights('model.h5')\n",
        "    return model\n",
        "\n",
        "# Compile model for training\n",
        "def compile_model(model, lr, loss=SSIML1_loss):\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr),\n",
        "                loss=loss,\n",
        "                # loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "                metrics = [{'psnr': psnr, 'ssim': ssim}])\n",
        "                # metrics = [{'mae': tf.keras.losses.mae}])\n",
        "    return model\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "def save_weight_callback(checkpoint_path, batch_size, epoch_freq=10):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=epoch_freq*batch_size)\n",
        "\n",
        "def display_training_stats(history, savepath, stat, axis_name):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.plot(history.history[stat], label=stat)\n",
        "    plt.plot(history.history['val_'+stat], label = 'val_'+stat)\n",
        "    plt.xlabel(axis_name)\n",
        "    plt.ylabel(stat)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.savefig(savepath+'_'+stat+'.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foNi8mdZluv9"
      },
      "source": [
        "### **Manual Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIfSb_aC7HqJ"
      },
      "outputs": [],
      "source": [
        "# for each patch_size, we will train different depths until smalles image shape is 4x4 px\n",
        "# max depth is determined by log2(patch_size) - 2. ex: log2(64) - 2 = 4 depths, or log2(256) -2 = 6 depths\n",
        "PATCH_SIZES = [32,64,128,256]\n",
        "\n",
        "if False:\n",
        "  res = np.zeros((1,4))\n",
        "  for patch_size in PATCH_SIZES:\n",
        "    max_depth = int(math.log2(patch_size)-2)\n",
        "    depths = np.arange(2,max_depth+1)\n",
        "    print(\"MANUAL TUNING PATCHES - patch size: \"+ str(patch_size) + \", max_depth: \"+str(max_depth)+\", all depths: \",depths)\n",
        "    # for every depth\n",
        "    for depth in depths:\n",
        "      print(\"    [CONFIGURING] \"+\"patch size: \"+ str(patch_size) + \", current depth: \"+str(depth))\n",
        "      config_train = config(patch_size, depth)\n",
        "\n",
        "      # learning rate reducer callback\n",
        "      reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(verbose=True,factor=config_train['lr_decay_factor'],min_delta=0,patience=config_train['lr_decay_patience'],)\n",
        "\n",
        "      # generating data\n",
        "      data_gen = DataGenerator(\n",
        "          config_train['input_shape'],\n",
        "          config_train['batch_size'],\n",
        "          transform_function=None)\n",
        "      [training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=patch_size)\n",
        "      [training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=patch_size)\n",
        "      tdata = data_gen.flow(*list(zip([training_data,training_data_labels])))\n",
        "      vdata = data_gen.flow(*list(zip([val_data,val_data_labels])))\n",
        "      # TRAINING\n",
        "      model = build_model(kern_size=config_train['kern_size'], n_depth=config_train['n_depth'],shape=(config_train['input_shape'][0],config_train['input_shape'][0],1))\n",
        "      model = compile_model(model,config_train['learning_rate'])\n",
        "      print(\"    [TRAINING] \"+\"patch size: \"+ str(patch_size) + \", current depth: \"+str(depth))\n",
        "      history = model.fit(\n",
        "                          x=tdata,\n",
        "                          epochs = config_train['epoch'],\n",
        "                          validation_data = vdata\n",
        "                          , callbacks=[reduce_lr,tb_callback])\n",
        "      print(\"    [TRAINING FINISHED] \"+\"p: \"+ str(patch_size) + \", d: \"+str(depth)+ \", results: val_ssim = \",np.amax(history.history['val_ssim']),\" ssim = \",np.amax(history.history['ssim']))\n",
        "      save_path = MAIN_PATH+\"/\"+MODEL_ROOT_NAME+\"/\"+\"p_d_valssim_\"+str(patch_size)+\"_\"+str(depth)+\"_\"+str(np.amax(history.history['val_ssim']))\n",
        "      print(\"    [SAVING MODEL] path is: \",save_path)\n",
        "      model.save(save_path)\n",
        "      display_training_stats(history,save_path, 'loss', 'Epoch')\n",
        "      display_training_stats(history,save_path, 'ssim', 'Epoch')\n",
        "      display_training_stats(history,save_path, 'psnr', 'Epoch')\n",
        "      res = np.concatenate((res, [[str(patch_size),str(depth),np.amax(history.history['ssim']),np.amax(history.history['val_ssim'])]]), axis=0)\n",
        "      del training_data, val_data,tdata,vdata, training_data_labels, val_data_labels, model\n",
        "\n",
        "  print(\"[TUNING  FINISHED, PRINTING RESULTS]\")\n",
        "  for i in res:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_gaussian_kernel(dim, size, sigma):\n",
        "    import tensorflow_probability as tfp\n",
        "    k = size // 2\n",
        "    normal = tfp.distributions.Normal(0.0, sigma)\n",
        "    p = normal.prob(tf.range(-k, size - k, dtype=tf.float32))\n",
        "\n",
        "    indices = [chr(i) for i in range(105, 105 + dim)]\n",
        "    eq = ','.join(indices) + '->' + ''.join(indices)\n",
        "    kernel = tf.einsum(eq, *([p] * dim))\n",
        "    kernel /= tf.reduce_sum(kernel)\n",
        "    kernel = kernel[..., tf.newaxis, tf.newaxis]\n",
        "\n",
        "    return kernel\n",
        "\n",
        "# RCAN ssim\n",
        "import keras.backend as K\n",
        "def ssim_rcan(y_true, y_pred):\n",
        "    '''\n",
        "    Computes the structural similarity index between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    References\n",
        "    ----------\n",
        "    Image Quality Assessment: From Error Visibility to Structural Similarity\n",
        "    https://doi.org/10.1109/TIP.2003.819861\n",
        "    '''\n",
        "\n",
        "    c1 = 0.01 ** 2\n",
        "    c2 = 0.03 ** 2\n",
        "\n",
        "    dim = K.ndim(y_pred) - 2\n",
        "    if dim not in (2, 3):\n",
        "        raise NotImplementedError(f'{dim}D SSIM is not suported')\n",
        "\n",
        "    num_channels = K.int_shape(y_pred)[-1]\n",
        "\n",
        "    kernel = _get_gaussian_kernel(dim, 11, 1.5)\n",
        "    conv = K.conv2d if dim == 2 else K.conv3d\n",
        "\n",
        "    def average(x):\n",
        "        # channel-wise weighted average using the Gaussian kernel\n",
        "        return tf.concat(\n",
        "            [conv(y, kernel) for y in tf.split(x, num_channels, axis=-1)],\n",
        "            axis=-1)\n",
        "\n",
        "    ux = average(y_true)\n",
        "    uy = average(y_pred)\n",
        "\n",
        "    a = ux * uy\n",
        "    b = K.square(ux) + K.square(uy)\n",
        "    c = average(y_true * y_pred)\n",
        "    d = average(K.square(y_true) + K.square(y_pred))\n",
        "\n",
        "    lum = (2 * a + c1) / (b + c1)\n",
        "    cs = (2 * (c - a) + c2) / (d - b + c2)\n",
        "\n",
        "    return K.mean(K.batch_flatten(lum * cs), axis=-1)\n",
        "\n",
        "def revert_img(img,original_size, patch_shape, slide):\n",
        "  # reverts original image and removes overlaps by splitting overlap over 2 images\n",
        "  step = int(patch_shape-slide)\n",
        "  reconstructed_arr = np.zeros((original_size,original_size))\n",
        "  for x in range(img.shape[0]):\n",
        "    for y in range(img.shape[1]):\n",
        "      start_x = int(slide/2)\n",
        "      start_y = int(slide/2)\n",
        "      end_x = 0\n",
        "      end_y = 0\n",
        "      if x == 0:\n",
        "        start_x = 0\n",
        "        end_x = int(slide/2)\n",
        "      if y == 0:\n",
        "        start_y = 0\n",
        "        end_y = int(slide/2)\n",
        "      if x == img.shape[0]-1: end_x = int(slide/2)\n",
        "      if y == img.shape[1]-1: end_y = int(slide/2)\n",
        "      x_pos, y_pos = x * step + start_x, y * step + start_y\n",
        "      reconstructed_arr[x_pos : x_pos + step + end_x, y_pos : y_pos + step + end_y] = img[x, y, start_x:start_x+step+end_x, start_y:start_y+step+end_y]\n",
        "  return reconstructed_arr\n",
        "\n",
        "def merge_patches(img, original_size, patch_shape, slide):\n",
        "  #  merging patches, img is a 3D array of stacked patches\n",
        "  row_len = int(math.sqrt(img.shape[0]))\n",
        "  patches = np.zeros((row_len,row_len,patch_shape,patch_shape))\n",
        "  for r in range(row_len):\n",
        "      patches[r,:,:,:] = img[r*row_len:r*row_len+row_len,:,:]\n",
        "  return revert_img(patches,original_size,patch_shape, slide)\n",
        "\n",
        "def normalize0to1(img):\n",
        "  # Normalizing images between 0 and 1 and preserving distribution\n",
        "  img_norm = (img - np.amin(img))/( np.amax(img)- np.amin(img))\n",
        "  return img_norm\n",
        "\n",
        "def mergeAndPredict(model,raw,gt,start, full_size, patch_size, overlap, merge=True, normalize=True, clip=False):\n",
        "  end = int((full_size - overlap) / (patch_size - overlap))**2\n",
        "  # outputs a 3D list of 3 images in order raw, predicted, ground truth\n",
        "  pred = model.predict(raw[start:end])\n",
        "\n",
        "  if len(np.shape(raw)) > 3:\n",
        "    if merge:\n",
        "      result = [merge_patches(raw[start:end,:,:,0],full_size,patch_size,overlap), merge_patches(pred[start:end,:,:,0],full_size,patch_size,overlap), merge_patches(gt[start:end,:,:,0],full_size,patch_size,overlap)]\n",
        "    else: result = [raw[start:end,:,:,0], pred[start:end,:,:,0],gt[start:end,:,:,0]]\n",
        "  else:\n",
        "    if merge:\n",
        "      result = [merge_patches(raw[start:end,:,:],full_size,patch_size,overlap), merge_patches(pred[start:end,:,:,0],full_size,patch_size,overlap), merge_patches(gt[start:end,:,:],full_size,patch_size,overlap)]\n",
        "    else: result = [raw[start:end,:,:], pred[start:end,:,:,0], gt[start:end,:,:]]\n",
        "  if normalize: result = [normalize0to1(m) for m in result]\n",
        "  if clip: result = [np.clip(255 * m, 0, 255).astype('uint8') for m in result]\n",
        "  plot(result[1], \"predicted image\")\n",
        "  return result\n",
        "\n",
        "def evalSSIM(result):\n",
        "  result1 = [np.expand_dims(m,-1) for m in result]\n",
        "  raw = tf.convert_to_tensor(np.expand_dims(result1[0],0),dtype=np.float32)\n",
        "  rest = tf.convert_to_tensor(np.expand_dims(result1[1],0),dtype=np.float32)\n",
        "  gt = tf.convert_to_tensor(np.expand_dims(result1[2],0),dtype=np.float32)\n",
        "  # Metric evaluation\n",
        "  print('raw vs gt=======================')\n",
        "  print(\"Our SSIM between raw and ground truth: \",ssim(gt,raw).numpy())\n",
        "  print(\"RCAN paper SSIM between raw and ground truth: \",ssim_rcan(gt, raw).numpy())\n",
        "  print('predicted vs gt=======================')\n",
        "  print(\"Our SSIM between predicted and ground truth: \",ssim(gt, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and ground truth: \",ssim_rcan(gt,rest).numpy())\n",
        "  print('predicted vs raw=======================')\n",
        "  print(\"Our SSIM between predicted and raw: \",ssim(raw, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and raw: \",ssim_rcan(raw, rest).numpy())\n",
        "\n",
        "def config(patch_size, depth):\n",
        "  return {\n",
        "        'img_size': 512,\n",
        "        'learning_rate': 1e-4,\n",
        "        'batch_size': 16,\n",
        "        'alpha': 0.84,\n",
        "        'patch_size':patch_size,\n",
        "        'input_shape': [patch_size, patch_size],\n",
        "        'kern_size':3,\n",
        "        'n_depth': depth,\n",
        "        'first_depth': 32,\n",
        "        'dropout': 0,\n",
        "        'epoch':100,\n",
        "        'lr_decay_factor':0.9,\n",
        "        'lr_decay_patience':5,\n",
        "  }\n",
        "\n",
        "def plot(img, label, hist=False, size=(10,10)):\n",
        "  plt.figure(figsize = size)\n",
        "  if hist==True:\n",
        "    plt.hist(img.flatten(), bins=120)\n",
        "  else: plt.imshow(img,cmap=\"gray\")\n",
        "  plt.title(label)\n",
        "\n"
      ],
      "metadata": {
        "id": "NB7XDna0zi33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the cwtRicker, cwtMortlet\n",
        "\n",
        "# \"\"\"\n",
        "# Continuous Wavelet Transforms\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     wav:      matrix - float32 - shape (N,)\n",
        "#     widthCwt: scalar - int32\n",
        "#     Returns\n",
        "#     -------\n",
        "#     output:   matrix - float32 - shape (widthCwt, N)\n",
        "# \"\"\"\n",
        "# def cwtRicker(tfType, wav, widthCwt):  return cwt(tfType, wav, widthCwt, rickerWavelet)\n",
        "# def cwtMortlet(tfType, wav, widthCwt): return cwt(tfType, wav, widthCwt, mortletWavelet)\n",
        "\n",
        "# # ------------------------------------------------------\n",
        "\n",
        "# def cwt(tfType, wav, widthCwt, wavelet):\n",
        "#     length = -1\n",
        "#     if None not in list(tf.shape(wav)):\n",
        "#       wav = tf.cast(wav, tfType)\n",
        "#       wav = tf.reshape(wav, [1,length,1,1])\n",
        "\n",
        "#     # While loop functions\n",
        "#     def body(i, m):\n",
        "#         v = conv1DWavelet(tfType, wav, i, wavelet)\n",
        "#         v = tf.reshape(v, [length, 1])\n",
        "\n",
        "#         m = tf.concat([m,v], 1)\n",
        "\n",
        "#         return [1 + i, m]\n",
        "\n",
        "#     def cond_(i, m):\n",
        "#         return tf.less_equal(i, widthCwt)\n",
        "\n",
        "#     # Initialize and run while loop\n",
        "#     emptyCwtMatrix = tf.zeros([length, 0], tfType)\n",
        "#     i = tf.constant(1)\n",
        "#     _, result = tf.while_loop(\n",
        "#             cond_,\n",
        "#             body,\n",
        "#             [i, emptyCwtMatrix],\n",
        "#             shape_invariants=[i.get_shape(), tf.TensorShape([length, None])],\n",
        "#             back_prop=False,\n",
        "#             parallel_iterations=1024,\n",
        "#             )\n",
        "#     result = tf.transpose(result)\n",
        "\n",
        "#     return result\n",
        "\n",
        "# # ------------------------------------------------------\n",
        "# #                 wavelets\n",
        "# def rickerWavelet(tfType, scale, sampleCount):\n",
        "#     def waveEquation(time):\n",
        "#         time = tf.cast(time, tfType)\n",
        "\n",
        "#         tSquare = time ** 2.\n",
        "#         sigma   = 1.\n",
        "#         sSquare = sigma ** 2.\n",
        "\n",
        "#         # _1 = 2 / ((3 * a) ** .5 * np.pi ** .25)\n",
        "#         _1a = (3. * sigma) ** .5\n",
        "#         _1b = np.pi ** .25\n",
        "#         _1 = 2. / (_1a * _1b)\n",
        "\n",
        "#         # _2 = 1 - t**2 / a**2\n",
        "#         _2 = 1. - tSquare / sSquare\n",
        "\n",
        "#         # _3 = np.exp(-(t**2) / (2 * a ** 2))\n",
        "#         _3a = -1. * tSquare\n",
        "#         _3b = 2. * sSquare\n",
        "#         _3 = tf.exp(_3a / _3b)\n",
        "\n",
        "#         return _1 * _2 * _3\n",
        "\n",
        "#     return waveletHelper(tfType, scale, sampleCount, waveEquation)\n",
        "\n",
        "# def mortletWavelet(tfType, scale, sampleCount):\n",
        "#     def waveEquation(time):\n",
        "#         return tf.exp(-1. * time ** 2. / 2.) * tf.cos(5. * time) # https://www.mathworks.com/help/wavelet/ref/morlet.html\n",
        "\n",
        "#     return waveletHelper(tfType, scale, sampleCount, waveEquation)\n",
        "\n",
        "# def waveletHelper(tfType, scale, sampleCount, waveEquation):\n",
        "#     scale         = tf.cast(scale, tfType)\n",
        "#     sampleCount   = tf.cast(sampleCount, tfType)\n",
        "#     unscaledTimes = tf.cast(tf.range(tf.cast(sampleCount, tf.int64)), tfType) - (sampleCount - 1.) / 2.\n",
        "#     times         = unscaledTimes / scale\n",
        "#     wav           = waveEquation(times)\n",
        "#     wav           = wav * scale ** -.5\n",
        "#     return wav\n",
        "\n",
        "# # ------------------------------------------------------\n",
        "# #                    helpers\n",
        "# def conv1DWavelet(tfType, wav, waveletWidth, waveletEquation):\n",
        "#     kernelSamples = waveletWidth * 10\n",
        "#     kernel = waveletEquation(tfType, waveletWidth, kernelSamples)\n",
        "#     kernel = tf.reverse(kernel, [0])\n",
        "#     kernel = tf.reshape(kernel, tf.stack([kernelSamples,1,1,1]))\n",
        "\n",
        "#     conv = tf.nn.convolution(wav, kernel, 'SAME')\n",
        "#     conv = tf.squeeze(tf.squeeze(conv))\n",
        "\n",
        "#     return conv"
      ],
      "metadata": {
        "id": "aeYnJ67ZWMEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func(a):\n",
        "  a = tf.reshape(4, 1)\n",
        "\n",
        "a = tf.reshape(tf.range(0, 3)"
      ],
      "metadata": {
        "id": "mazS8ba-5ueT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied CW_SSIM (https://github.com/nickgeoca/cwt-tensorflow)\n",
        "\"\"\"\n",
        "Continuous Wavelet Transforms\n",
        "    Parameters\n",
        "    ----------\n",
        "    wav:      matrix - float32 - shape (N,)\n",
        "    widthCwt: scalar - int32\n",
        "    Returns\n",
        "    -------\n",
        "    output:   matrix - float32 - shape (widthCwt, N)\n",
        "\"\"\"\n",
        "def cwtRicker(tfType, wav, widthCwt):  return cwt(tfType, wav, widthCwt, rickerWavelet)\n",
        "def cwtMortlet(tfType, wav, widthCwt): return cwt(tfType, wav, widthCwt, mortletWavelet)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "\n",
        "@tf.function\n",
        "def cwt(tfType, wav, widthCwt, wavelet):\n",
        "    print(tf.shape(wav), tf.shape(widthCwt))\n",
        "\n",
        "    length = wav.shape[0]\n",
        "    wav = tf.cast(wav, tfType)\n",
        "    wav = tf.reshape(wav, tf.TensorShape([1,length,1,1]))\n",
        "\n",
        "    # While loop functions\n",
        "    def body(i, m):\n",
        "        v = conv1DWavelet(tfType, wav, i, wavelet)\n",
        "        v = tf.reshape(v, [length, 1])\n",
        "\n",
        "        m = tf.concat([m,v], 1)\n",
        "\n",
        "        return [1 + i, m]\n",
        "\n",
        "    def cond_(i, m):\n",
        "        return tf.less_equal(i, widthCwt)\n",
        "\n",
        "    # Initialize and run while loop\n",
        "    emptyCwtMatrix = tf.zeros([length, 0], tfType)\n",
        "    i = tf.constant(1)\n",
        "    _, result = tf.while_loop(\n",
        "            cond_,\n",
        "            body,\n",
        "            [i, emptyCwtMatrix],\n",
        "            shape_invariants=[i.get_shape(), tf.TensorShape([length, None])],\n",
        "            back_prop=False,\n",
        "            parallel_iterations=1024,\n",
        "            )\n",
        "    result = tf.transpose(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "# ------------------------------------------------------\n",
        "#                 wavelets\n",
        "@tf.function\n",
        "def rickerWavelet(tfType, scale, sampleCount):\n",
        "    def waveEquation(time):\n",
        "        time = tf.cast(time, tfType)\n",
        "\n",
        "        tSquare = time ** 2.\n",
        "        sigma   = 1.\n",
        "        sSquare = sigma ** 2.\n",
        "\n",
        "        # _1 = 2 / ((3 * a) ** .5 * np.pi ** .25)\n",
        "        _1a = (3. * sigma) ** .5\n",
        "        _1b = np.pi ** .25\n",
        "        _1 = 2. / (_1a * _1b)\n",
        "\n",
        "        # _2 = 1 - t**2 / a**2\n",
        "        _2 = 1. - tSquare / sSquare\n",
        "\n",
        "        # _3 = np.exp(-(t**2) / (2 * a ** 2))\n",
        "        _3a = -1. * tSquare\n",
        "        _3b = 2. * sSquare\n",
        "        _3 = tf.exp(_3a / _3b)\n",
        "\n",
        "        return _1 * _2 * _3\n",
        "\n",
        "    return waveletHelper(tfType, scale, sampleCount, waveEquation)\n",
        "\n",
        "def mortletWavelet(tfType, scale, sampleCount):\n",
        "    def waveEquation(time):\n",
        "        return tf.exp(-1. * time ** 2. / 2.) * tf.cos(5. * time) # https://www.mathworks.com/help/wavelet/ref/morlet.html\n",
        "\n",
        "    return waveletHelper(tfType, scale, sampleCount, waveEquation)\n",
        "\n",
        "@tf.function\n",
        "def waveletHelper(tfType, scale, sampleCount, waveEquation):\n",
        "    scale         = tf.cast(scale, tfType)\n",
        "    sampleCount   = tf.cast(sampleCount, tfType)\n",
        "    unscaledTimes = tf.cast(tf.range(tf.cast(sampleCount, tf.int64)), tfType) - (sampleCount - 1.) / 2.\n",
        "    times         = unscaledTimes / scale\n",
        "    wav           = waveEquation(times)\n",
        "    wav           = wav * scale ** -.5\n",
        "    return wav\n",
        "\n",
        "# ------------------------------------------------------\n",
        "#                    helpers\n",
        "@tf.function\n",
        "def conv1DWavelet(tfType, wav, waveletWidth, waveletEquation):\n",
        "    kernelSamples = waveletWidth * 10\n",
        "    kernel = waveletEquation(tfType, waveletWidth, kernelSamples)\n",
        "    kernel = tf.reverse(kernel, [0])\n",
        "    kernel = tf.reshape(kernel, tf.stack([kernelSamples,1,1,1]))\n",
        "\n",
        "    conv = tf.nn.convolution(wav, kernel, padding='SAME')\n",
        "    conv = tf.squeeze(tf.squeeze(conv))\n",
        "\n",
        "    return conv\n",
        "\n",
        "@tf.function\n",
        "def cw_ssim(y_true, y_pred, width=30):\n",
        "    \"\"\"Compute the complex wavelet SSIM (CW-SSIM) value from the reference\n",
        "    image to the target image.\n",
        "    Args:\n",
        "    target (str or PIL.Image): Input image to compare the reference image\n",
        "    to. This may be a PIL Image object or, to save time, an SSIMImage\n",
        "    object (e.g. the img member of another SSIM object).\n",
        "    width: width for the wavelet convolution (default: 30)\n",
        "    Returns:\n",
        "    Computed CW-SSIM float value.\n",
        "    \"\"\"\n",
        "\n",
        "    # print(wav.get_shape())\n",
        "    # if None in wav.get_shape():\n",
        "    #     # with tf.control_dependencies(checks):\n",
        "    #     wav = tf.identity(tf.shape(wav))\n",
        "\n",
        "    k = 0.01 #k (float): CW-SSIM configuration variable (default 0.01)\n",
        "    # Define a width for the wavelet convolution\n",
        "    widths = tf.range(1, width+1)\n",
        "\n",
        "    # Use the image data as arrays\n",
        "    sig1 = tf.reshape(y_pred, [-1])\n",
        "    sig2 = tf.reshape(y_true, [-1])\n",
        "\n",
        "    # Convolution\n",
        "\n",
        "    # cwtmatr1 = signal.cwt(sig1, signal.ricker, widths)\n",
        "    cwtmatr1 = cwtRicker(tf.float32, sig1, widths)\n",
        "    # cwtmatr2 = signal.cwt(sig2, signal.ricker, widths)\n",
        "    cwtmatr2 = cwtRicker(tf.float32, sig2, widths)\n",
        "\n",
        "    # Compute the first term\n",
        "    c1c2 = tf.math.multiply(abs(cwtmatr1), abs(cwtmatr2))\n",
        "    c1_2 = tf.math.square(abs(cwtmatr1))\n",
        "    c2_2 = tf.math.square(abs(cwtmatr2))\n",
        "    num_ssim_1 = 2 * tf.math.reduce_sum(c1c2, axis=0) + k\n",
        "    den_ssim_1 = tf.math.reduce_sum(c1_2, axis=0) + tf.math.reduce_sum(c2_2, axis=0) + k\n",
        "\n",
        "    # Compute the second term\n",
        "    c1c2_conj = tf.math.multiply(cwtmatr1, tf.math.conj(cwtmatr2))\n",
        "    num_ssim_2 = 2 * tf.math.abs(tf.math.reduce_sum(c1c2_conj, axis=0)) + k\n",
        "    den_ssim_2 = 2 * tf.math.reduce_sum(tf.math.abs(c1c2_conj), axis=0) + k\n",
        "\n",
        "    # Construct the result\n",
        "    ssim_map = (num_ssim_1 / den_ssim_1) * (num_ssim_2 / den_ssim_2)\n",
        "\n",
        "    # Average the per pixel results\n",
        "    index = tf.math.reduce_mean(ssim_map)\n",
        "\n",
        "    return tf.cast(index, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "efl2OSXFUkR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = [ 0.84]\n",
        "config_train = config(128, 2)\n",
        "# generating data\n",
        "data_gen = DataGenerator(\n",
        "    config_train['input_shape'],\n",
        "    config_train['batch_size'],\n",
        "    transform_function=None)\n",
        "[training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=config_train['patch_size'])\n",
        "[training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=config_train['patch_size'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugIHlUp_C6g1",
        "outputId": "1a080265-a2f8-4652-e7ec-c41bc55b3b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      [PATCHIFYING COMPLETED] output shape, slide:  (10486, 128, 128) 64 ; number of images:  214 , number of patches:  10486\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (1666, 128, 128) 64 ; number of images:  34 , number of patches:  1666\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (10486, 128, 128) 64 ; number of images:  214 , number of patches:  10486\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (1666, 128, 128) 64 ; number of images:  34 , number of patches:  1666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img1, img2 = [np.expand_dims(training_data[0],-1),], [np.expand_dims(training_data_labels[0],-1),]\n",
        "assert tf.reduce_all(tf.equal(tf.shape(img1), tf.shape(img2)))\n",
        "\n",
        "cw_ssim(tf.cast(img1,dtype=tf.float32), tf.cast(img2,dtype=tf.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r-DzqAEgE7Zs",
        "outputId": "7b06b101-e94c-4bc9-b3fb-681a221e9dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"Shape:0\", shape=(1,), dtype=int32) Tensor(\"Shape_1:0\", shape=(1,), dtype=int32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-dba57dc1470e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcw_ssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'while' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n      handler_func(fileobj, events)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 577, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 606, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 556, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-47-dba57dc1470e>\", line 4, in <module>\n      cw_ssim(tf.cast(img1,dtype=tf.float32), tf.cast(img2,dtype=tf.float32))\n    File \"<ipython-input-37-79017f7e6926>\", line 139, in cw_ssim\n      cwtmatr1 = cwtRicker(tf.float32, sig1, widths)\n    File \"<ipython-input-40-e0da93ce0c74>\", line 12, in cwtRicker\n      def cwtRicker(tfType, wav, widthCwt):  return cwt(tfType, wav, widthCwt, rickerWavelet)\n    File \"<ipython-input-46-9ed1a13e5ff9>\", line 40, in cwt\n      _, result = tf.while_loop(\nNode: 'while'\nThe second input must be a scalar, but it has shape [30]\n\t [[{{node while}}]] [Op:__inference_cw_ssim_1369]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Experiment B: testing wavelet SSIM\n",
        "\n",
        "alphas = [ 0.84]\n",
        "config_train = config(128, 2)\n",
        "# generating data\n",
        "data_gen = DataGenerator(\n",
        "    config_train['input_shape'],\n",
        "    config_train['batch_size'],\n",
        "    transform_function=None)\n",
        "[training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=config_train['patch_size'])\n",
        "[training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=config_train['patch_size'])\n",
        "\n",
        "tdata = data_gen.flow(*list(zip([training_data[:1000],training_data_labels[:1000]])))\n",
        "vdata = data_gen.flow(*list(zip([val_data[:100],val_data_labels[:100]])))\n",
        "\n",
        "if True:\n",
        "  res = np.zeros((1,3))\n",
        "  for alpha in alphas:\n",
        "    curr_loss = genSSIML1_loss(alpha = alpha)\n",
        "    print(\"EXPERIMENT 2: ALPHA - Current Alpha: \", alpha)\n",
        "    # tf.config.run_functions_eagerly(True) # to enable tensor to numpy\n",
        "\n",
        "    # learning rate reducer callback\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(verbose=True,factor=config_train['lr_decay_factor'],min_delta=0,patience=config_train['lr_decay_patience'],)\n",
        "\n",
        "    # TRAINING\n",
        "    model = build_model(kern_size=config_train['kern_size'], n_depth=config_train['n_depth'],shape=(config_train['input_shape'][0],config_train['input_shape'][0],1))\n",
        "    model = compile_model(model,config_train['learning_rate'], loss=curr_loss)\n",
        "    print(\"    [TRAINING] - Current Alpha: \", alpha)\n",
        "    history = model.fit(\n",
        "                        x=tdata,\n",
        "                        epochs = config_train['epoch'],\n",
        "                        validation_data = vdata\n",
        "                        , callbacks=[reduce_lr,tb_callback])\n",
        "    print(\"    [TRAINING FINISHED] \"+\"a: \"+str(alpha)+ \", results: val_ssim = \",np.amax(history.history['val_ssim']),\" ssim = \",np.amax(history.history['ssim']))\n",
        "    save_path = MAIN_PATH+\"/\"+MODEL_ROOT_NAME+\"/\"+\"a_valssim_\"+str(alpha)+ \"_\"+str(np.amax(history.history['val_ssim']))\n",
        "    print(\"    [SAVING MODEL] path is: \",save_path)\n",
        "    model.save(save_path)\n",
        "    display_training_stats(history,save_path, 'loss', 'Epoch')\n",
        "    display_training_stats(history,save_path, 'ssim', 'Epoch')\n",
        "    display_training_stats(history,save_path, 'psnr', 'Epoch')\n",
        "\n",
        "    res = np.concatenate((res, [[alpha,np.amax(history.history['ssim']),np.amax(history.history['val_ssim'])]]), axis=0)\n",
        "    print(\"[CURRENT RESULT: ]\")\n",
        "    for i in res:\n",
        "      print(i)\n",
        "\n",
        "    # evaluating SSIM on merged images\n",
        "    result = mergeAndPredict(model,val_data,val_data_labels,0, config_train['img_size'], config_train['patch_size'], config_train['patch_size']/2)\n",
        "    evalSSIM(result)\n",
        "    print(\"END TRIAL =========================================\")\n",
        "    print(\"\")\n",
        "    tf.keras.backend.clear_session()\n",
        "    del model\n",
        "\n",
        "  print(\"[TUNING  FINISHED, PRINTING RESULTS]\")\n",
        "  for i in res:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HK27J3hGCzZJ",
        "outputId": "28e7930d-3d36-462f-9591-ef83a53c2234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      [PATCHIFYING COMPLETED] output shape, slide:  (10486, 128, 128) 64 ; number of images:  214 , number of patches:  10486\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (1666, 128, 128) 64 ; number of images:  34 , number of patches:  1666\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (10486, 128, 128) 64 ; number of images:  214 , number of patches:  10486\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (1666, 128, 128) 64 ; number of images:  34 , number of patches:  1666\n",
            "1000\n",
            "100\n",
            "EXPERIMENT 2: ALPHA - Current Alpha:  0.84\n",
            "CUSTOM UNET INPUT SHAPE:  (128, 128, 1)\n",
            "n_dim 2\n",
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " down_level_0_no_0 (Conv2D)     (None, 128, 128, 32  320         ['input[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " down_level_0_no_1 (Conv2D)     (None, 128, 128, 32  9248        ['down_level_0_no_0[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_0 (MaxPooling2D)           (None, 64, 64, 32)   0           ['down_level_0_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " down_level_1_no_0 (Conv2D)     (None, 64, 64, 64)   18496       ['max_0[0][0]']                  \n",
            "                                                                                                  \n",
            " down_level_1_no_1 (Conv2D)     (None, 64, 64, 64)   36928       ['down_level_1_no_0[0][0]']      \n",
            "                                                                                                  \n",
            " max_1 (MaxPooling2D)           (None, 32, 32, 64)   0           ['down_level_1_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " middle_0 (Conv2D)              (None, 32, 32, 128)  73856       ['max_1[0][0]']                  \n",
            "                                                                                                  \n",
            " middle_2 (Conv2D)              (None, 32, 32, 64)   73792       ['middle_0[0][0]']               \n",
            "                                                                                                  \n",
            " up_sampling2d_28 (UpSampling2D  (None, 64, 64, 64)  0           ['middle_2[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenate)   (None, 64, 64, 128)  0           ['up_sampling2d_28[0][0]',       \n",
            "                                                                  'down_level_1_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_1_no_0 (Conv2D)       (None, 64, 64, 64)   73792       ['concatenate_28[0][0]']         \n",
            "                                                                                                  \n",
            " up_level_1_no_2 (Conv2D)       (None, 64, 64, 32)   18464       ['up_level_1_no_0[0][0]']        \n",
            "                                                                                                  \n",
            " up_sampling2d_29 (UpSampling2D  (None, 128, 128, 32  0          ['up_level_1_no_2[0][0]']        \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_29 (Concatenate)   (None, 128, 128, 64  0           ['up_sampling2d_29[0][0]',       \n",
            "                                )                                 'down_level_0_no_1[0][0]']      \n",
            "                                                                                                  \n",
            " up_level_0_no_0 (Conv2D)       (None, 128, 128, 32  18464       ['concatenate_29[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " up_level_0_no_2 (Conv2D)       (None, 128, 128, 32  9248        ['up_level_0_no_0[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 128, 128, 1)  33          ['up_level_0_no_2[0][0]']        \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 128, 128, 1)  0           ['conv2d_14[0][0]',              \n",
            "                                                                  'input[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 128, 128, 1)  0           ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 332,641\n",
            "Trainable params: 332,641\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "    [TRAINING] - Current Alpha:  0.84\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-2ea03971fe4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         , callbacks=[reduce_lr,tb_callback])\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    [TRAINING FINISHED] \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"a: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\", results: val_ssim = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_ssim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" ssim = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ssim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAIN_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mMODEL_ROOT_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"a_valssim_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_ssim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-8-85e3df58d660>\", line 88, in SSIM_L1_loss  *\n        ssim_partial = 1-((cw_ssim(y_true, y_pred)+1)*0.5)\n    File \"<ipython-input-39-192753b72258>\", line 206, in cw_ssim  *\n        return cw_ssim_impl(img1, img2)\n    File \"<ipython-input-39-192753b72258>\", line 152, in cw_ssim_impl  *\n        cwtmatr1 = cwtRicker(tf.float32, sig1, widths)\n    File \"<ipython-input-42-b40d9103521b>\", line 13, in cwtRicker  *\n        def cwtRicker(tfType, wav, widthCwt):  return cwt(tfType, wav, widthCwt, rickerWavelet)\n    File \"<ipython-input-42-b40d9103521b>\", line 22, in cwt  *\n        wav = tf.reshape(wav, tf.TensorShape([1,length,1,1]))\n\n    ValueError: Tried to convert 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape (1, None, 1, 1) to a Tensor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Evaluate SSIML1, SSIMVar, SSIMVarL1 losses\n",
        "# The goal of this experiment is to determine whether SSIMVarL1 outperforms other Losses.\n",
        "\n",
        "# **Methods**: run a script which runs a model over 3 losses with same configuration.\n",
        "\n",
        "# - Epoch: 20, learning rate: 5e-5, batch size: 64, patch size: 128, overlap: half, depth: 2\n",
        "# - Perform SSIM evaluation on merged images at the end.\n",
        "\n",
        "losses = {\"genSSIML1_loss\": genSSIML1_loss,\n",
        "          \"genSSIMVar_loss\": genSSIMVar_loss,\n",
        "          \"genSSIMVarL1_loss\": genSSIMVarL1_loss,\n",
        "          }\n",
        "\n",
        "if False:\n",
        "  res = np.zeros((1,3))\n",
        "  for genLoss in losses.keys():\n",
        "    curr_loss = losses[genLoss](alpha = 0.84)\n",
        "    print(\"EXPERIMENT 1: LOSSES - Current Loss: \", genLoss)\n",
        "    config_train = config(128, 2)\n",
        "\n",
        "    # learning rate reducer callback\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(verbose=True,factor=config_train['lr_decay_factor'],min_delta=0,patience=config_train['lr_decay_patience'],)\n",
        "\n",
        "    # generating data\n",
        "    data_gen = DataGenerator(\n",
        "        config_train['input_shape'],\n",
        "        config_train['batch_size'],\n",
        "        transform_function=None)\n",
        "    [training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=config_train['patch_size'])\n",
        "    [training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=config_train['patch_size'])\n",
        "    tdata = data_gen.flow(*list(zip([training_data,training_data_labels])))\n",
        "    vdata = data_gen.flow(*list(zip([val_data,val_data_labels])))\n",
        "    # TRAINING\n",
        "    model = build_model(kern_size=config_train['kern_size'], n_depth=config_train['n_depth'],shape=(config_train['input_shape'][0],config_train['input_shape'][0],1))\n",
        "    model = compile_model(model,config_train['learning_rate'], loss=curr_loss)\n",
        "    print(\"    [TRAINING] - Current Loss: \", genLoss)\n",
        "    history = model.fit(\n",
        "                        x=tdata,\n",
        "                        epochs = config_train['epoch'],\n",
        "                        validation_data = vdata\n",
        "                        , callbacks=[reduce_lr,tb_callback])\n",
        "    print(\"    [TRAINING FINISHED] \"+\"p: \"+ genLoss + \", results: val_ssim = \",np.amax(history.history['val_ssim']),\" ssim = \",np.amax(history.history['ssim']))\n",
        "    save_path = MAIN_PATH+\"/\"+MODEL_ROOT_NAME+\"/\"+\"p_d_valssim_\"+genLoss+\"_\"+str(np.amax(history.history['val_ssim']))\n",
        "    print(\"    [SAVING MODEL] path is: \",save_path)\n",
        "    model.save(save_path)\n",
        "    display_training_stats(history,save_path, 'loss', 'Epoch')\n",
        "    display_training_stats(history,save_path, 'ssim', 'Epoch')\n",
        "    display_training_stats(history,save_path, 'psnr', 'Epoch')\n",
        "\n",
        "    res = np.concatenate((res, [[genLoss,np.amax(history.history['ssim']),np.amax(history.history['val_ssim'])]]), axis=0)\n",
        "    print(\"[CURRENT RESULT: ]\")\n",
        "    for i in res:\n",
        "      print(i)\n",
        "\n",
        "    # evaluating SSIM on merged images\n",
        "    result = mergeAndPredict(model,val_data,val_data_labels,0, config_train['img_size'], config_train['patch_size'], config_train['patch_size']/2)\n",
        "    evalSSIM(result)\n",
        "    print(\"END TRIAL =========================================\")\n",
        "    print(\"\")\n",
        "    tf.keras.backend.clear_session()\n",
        "    del training_data, val_data,tdata,vdata, training_data_labels, val_data_labels, model\n",
        "\n",
        "  print(\"[TUNING  FINISHED, PRINTING RESULTS]\")\n",
        "  for i in res:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "MslDodcLv8rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRAIN PLAYGROUND**\n",
        "To test:\n",
        "- SSIM smaller filter: 3,5\n",
        "- SSIM_mov_mae loss"
      ],
      "metadata": {
        "id": "Hvh8HrVwxzLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test train, smaller SSIM filter size, zero overlap\n",
        "config_train = config(64, 4)\n",
        "config_train['learning_rate'] = 1e-4\n",
        "# learning rate reducer callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(verbose=True,factor=config_train['lr_decay_factor'],min_delta=0,patience=config_train['lr_decay_patience'],)\n",
        "\n",
        "# generating data\n",
        "data_gen = DataGenerator(\n",
        "    config_train['input_shape'],\n",
        "    config_train['batch_size'],\n",
        "    transform_function=None)\n",
        "[training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=config_train['patch_size'])\n",
        "[training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=config_train['patch_size'])\n",
        "tdata = data_gen.flow(*list(zip([training_data,training_data_labels])))\n",
        "vdata = data_gen.flow(*list(zip([val_data,val_data_labels])))\n",
        "# TRAINING\n",
        "model = build_model(kern_size=config_train['kern_size'], n_depth=config_train['n_depth'],shape=(config_train['input_shape'][0],config_train['input_shape'][0],1))\n",
        "model = compile_model(model,config_train['learning_rate'])\n",
        "print(\"    [TRAINING] \"+\"patch size: \"+ str(config_train['patch_size']) + \", current depth: \"+str(config_train['n_depth']))\n",
        "history = model.fit(\n",
        "                    x=tdata,\n",
        "                    epochs = config_train['epoch'],\n",
        "                    validation_data = vdata\n",
        "                    , callbacks=[reduce_lr,tb_callback])\n",
        "print(\"    [TRAINING FINISHED] \"+\"p: \"+ str(config_train['patch_size']) + \", d: \"+str(config_train['n_depth'])+ \", results: val_ssim = \",np.amax(history.history['val_ssim']),\" ssim = \",np.amax(history.history['ssim']))\n",
        "save_path = MAIN_PATH+\"/\"+MODEL_ROOT_NAME+\"/\"+\"p_d_valssim_\"+str(config_train['patch_size'])+\"_\"+str(config_train['n_depth'])+\"_\"+str(np.amax(history.history['val_ssim']))\n",
        "print(\"    [SAVING MODEL] path is: \",save_path)\n",
        "model.save(save_path)\n",
        "display_training_stats(history,save_path, 'loss', 'Epoch')\n",
        "display_training_stats(history,save_path, 'ssim', 'Epoch')\n",
        "display_training_stats(history,save_path, 'psnr', 'Epoch')"
      ],
      "metadata": {
        "id": "aQooJy7JwTOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "apqQjOjF-HbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = genSSIMVar_loss(alpha=0.3)\n",
        "# test new loss, no overlap, ssim filter 3\n",
        "config_train = config(64, 3)\n",
        "config_train['learning_rate'] = 1e-4\n",
        "# learning rate reducer callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(verbose=True,factor=config_train['lr_decay_factor'],min_delta=0,patience=config_train['lr_decay_patience'],)\n",
        "\n",
        "# generating data\n",
        "data_gen = DataGenerator(\n",
        "    config_train['input_shape'],\n",
        "    config_train['batch_size'],\n",
        "    transform_function=None)\n",
        "[training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=config_train['patch_size'])\n",
        "[training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=config_train['patch_size'])\n",
        "tdata = data_gen.flow(*list(zip([training_data,training_data_labels])))\n",
        "vdata = data_gen.flow(*list(zip([val_data,val_data_labels])))\n",
        "# TRAINING\n",
        "model = build_model(kern_size=config_train['kern_size'], n_depth=config_train['n_depth'],shape=(config_train['input_shape'][0],config_train['input_shape'][0],1))\n",
        "model = compile_model(model,config_train['learning_rate'], loss=test_loss)\n",
        "print(\"    [TRAINING] \"+\"patch size: \"+ str(config_train['patch_size']) + \", current depth: \"+str(config_train['n_depth']))\n",
        "history = model.fit(\n",
        "                    x=tdata,\n",
        "                    epochs = config_train['epoch'],\n",
        "                    validation_data = vdata\n",
        "                    , callbacks=[reduce_lr,tb_callback])\n",
        "print(\"    [TRAINING FINISHED] \"+\"p: \"+ str(config_train['patch_size']) + \", d: \"+str(config_train['n_depth'])+ \", results: val_ssim = \",np.amax(history.history['val_ssim']),\" ssim = \",np.amax(history.history['ssim']))\n",
        "save_path = MAIN_PATH+\"/\"+MODEL_ROOT_NAME+\"/\"+\"p_d_valssim_\"+str(config_train['patch_size'])+\"_\"+str(config_train['n_depth'])+\"_\"+str(np.amax(history.history['val_ssim']))\n",
        "print(\"    [SAVING MODEL] path is: \",save_path)\n",
        "model.save(save_path)\n",
        "display_training_stats(history,save_path, 'loss', 'Epoch')\n",
        "display_training_stats(history,save_path, 'ssim', 'Epoch')\n",
        "display_training_stats(history,save_path, 'psnr', 'Epoch')"
      ],
      "metadata": {
        "id": "_4K5yKUI0BHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n"
      ],
      "metadata": {
        "id": "jGzpC_gWLycJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [training_data, val_data] = load_data(MAIN_PATH+DATA_PATH, patch=None)\n",
        "# [training_data_labels,val_data_labels] = load_data(MAIN_PATH+LABEL_PATH,patch=None)\n"
      ],
      "metadata": {
        "id": "sKzSQo0fOxQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # print(np.amax(val_data[0]))\n",
        "# plot(val_data[0],'test',hist=True)\n",
        "# [training_dataa, val_dataa] = load_data(MAIN_PATH+DATA_PATH, patch=32)\n",
        "# print(\"TEST AFTER\", np.amax(training_dataa))\n",
        "# test = normalize(merge_patches(val_dataa[0:961,:,:],512,32,16))\n",
        "# plot(test,'test',hist=True)\n"
      ],
      "metadata": {
        "id": "tBE8nETFO6B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result = predictAndPlot(model,val_data,val_data_labels, 0, 961, isPlot=True, merge=True, normalize=False, clip=False)\n",
        "# evalSSIM(result)"
      ],
      "metadata": {
        "id": "QD9AhngwMLFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def revert_img(img,original_size, patch_shape, slide):\n",
        "  # reverts original image and removes overlaps by splitting overlap over 2 images\n",
        "  step = int(patch_shape-slide)\n",
        "  reconstructed_arr = np.zeros((original_size,original_size))\n",
        "  for x in range(img.shape[0]):\n",
        "    for y in range(img.shape[1]):\n",
        "      start_x = int(slide/2)\n",
        "      start_y = int(slide/2)\n",
        "      end_x = 0\n",
        "      end_y = 0\n",
        "      if x == 0:\n",
        "        start_x = 0\n",
        "        end_x = int(slide/2)\n",
        "      if y == 0:\n",
        "        start_y = 0\n",
        "        end_y = int(slide/2)\n",
        "      if x == img.shape[0]-1: end_x = int(slide/2)\n",
        "      if y == img.shape[1]-1: end_y = int(slide/2)\n",
        "      x_pos, y_pos = x * step + start_x, y * step + start_y\n",
        "      # print('x_pos, start_x: ',x_pos, start_x)\n",
        "      # print('x_pos + step + end_x, start_x+step+end_x: ',x_pos + step + end_x, start_x+step+end_x)\n",
        "      # print('y_pos, start_y: ',y_pos, start_y)\n",
        "      # print('y_pos + step + end_y,  start_y+step+end_y: ',y_pos + step + end_y,  start_y+step+end_y)\n",
        "      # print(\"end===============x,y: \",x,y)\n",
        "      reconstructed_arr[x_pos : x_pos + step + end_x, y_pos : y_pos + step + end_y] = img[x, y, start_x:start_x+step+end_x, start_y:start_y+step+end_y]\n",
        "  return reconstructed_arr\n",
        "\n",
        "def merge_patches(img, original_size, patch_shape, slide):\n",
        "  #  merging patches, img is a 3D array of stacked patches\n",
        "  print(\"merging patches, img shape: \", img.shape)\n",
        "  row_len = int(math.sqrt(img.shape[0]))\n",
        "  patches = np.zeros((row_len,row_len,patch_shape,patch_shape))\n",
        "  print(img.shape)\n",
        "  print(patches.shape)\n",
        "  for r in range(row_len):\n",
        "      patches[r,:,:,:] = img[r*row_len:r*row_len+row_len,:,:]\n",
        "  # plt.figure(5)\n",
        "  # plt.imshow(patches[1,1,:,:])\n",
        "  return revert_img(patches,original_size,patch_shape, slide)\n",
        "\n",
        "\n",
        "def MSE(img1, img2):\n",
        "  # comparing one processed and preprocessed image\n",
        "  squared_diff = (img1 -img2) ** 2\n",
        "  summed = np.sum(squared_diff)\n",
        "  num_pix = img1.shape[0] * img1.shape[1] #img1 and 2 should have same shape\n",
        "  err = summed / num_pix\n",
        "  return err\n",
        "\n",
        "def normalize(img):\n",
        "  # Normalizing images between 0 and 1 and preserving distribution\n",
        "  img_norm = (img - np.amin(img))/( np.amax(img)- np.amin(img))\n",
        "  return img_norm\n",
        "\n",
        "def plot(img, label, hist=False, size=(10,10)):\n",
        "  plt.figure(figsize = size)\n",
        "  if hist==True:\n",
        "    plt.hist(img.flatten(), bins=120)\n",
        "  else: plt.imshow(img,cmap=\"gray\")\n",
        "  plt.title(label)\n",
        "\n",
        "def normalize_between_zero_and_one(m):\n",
        "    max_val, min_val = m.max(), m.min()\n",
        "    diff = max_val - min_val\n",
        "    return (m - min_val) / diff if diff > 0 else np.zeros_like(m)\n",
        "\n",
        "def normalize_percentile(img, pmin=0.1, pmax=99.9, clip = True):\n",
        "  eps=1e-20 # avoid zero division\n",
        "  mi = np.percentile(img,pmin,axis=None,keepdims=True)\n",
        "  # mi = np.amin(img)\n",
        "  # print(\"mi\",mi)\n",
        "  ma = np.percentile(img,pmax,axis=None,keepdims=True)\n",
        "  if clip == True: return np.clip((img - mi) / ( ma - mi + eps ), 0, 1)\n",
        "  return (img - mi) / ( ma - mi + eps )\n",
        "\n",
        "# def ssim(y_true, y_pred):\n",
        "#     '''\n",
        "#     Computes the structural similarity index between two images. Note that the\n",
        "#     maximum signal value is assumed to be 1.\n",
        "#     '''\n",
        "\n",
        "#     return ssim2(y_true, y_pred,1,k2=0.05)\n",
        "# our SSIM loss\n",
        "from tensorflow.image import ssim as ssim2\n",
        "def ssim_our(y_true, y_pred):\n",
        "  return ssim2(y_true, y_pred,1,k2=0.05)\n",
        "\n",
        "def _get_gaussian_kernel(dim, size, sigma):\n",
        "    k = size // 2\n",
        "    normal = tfp.distributions.Normal(0.0, sigma)\n",
        "    p = normal.prob(tf.range(-k, size - k, dtype=tf.float32))\n",
        "\n",
        "    indices = [chr(i) for i in range(105, 105 + dim)]\n",
        "    eq = ','.join(indices) + '->' + ''.join(indices)\n",
        "    kernel = tf.einsum(eq, *([p] * dim))\n",
        "    kernel /= tf.reduce_sum(kernel)\n",
        "    kernel = kernel[..., tf.newaxis, tf.newaxis]\n",
        "\n",
        "    return kernel\n",
        "\n",
        "# RCAN ssim\n",
        "import keras.backend as K\n",
        "def ssim_rcan(y_true, y_pred):\n",
        "    '''\n",
        "    Computes the structural similarity index between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    References\n",
        "    ----------\n",
        "    Image Quality Assessment: From Error Visibility to Structural Similarity\n",
        "    https://doi.org/10.1109/TIP.2003.819861\n",
        "    '''\n",
        "\n",
        "    c1 = 0.01 ** 2\n",
        "    c2 = 0.03 ** 2\n",
        "\n",
        "    dim = K.ndim(y_pred) - 2\n",
        "    if dim not in (2, 3):\n",
        "        raise NotImplementedError(f'{dim}D SSIM is not suported')\n",
        "\n",
        "    num_channels = K.int_shape(y_pred)[-1]\n",
        "\n",
        "    kernel = _get_gaussian_kernel(dim, 11, 1.5)\n",
        "    conv = K.conv2d if dim == 2 else K.conv3d\n",
        "\n",
        "    def average(x):\n",
        "        # channel-wise weighted average using the Gaussian kernel\n",
        "        return tf.concat(\n",
        "            [conv(y, kernel) for y in tf.split(x, num_channels, axis=-1)],\n",
        "            axis=-1)\n",
        "\n",
        "    ux = average(y_true)\n",
        "    uy = average(y_pred)\n",
        "\n",
        "    a = ux * uy\n",
        "    b = K.square(ux) + K.square(uy)\n",
        "    c = average(y_true * y_pred)\n",
        "    d = average(K.square(y_true) + K.square(y_pred))\n",
        "\n",
        "    lum = (2 * a + c1) / (b + c1)\n",
        "    cs = (2 * (c - a) + c2) / (d - b + c2)\n",
        "\n",
        "    return K.mean(K.batch_flatten(lum * cs), axis=-1)\n",
        "\n",
        "def predictAndPlot(model,input,val, start, end, isPlot=True, merge=True, normalize=False, clip=False):\n",
        "  pred = model.predict(input[start:end])\n",
        "  print(\"shape of predicted image: \", np.shape(pred))\n",
        "  if len(np.shape(input)) > 3:\n",
        "    if merge:\n",
        "      result = [merge_patches(input[start:end,:,:,0],512,64,0), merge_patches(pred[start:end,:,:,0],512,64,0), merge_patches(val[start:end,:,:,0],512,64,0)]\n",
        "    else: result = [input[start:end,:,:,0], pred[start:end,:,:,0],val[start:end,:,:,0]]\n",
        "  else:\n",
        "    if merge:\n",
        "      result = [merge_patches(input[start:end,:,:],512,64,0), merge_patches(pred[start:end,:,:,0],512,64,0), merge_patches(val[start:end,:,:],512,64,0)]\n",
        "    else: result = [input[start:end,:,:], pred[start:end,:,:,0], val[start:end,:,:]]\n",
        "  if normalize: result = [normalize_percentile(m) for m in result]\n",
        "  if clip: result = [np.clip(255 * m, 0, 255).astype('uint8') for m in result]\n",
        "  res_img =  np.concatenate((np.concatenate((result[0],result[1]),axis=1),result[2]),axis=1)\n",
        "  plot(res_img, \"input, restored, gt\", size=(30,30))\n",
        "  # plot(result[0],'input 25x')\n",
        "  # plot(result[1],'restored')\n",
        "  # plot(result[2],'gt 40x')\n",
        "  if isPlot:\n",
        "    plot(result[0],'hist input 25x',hist=True)\n",
        "    plot(result[1],'hist restored',hist=True)\n",
        "    plot(result[2],'hist gt 40x',hist=True)\n",
        "  return result\n",
        "\n",
        "def evalSSIM(result):\n",
        "  result1 = [np.expand_dims(m,-1) for m in result]\n",
        "  raw = tf.convert_to_tensor(np.expand_dims(result1[0],0),dtype=np.float32)\n",
        "  rest = tf.convert_to_tensor(np.expand_dims(result1[1],0),dtype=np.float32)\n",
        "  gt = tf.convert_to_tensor(np.expand_dims(result1[2],0),dtype=np.float32)\n",
        "  # Metric evaluation\n",
        "  print('raw vs gt=======================')\n",
        "  print(\"Our SSIM between raw and ground truth: \",ssim_our(gt,raw).numpy())\n",
        "  print(\"RCAN paper SSIM between raw and ground truth: \",ssim_rcan(gt, raw).numpy())\n",
        "  print('predicted vs gt=======================')\n",
        "  print(\"Our SSIM between predicted and ground truth: \",ssim_our(gt, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and ground truth: \",ssim_rcan(gt,rest).numpy())\n",
        "  print('predicted vs raw=======================')\n",
        "  print(\"Our SSIM between predicted and raw: \",ssim_our(raw, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and raw: \",ssim_rcan(raw, rest).numpy())"
      ],
      "metadata": {
        "id": "iO-rSGKhMVvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp\n",
        "result = predictAndPlot(model,expandLastDim(val_data), expandLastDim(val_data_labels), 0, 64, isPlot=True, merge=True, normalize=False, clip=False)\n",
        "evalSSIM(result)"
      ],
      "metadata": {
        "id": "LxNKJi3L6D_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(val_data))"
      ],
      "metadata": {
        "id": "7vNyttLT6YFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "gpu2",
      "language": "python",
      "name": "gpu2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}