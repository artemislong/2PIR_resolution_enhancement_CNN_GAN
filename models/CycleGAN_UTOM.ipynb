{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GCBJetaJSeu",
        "outputId": "bb2fe4e1-6c6c-4493-9091-14db10c74893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons\n",
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcQLrPBw_DJ4",
        "outputId": "472e5118-7a46-4446-ad66-fe75576f4099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import_ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import_ipynb) (1.15.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.11.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import_ipynb) (3.8.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AXH0J9__Nrz"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "\n",
        "# wandb.init(project=\"CycleGAN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtDY2Uos9Iy6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tifffile as tiff\n",
        "import re\n",
        "import zipfile\n",
        "import scipy.io\n",
        "import random\n",
        "from tifffile import imread\n",
        "import argparse\n",
        "import json\n",
        "import pickle as pkl\n",
        "import imageio\n",
        "import itertools\n",
        "import jsonschema\n",
        "import keras\n",
        "import pathlib\n",
        "from tensorflow.image import ssim as ssim2\n",
        "from tensorflow.image import ssim_multiscale as msssim\n",
        "from tqdm.utils import IS_WIN\n",
        "from tqdm.keras import TqdmCallback as _TqdmCallback\n",
        "import functools\n",
        "import warnings\n",
        "import tqdm\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "from importlib import import_module\n",
        "from tensorflow import __version__ as _tf_version\n",
        "from tensorflow.python.client.device_lib import list_local_devices\n",
        "from packaging import version\n",
        "from keras.utils.conv_utils import normalize_tuple\n",
        "import tensorflow_probability as tfp\n",
        "import fractions\n",
        "import numexpr\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import constant_op\n",
        "import skimage\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRdHsJUDGtwp",
        "outputId": "928c44d6-a235-45cd-be9d-c80f03c99d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "importing Jupyter notebook from CARE_util.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Open drive dynamically; prefer drives that have 'drive' in their name.\n",
        "MAIN_DIR= r\"/objective_transfer/deep_learning\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(MAIN_DIR+'/CAREstd/')\n",
        "import import_ipynb\n",
        "from CARE_util import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWgQ8NZkXxKw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# autotune = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "PATCH_SIZE = 256\n",
        "\n",
        "# Define the standard image size.\n",
        "# orig_img_size = (512, 512,1)\n",
        "# Size of the random crops to be used during training.\n",
        "input_img_size = (PATCH_SIZE, PATCH_SIZE,1)\n",
        "# Weights initializer for the layers.\n",
        "# kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "kernel_init = \"glorot_uniform\"\n",
        "# Gamma initializer for instance normalization.\n",
        "# gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "gamma_init = \"glorot_uniform\"\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def revert_img(img,original_size, patch_shape, slide):\n",
        "  # reverts original image and removes overlaps by splitting overlap over 2 images\n",
        "  step = int(patch_shape-slide)\n",
        "  reconstructed_arr = np.zeros((original_size,original_size))\n",
        "  for x in range(img.shape[0]):\n",
        "    for y in range(img.shape[1]):\n",
        "      start_x = int(slide/2)\n",
        "      start_y = int(slide/2)\n",
        "      end_x = 0\n",
        "      end_y = 0\n",
        "      if x == 0:\n",
        "        start_x = 0\n",
        "        end_x = int(slide/2)\n",
        "      if y == 0:\n",
        "        start_y = 0\n",
        "        end_y = int(slide/2)\n",
        "      if x == img.shape[0]-1: end_x = int(slide/2)\n",
        "      if y == img.shape[1]-1: end_y = int(slide/2)\n",
        "      x_pos, y_pos = x * step + start_x, y * step + start_y\n",
        "      reconstructed_arr[x_pos : x_pos + step + end_x, y_pos : y_pos + step + end_y] = img[x, y, start_x:start_x+step+end_x, start_y:start_y+step+end_y]\n",
        "  return reconstructed_arr\n",
        "\n",
        "def merge_patches(img, original_size, patch_shape, slide):\n",
        "  #  merging patches, img is a 3D array of stacked patches\n",
        "  row_len = int(math.sqrt(img.shape[0]))\n",
        "  patches = np.zeros((row_len,row_len,patch_shape,patch_shape))\n",
        "  for r in range(row_len):\n",
        "      patches[r,:,:,:] = img[r*row_len:r*row_len+row_len,:,:]\n",
        "  return revert_img(patches,original_size,patch_shape, slide)\n",
        "\n",
        "\n",
        "# Metrics and losses\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    '''\n",
        "    Computs the peak signal-to-noise ratio between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    '''\n",
        "    psnr2 = tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "    return psnr2\n",
        "\n",
        "# our ssim config\n",
        "def ssim(y_true, y_pred):\n",
        "    return ssim2(y_true, y_pred, 1, filter_size=3, filter_sigma=0.5, k2=0.05)\n",
        "\n",
        "w = (0.0448, 0.2856, 0.3001, 0.2363, 0.1333)\n",
        "def mssim(y_true, y_pred):\n",
        "   return tf.image.ssim_multiscale(y_true, y_pred, 1, filter_size=11,power_factors=w, filter_sigma=1.5, k2=0.05)\n",
        "\n",
        "\n",
        "def mse(img1, img2):\n",
        "  # comparing one processed and preprocessed image\n",
        "  squared_diff = (img1 -img2) ** 2\n",
        "  summed = np.sum(squared_diff)\n",
        "  num_pix = img1.shape[0] * img1.shape[1] #img1 and 2 should have same shape\n",
        "  err = summed / num_pix\n",
        "  return err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLtsxneIR16-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "## Building blocks used in the CycleGAN generators and discriminators\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ReflectionPadding2D(layers.Layer):\n",
        "    \"\"\"Implements Reflection Padding as a layer.\n",
        "    Args:\n",
        "        padding(tuple): Amount of padding for the\n",
        "        spatial dimensions.\n",
        "    Returns:\n",
        "        A padded tensor with the same type as the input tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        padding_width, padding_height = self.padding\n",
        "        padding_tensor = [\n",
        "            [0, 0],\n",
        "            [padding_height, padding_height],\n",
        "            [padding_width, padding_width],\n",
        "            [0, 0],\n",
        "        ]\n",
        "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
        "\n",
        "\n",
        "def residual_block(\n",
        "    x,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"valid\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    dim = x.shape[-1]\n",
        "    input_tensor = x\n",
        "\n",
        "    x = ReflectionPadding2D()(input_tensor)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = activation(x)\n",
        "\n",
        "    x = ReflectionPadding2D()(x)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.add([input_tensor, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def downsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def upsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=kernel_init,\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2DTranspose(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Build the generators\n",
        "The generator consists of downsampling blocks: nine residual blocks\n",
        "and upsampling blocks. The structure of the generator is the following:\n",
        "```\n",
        "c7s1-64 ==> Conv block with `relu` activation, filter size of 7\n",
        "d128 ====|\n",
        "         |-> 2 downsampling blocks\n",
        "d256 ====|\n",
        "R256 ====|\n",
        "R256     |\n",
        "R256     |\n",
        "R256     |\n",
        "R256     |-> 9 residual blocks\n",
        "R256     |\n",
        "R256     |\n",
        "R256     |\n",
        "R256 ====|\n",
        "u128 ====|\n",
        "         |-> 2 upsampling blocks\n",
        "u64  ====|\n",
        "c7s1-3 => Last conv block with `tanh` activation, filter size of 7.\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_resnet_generator(\n",
        "    filters=64,\n",
        "    num_downsampling_blocks=2,\n",
        "    num_residual_blocks=9,\n",
        "    num_upsample_blocks=2,\n",
        "    gamma_initializer=gamma_init,\n",
        "    name=None,\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
        "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Downsampling\n",
        "    for _ in range(num_downsampling_blocks):\n",
        "        filters *= 2\n",
        "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(num_residual_blocks):\n",
        "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Upsampling\n",
        "    for _ in range(num_upsample_blocks):\n",
        "        filters //= 2\n",
        "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Final block\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
        "    x = layers.Conv2D(1, (7, 7), padding=\"valid\")(x)\n",
        "    x = layers.Activation(\"tanh\")(x)\n",
        "\n",
        "\n",
        "    # model_config = {'n_dim':2, 'n_depth':3, 'input_shape': input_img_size, 'kern_size':5, 'n_first':32, 'n_channel_out':1, 'residual':True, 'last_activation':'relu','batch_norm':False, 'dropout':0.0,}\n",
        "    # model = custom_unet(model_config['input_shape'], model_config['last_activation'], model_config['n_depth'], model_config['n_first'], (model_config['kern_size'],)*model_config['n_dim'], batch_norm=model_config['batch_norm'], dropout=model_config['dropout'], pool_size=(2,)*model_config['n_dim'], n_channel_out=model_config['n_channel_out'], residual=model_config['residual'])\n",
        "    model = keras.models.Model(img_input, x, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Build the discriminators\n",
        "The discriminators implement the following architecture:\n",
        "`C64->C128->C256->C512`\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_discriminator(\n",
        "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        (4, 4),\n",
        "        strides=(2, 2),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_initializer,\n",
        "    )(img_input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    num_filters = filters\n",
        "    for num_downsample_block in range(3):\n",
        "        num_filters *= 2\n",
        "        if num_downsample_block < 2:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(2, 2),\n",
        "            )\n",
        "        else:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(1, 1),\n",
        "            )\n",
        "\n",
        "    x = layers.Conv2D(\n",
        "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
        "    )(x)\n",
        "\n",
        "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the generators\n",
        "gen_G = get_resnet_generator(name=\"generator_G\")\n",
        "gen_F = get_resnet_generator(name=\"generator_F\")\n",
        "\n",
        "# Get the discriminators\n",
        "disc_X = get_discriminator(name=\"discriminator_X\")\n",
        "disc_Y = get_discriminator(name=\"discriminator_Y\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Build the CycleGAN model\n",
        "We will override the `train_step()` method of the `Model` class\n",
        "for training via `fit()`.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator_G,\n",
        "        generator_F,\n",
        "        discriminator_X,\n",
        "        discriminator_Y,\n",
        "        lambda_cycle=10.0,\n",
        "        lambda_identity=0.5,\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.gen_G = generator_G\n",
        "        self.gen_F = generator_F\n",
        "        self.disc_X = discriminator_X\n",
        "        self.disc_Y = discriminator_Y\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.lambda_identity = lambda_identity\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return (\n",
        "            self.disc_X(inputs),\n",
        "            self.disc_Y(inputs),\n",
        "            self.gen_G(inputs),\n",
        "            self.gen_F(inputs),\n",
        "        )\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        gen_G_optimizer,\n",
        "        gen_F_optimizer,\n",
        "        disc_X_optimizer,\n",
        "        disc_Y_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "        metrics\n",
        "    ):\n",
        "        super(CycleGan, self).compile(metrics=metrics)\n",
        "        self.gen_G_optimizer = gen_G_optimizer\n",
        "        self.gen_F_optimizer = gen_F_optimizer\n",
        "        self.disc_X_optimizer = disc_X_optimizer\n",
        "        self.disc_Y_optimizer = disc_Y_optimizer\n",
        "        self.generator_loss_fn = gen_loss_fn\n",
        "        self.discriminator_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
        "        self.identity_loss_fn = keras.losses.MeanAbsoluteError() # TODO: disable\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        # print(\"BATCH DATA: \", batch_data)\n",
        "        real_x, real_y = batch_data\n",
        "\n",
        "        # For CycleGAN, we need to calculate different\n",
        "        # kinds of losses for the generators and discriminators.\n",
        "        # We will perform the following steps here:\n",
        "        #\n",
        "        # 1. Pass real images through the generators and get the generated images\n",
        "        # 2. Pass the generated images back to the generators to check if we\n",
        "        #    we can predict the original image from the generated image.\n",
        "        # 3. Do an identity mapping of the real images using the generators.\n",
        "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
        "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
        "        # 6. Calculate the discriminators loss\n",
        "        # 7. Update the weights of the generators\n",
        "        # 8. Update the weights of the discriminators\n",
        "        # 9. Return the losses in a dictionary\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # print(\"1\")\n",
        "            fake_y = self.gen_G(real_x, training=True)\n",
        "            fake_x = self.gen_F(real_y, training=True)\n",
        "            # Cycle (x to fake y to fake x): x -> y -> x\n",
        "            cycled_x = self.gen_F(fake_y, training=True)\n",
        "            # Cycle (y to fake x to fake y) y -> x -> y\n",
        "            cycled_y = self.gen_G(fake_x, training=True)\n",
        "            # Identity mapping\n",
        "            same_x = self.gen_F(real_x, training=True)\n",
        "            same_y = self.gen_G(real_y, training=True)\n",
        "            # print(\"4\")\n",
        "            # Discriminator output\n",
        "            disc_real_x = self.disc_X(real_x, training=True)\n",
        "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
        "\n",
        "            disc_real_y = self.disc_Y(real_y, training=True)\n",
        "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
        "            # print(\"5\")\n",
        "            # Generator adverserial loss\n",
        "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
        "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
        "            # print(\"6\")\n",
        "            # Generator cycle loss\n",
        "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
        "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
        "            # print(\"7\")\n",
        "            # Generator identity loss\n",
        "            id_loss_G = (\n",
        "                self.identity_loss_fn(real_y, same_y)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "            id_loss_F = (\n",
        "                self.identity_loss_fn(real_x, same_x)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "            # print(\"8\")\n",
        "            # Total generator loss\n",
        "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
        "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
        "            # print(\"9\")\n",
        "            # Discriminator loss\n",
        "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
        "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
        "\n",
        "        # Get the gradients for the generators\n",
        "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
        "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
        "        # print(\"10\")\n",
        "        # Get the gradients for the discriminators\n",
        "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
        "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
        "        # print(\"11\")\n",
        "        # Update the weights of the generators\n",
        "        self.gen_G_optimizer.apply_gradients(\n",
        "            zip(grads_G, self.gen_G.trainable_variables)\n",
        "        )\n",
        "        self.gen_F_optimizer.apply_gradients(\n",
        "            zip(grads_F, self.gen_F.trainable_variables)\n",
        "        )\n",
        "        # print(\"12\")\n",
        "        # Update the weights of the discriminators\n",
        "        self.disc_X_optimizer.apply_gradients(\n",
        "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
        "        )\n",
        "        self.disc_Y_optimizer.apply_gradients(\n",
        "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"G_loss\": total_loss_G,\n",
        "            \"F_loss\": total_loss_F,\n",
        "            \"D_X_loss\": disc_X_loss,\n",
        "            \"D_Y_loss\": disc_Y_loss,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjaVaDYwGlk2"
      },
      "outputs": [],
      "source": [
        "# add arbitrary channel dimension and augments data on flow, also splits data into batches\n",
        "class DataGenerator:\n",
        "    '''\n",
        "    Generates batches of image pairs with real-time data augmentation.\n",
        "    Parameters\n",
        "    ----------\n",
        "    shape: tuple of int\n",
        "        Shape of batch images (excluding the channel dimension).\n",
        "    batch_size: int\n",
        "        Batch size.\n",
        "    transform_function: str or callable or None\n",
        "        Function used for data augmentation. Typically you will set\n",
        "        ``transform_function='rotate_and_flip'`` to apply combination of\n",
        "        randomly selected image rotation and flipping.  Alternatively, you can\n",
        "        specify an arbitrary transformation function which takes two input\n",
        "        images (source and target) and returns transformed images. If\n",
        "        ``transform_function=None``, no augmentation will be performed.\n",
        "    intensity_threshold: float\n",
        "        If ``intensity_threshold > 0``, pixels whose intensities are greater\n",
        "        than this threshold will be considered as foreground.\n",
        "    area_ratio_threshold: float between 0 and 1\n",
        "        If ``intensity_threshold > 0``, the generator calculates the ratio of\n",
        "        foreground pixels in a target patch, and rejects the patch if the ratio\n",
        "        is smaller than this threshold.\n",
        "    scale_factor: int != 0\n",
        "        Scale factor for the target patch size. Positive and negative values\n",
        "        mean up- and down-scaling respectively.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 shape,\n",
        "                 batch_size,\n",
        "                 transform_function='rotate_and_flip',\n",
        "                 intensity_threshold=0.0,\n",
        "                 area_ratio_threshold=0.0,\n",
        "                 scale_factor=1):\n",
        "        def rotate_and_flip(x, y, dim):\n",
        "            if dim == 2:\n",
        "                k = np.random.randint(0, 4)\n",
        "                x, y = [np.rot90(v, k=k) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    x, y = [np.fliplr(v) for v in (x, y)]\n",
        "                if np.random.random() < 0.5:\n",
        "                    deg = np.random.randint(0, high=25)\n",
        "                    x, y = [scipy.ndimage.interpolation.rotate(v, deg, axes=(1, 0), reshape=False, order=3, mode='constant', cval=0.0) for v in (x,y)]\n",
        "                return x, y\n",
        "            else:\n",
        "                raise ValueError('Unsupported dimension')\n",
        "\n",
        "        self._shape = tuple(shape)\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        dim = len(self._shape)\n",
        "\n",
        "        if transform_function == 'rotate_and_flip':\n",
        "            if shape[-2] != shape[-1]:\n",
        "                raise ValueError(\n",
        "                    'Patch shape must be square when using `rotate_and_flip`; '\n",
        "                    f'Received shape: {shape}')\n",
        "            self._transform_function = lambda x, y: rotate_and_flip(x, y, dim)\n",
        "        elif callable(transform_function):\n",
        "            self._transform_function = transform_function\n",
        "        elif transform_function is None:\n",
        "            self._transform_function = lambda x, y: (x, y)\n",
        "        else:\n",
        "            raise ValueError('Invalid transform function')\n",
        "\n",
        "        self._intensity_threshold = intensity_threshold\n",
        "\n",
        "        if not 0 <= area_ratio_threshold <= 1:\n",
        "            raise ValueError('\"area_ratio_threshold\" must be between 0 and 1')\n",
        "        self._area_threshold = area_ratio_threshold * np.prod(shape)\n",
        "\n",
        "        self._scale_factor = normalize_tuple(scale_factor, dim, 'scale_factor')\n",
        "        if any(not isinstance(f, int) or f == 0 for f in self._scale_factor):\n",
        "            raise ValueError('\"scale_factor\" must be nonzero integer')\n",
        "\n",
        "    class _Sequence(tf.keras.utils.Sequence):\n",
        "        def _scale(self, shape):\n",
        "            return tuple(\n",
        "                s * f if f > 0 else s // -f\n",
        "                for s, f in zip(shape, self._scale_factor))\n",
        "\n",
        "        def __init__(self,\n",
        "                     x,\n",
        "                     y,\n",
        "                     batch_size,\n",
        "                     shape,\n",
        "                     transform_function,\n",
        "                     intensity_threshold,\n",
        "                     area_threshold,\n",
        "                     scale_factor):\n",
        "            self._batch_size = batch_size\n",
        "            self._transform_function = transform_function\n",
        "            self._intensity_threshold = intensity_threshold\n",
        "            self._area_threshold = area_threshold\n",
        "            self._scale_factor = scale_factor\n",
        "\n",
        "            for s, f, in zip(shape, self._scale_factor):\n",
        "                if f < 0 and s % -f != 0:\n",
        "                    raise ValueError(\n",
        "                        'When downsampling, all elements in `shape` must be '\n",
        "                        'divisible by the scale factor; '\n",
        "                        f'Received shape: {shape}, '\n",
        "                        f'scale factor: {self._scale_factor}')\n",
        "\n",
        "            self._x, self._y = [\n",
        "                list(m) if isinstance(m, (list, tuple)) else [m]\n",
        "                for m in [x, y]]\n",
        "            self._x = np.moveaxis(self._x,0,-1)\n",
        "            self._y = np.moveaxis(self._y,0,-1)\n",
        "            if len(self._x) != len(self._y):\n",
        "                raise ValueError(\n",
        "                    'Different number of images are given: '\n",
        "                    f'{len(self._x)} vs. {len(self._y)}')\n",
        "\n",
        "            if len({m.dtype for m in self._x}) != 1:\n",
        "                raise ValueError('All source images must be the same type')\n",
        "            if len({m.dtype for m in self._y}) != 1:\n",
        "                raise ValueError('All target images must be the same type')\n",
        "            for i in range(len(self._x)):\n",
        "                if len(self._x[i].shape) == len(shape):\n",
        "                    self._x[i] = self._x[i][..., np.newaxis]\n",
        "\n",
        "                if len(self._y[i].shape) == len(shape):\n",
        "                    self._y[i] = self._y[i][..., np.newaxis]\n",
        "\n",
        "                if len(self._x[i].shape) != len(shape) + 1:\n",
        "                    raise ValueError(f'Source image must be {len(shape)}D')\n",
        "\n",
        "                if len(self._y[i].shape) != len(shape) + 1:\n",
        "                    raise ValueError(f'Target image must be {len(shape)}D')\n",
        "                if self._x[i].shape[:-1] < shape:\n",
        "                    raise ValueError(\n",
        "                        'Source image must be larger than the patch size')\n",
        "\n",
        "                expected_y_image_size = self._scale(self._x[i].shape[:-1])\n",
        "                if self._y[i].shape[:-1] != expected_y_image_size:\n",
        "                    raise ValueError('Invalid target image size: '\n",
        "                                     f'expected {expected_y_image_size}, '\n",
        "                                     f'but received {self._y[i].shape[:-1]}')\n",
        "\n",
        "            if len({m.shape[-1] for m in self._x}) != 1:\n",
        "                raise ValueError(\n",
        "                    'All source images must have the same number of channels')\n",
        "            if len({m.shape[-1] for m in self._y}) != 1:\n",
        "                raise ValueError(\n",
        "                    'All target images must have the same number of channels')\n",
        "            self._batch_x = np.zeros(\n",
        "                (batch_size, *shape, self._x[0].shape[-1]),\n",
        "                dtype=self._x[0].dtype)\n",
        "            self._batch_y = np.zeros(\n",
        "                (batch_size, *self._scale(shape),self._y[0].shape[-1]),\n",
        "                dtype=self._y[0].dtype)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self._x) // self._batch_size # return a dummy value\n",
        "\n",
        "        def __next__(self):\n",
        "            return self.__getitem__(0)\n",
        "\n",
        "        def __getitem__(self, _):\n",
        "            for i in range(self._batch_size):\n",
        "                for _ in range(139):\n",
        "                    j = np.random.randint(0, len(self._x))\n",
        "\n",
        "                    tl = [np.random.randint(0, a - b + 1)\n",
        "                          for a, b in zip(\n",
        "                              self._x[j].shape, self._batch_x.shape[1:])]\n",
        "\n",
        "                    x = np.copy(self._x[j][tuple(\n",
        "                        [slice(a, a + b) for a, b in zip(\n",
        "                            tl, self._batch_x.shape[1:])])])\n",
        "                    y = np.copy(self._y[j][tuple(\n",
        "                        [slice(a, a + b) for a, b in zip(\n",
        "                            self._scale(tl), self._batch_y.shape[1:])])])\n",
        "\n",
        "                    if (self._intensity_threshold <= 0.0 or\n",
        "                            np.count_nonzero(y > self._intensity_threshold)\n",
        "                            >= self._area_threshold):\n",
        "                        break\n",
        "                else:\n",
        "                    import warnings\n",
        "                    warnings.warn(\n",
        "                        'Failed to sample a valid patch',\n",
        "                        RuntimeWarning,\n",
        "                        stacklevel=3)\n",
        "\n",
        "\n",
        "                self._batch_x[i], self._batch_y[i] = \\\n",
        "                    self._transform_function(x, y)\n",
        "            return self._batch_x, self._batch_y\n",
        "\n",
        "    def flow(self, x, y):\n",
        "        '''\n",
        "        Returns a `keras.utils.Sequence` object which generates batches\n",
        "        infinitely. It can be used as an input generator for\n",
        "        `keras.models.Model.fit_generator()`.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: array_like or list of array_like\n",
        "            Source image(s).\n",
        "        y: array_like or list of array_like\n",
        "            Target image(s).\n",
        "        Returns\n",
        "        -------\n",
        "        keras.utils.Sequence\n",
        "            `keras.utils.Sequence` object which generates tuples of source and\n",
        "            target image patches.\n",
        "        '''\n",
        "        return self._Sequence(x,\n",
        "                              y,\n",
        "                              self._batch_size,\n",
        "                              self._shape,\n",
        "                              self._transform_function,\n",
        "                              self._intensity_threshold,\n",
        "                              self._area_threshold,\n",
        "                              self._scale_factor)\n",
        "\n",
        "\n",
        "def expandLastDim(data):\n",
        "    return np.expand_dims(data, -1)\n",
        "\n",
        "def create_patches(img, patch_shape, slide):\n",
        "    # returns stack of patches and number of patches\n",
        "    patch_img = skimage.util.view_as_windows(img, (patch_shape,patch_shape), step=patch_shape-slide)\n",
        "    patch = patch_img.reshape(patch_img.shape[0]*patch_img.shape[1],patch_shape,patch_shape) # more time efficient\n",
        "    return patch\n",
        "\n",
        "def patchify(input, patch_shape, slide):\n",
        "    # getting number of input images\n",
        "    len_to_allocate = int(np.shape(input)[0]*((slide-np.shape(input)[1]) / (slide-patch_shape))**2)\n",
        "    data = np.zeros((len_to_allocate,patch_shape,patch_shape))\n",
        "    count = 0\n",
        "    for i in range(np.shape(input)[0]):\n",
        "      A = create_patches(input[i], patch_shape, slide)\n",
        "      data[count:count+len(A),:,:] = A[:,:,:]\n",
        "      count = count + len(A)\n",
        "    print(\"      [PATCHIFYING COMPLETED] output shape, slide: \",np.shape(data),slide,\"; number of images: \", np.shape(input)[0], \", number of patches: \", np.shape(data)[0])\n",
        "    return data\n",
        "\n",
        "def load_data(path, expand=False, patch=None, slide='half', shuffle=False):\n",
        "    # Loading preprocessed image patches and adding 4th arbitrary dimension\n",
        "    b = np.load(path)\n",
        "    training_data = b['t']\n",
        "    val_data = b['v']\n",
        "\n",
        "    # patchify if patch size is passed\n",
        "    if patch != None:\n",
        "      if slide=='half':\n",
        "        slide = int(patch/2)\n",
        "      else: slide = 0\n",
        "      training_data = patchify(training_data, patch, slide)\n",
        "      val_data = patchify(val_data, patch, slide)\n",
        "\n",
        "    # if len(np.shape(training_data))>3:\n",
        "    #   # for 3D only, changing dimensions\n",
        "    #   training_data = np.transpose(training_data, (0, 3, 1, 2,4))\n",
        "    #   val_data = np.transpose(val_data, (0, 3, 1, 2,4))\n",
        "    if shuffle == True:\n",
        "      training_data = np.random.shuffle(training_data)\n",
        "      val_data = np.random.shuffle(val_data)\n",
        "    if expand == True:\n",
        "      training_data = expandLastDim(training_data)\n",
        "      val_data = expandLastDim(val_data)\n",
        "\n",
        "    return [training_data, val_data ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwuYzEFr9wCG"
      },
      "source": [
        "## **Execution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOjYIRskAxAw"
      },
      "outputs": [],
      "source": [
        "def config(patch_size, depth):\n",
        "  return {\n",
        "      'img_size': 512,\n",
        "      'learning_rate': 1e-4,\n",
        "      'batch_size': 16,\n",
        "      'alpha': 0.6,\n",
        "      'patch_size':patch_size,\n",
        "      'input_shape': [patch_size, patch_size],\n",
        "      'kern_size':3,\n",
        "      'kern_sigma': 0.99,\n",
        "      'n_depth': depth,\n",
        "      'first_depth': 32,\n",
        "      'dropout': 0,\n",
        "      'epoch':200,\n",
        "      'lr_decay_factor':0.99,\n",
        "      'lr_decay_patience':5,\n",
        "  }\n",
        "\n",
        "def genSSIML1_loss(alpha=0.84):\n",
        "  # this loss takes in 2D patch of dimension (n,n,3) and calculate loss on middle patch only\n",
        "  def SSIM_L1_loss(y_true, y_pred):\n",
        "    ssim_partial = 1-((mssim(y_true, y_pred)+1)*0.5)\n",
        "    mae_partial = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [y_true, y_pred]])\n",
        "\n",
        "    # # adding l2 regulizer\n",
        "    # l2_norms = [tf.nn.l2_loss(v) for v in model.trainable_variables]\n",
        "    # l2_norm = tf.reduce_sum(l2_norms)\n",
        "    # lambda_ = 0.1\n",
        "    return alpha*ssim_partial  + (1-alpha)*mae_partial\n",
        "  return SSIM_L1_loss\n",
        "\n",
        "def genSSIMVar_loss(alpha=0.84):\n",
        "  # this loss takes in 2D patch of dimension (n,n,3) and calculate loss on middle patch only\n",
        "  def SSIMVar_loss(y_true, y_pred):\n",
        "    SSIM = 1-((mssim(y_true, y_pred)+1)*0.5)\n",
        "    MAE = tf.keras.losses.mae(\n",
        "          *[tf.keras.backend.batch_flatten(y) for y in [mov_var(y_true), mov_var(y_pred)]])\n",
        "    return alpha * SSIM + (1-alpha) * MAE\n",
        "  return SSIMVar_loss\n",
        "\n",
        "# Loss function for evaluating adversarial loss\n",
        "adv_loss_fn = keras.losses.MeanSquaredError()\n",
        "\n",
        "def generator_loss_fn(fake):\n",
        "  fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
        "  return fake_loss\n",
        "\n",
        "\n",
        "# Define the loss function for the discriminators\n",
        "def discriminator_loss_fn(real, fake):\n",
        "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
        "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
        "    return (real_loss + fake_loss) * 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdHwVov_cgW5"
      },
      "outputs": [],
      "source": [
        "def normalize_between_zero_and_one(m):\n",
        "    max_val, min_val = m.max(), m.min()\n",
        "    diff = max_val - min_val\n",
        "    return (m - min_val) / diff if diff > 0 else np.zeros_like(m)\n",
        "\n",
        "\"\"\"\n",
        "## Create a callback that periodically saves generated images\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
        "    # changed to computing SSIM\n",
        "    def __init__(self, val_set, img_size = 512, patch_shape=128, slide=64):\n",
        "        # input: val_set = [x_val, y_val]\n",
        "        print(\"init: \", np.shape(val_set[0]))\n",
        "        self.x_val = val_set[0]\n",
        "        self.y_val = val_set[1]\n",
        "        self.img_size = img_size\n",
        "        self.patch_shape = patch_shape\n",
        "        self.slide = slide\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\"\")\n",
        "        # computing metrics of each patch\n",
        "        val_ssim = []\n",
        "        val_psnr = []\n",
        "        val_mse = []\n",
        "        # for i in range(len(self.x_val)):\n",
        "        # curr = np.expand_dims(self.x_val[i],-1)\n",
        "        # self.model.gen_G.summary()\n",
        "        prediction = self.model.gen_G(self.x_val).numpy()\n",
        "        for i in range(len(self.x_val)):\n",
        "          val_ssim.append(ssim(tf.convert_to_tensor([self.y_val[i,...],],dtype=tf.float32) ,tf.convert_to_tensor([prediction[i,...],],dtype=tf.float32)))\n",
        "          val_psnr.append(psnr(tf.convert_to_tensor([self.y_val[i,...],],dtype=tf.float32) ,tf.convert_to_tensor([prediction[i,...],],dtype=tf.float32)))\n",
        "          val_mse.append(mse(self.y_val[i,...],[prediction[i,...],]))\n",
        "        average_val_ssim = np.average(val_ssim)\n",
        "        average_val_psnr = np.average(val_psnr)\n",
        "        average_val_mse = np.average(val_mse)\n",
        "\n",
        "        # prediction = self.model.gen_G(self.x_val[:number]).numpy()\n",
        "        # x_merge = np.expand_dims(merge_patches(self.x_val[:number,...,0], self.img_size , self.patch_shape, self.slide), axis=-1)\n",
        "        # p_merge = np.expand_dims(merge_patches(prediction[:number,...,0], self.img_size , self.patch_shape, self.slide), axis=-1)\n",
        "        # y_merge = np.expand_dims(merge_patches(self.y_val[:number,...,0], self.img_size , self.patch_shape, self.slide), axis=-1)\n",
        "        # val_merged_ssim = ssim(tf.convert_to_tensor([y_merge,],dtype=tf.float32) ,tf.convert_to_tensor([p_merge,],dtype=tf.float32)).numpy()\n",
        "        # val_merged_psnr = psnr(tf.convert_to_tensor([y_merge,],dtype=tf.float32) ,tf.convert_to_tensor([p_merge,],dtype=tf.float32)).numpy()\n",
        "        # va_merged_mse = mse(y_merge,x_merge)\n",
        "        print(\"Epoch: \"+str(epoch)+\"| VALIDATION | average_val_ssim: \"+str(average_val_ssim)+\" | average_val_psnr: \"+str(average_val_psnr) +\" | average_val_mse: \"+str(average_val_mse))\n",
        "        result = [self.x_val[0,...,0], prediction[0,...,0],self.y_val[0,...,0]]\n",
        "        result = [normalize_between_zero_and_one(m) for m in result]\n",
        "        # plot and saving merged images\n",
        "        _, ax = plt.subplots(1, 3, figsize=(30,30))\n",
        "        ax[0].imshow(result[0])\n",
        "        ax[1].imshow(result[1])\n",
        "        ax[2].imshow(result[2])\n",
        "        ax[0].set_title(\"Input image\")\n",
        "        ax[1].set_title(\"Translated image\")\n",
        "        ax[2].set_title(\"GT image\")\n",
        "        ax[0].axis(\"off\")\n",
        "        ax[1].axis(\"off\")\n",
        "        ax[2].axis(\"off\")\n",
        "        plt.show()\n",
        "        plt.savefig(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkjPmj4iGXnN",
        "outputId": "afe2ef7e-1010-4811-cc29-9c079d5d5256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      [PATCHIFYING COMPLETED] output shape, slide:  (952, 256, 256) 0 ; number of images:  238 , number of patches:  952\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (136, 256, 256) 0 ; number of images:  34 , number of patches:  136\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (952, 256, 256) 0 ; number of images:  238 , number of patches:  952\n",
            "      [PATCHIFYING COMPLETED] output shape, slide:  (136, 256, 256) 0 ; number of images:  34 , number of patches:  136\n",
            "(952, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "#################################################################################\n",
        "''' Data was Normalized in Local Prep'''\n",
        "\n",
        "model_name = 'CycleGAN_original_keras_0108'\n",
        "main_path = MAIN_DIR\n",
        "\n",
        "if not os.path.exists(os.path.join(main_path,model_name)): os.mkdir(os.path.join(main_path,model_name))\n",
        "model_save_path = os.path.join(main_path,model_name)\n",
        "os.chdir(MAIN_DIR)\n",
        "(X,X_val) = load_data(MAIN_DIR+'/npz/Cervix_all_data_original.npz', expand=False, patch=PATCH_SIZE, slide=None, shuffle=False)\n",
        "(Y,Y_val) = load_data(MAIN_DIR+'/npz/Cervix_all_data_labels_original.npz', expand=False, patch=PATCH_SIZE, slide=None, shuffle=False)\n",
        "training_data = np.copy(X)\n",
        "training_data_labels = np.copy(Y)\n",
        "# shuffling training set, data and labels separatedly\n",
        "np.random.shuffle(X)\n",
        "np.random.shuffle(Y)\n",
        "print(np.shape(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46cENkpHY0dY"
      },
      "source": [
        "## **Train GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50TC96yAg-1l"
      },
      "outputs": [],
      "source": [
        "cycle_gan_model = CycleGan(\n",
        "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKwNwkQHf5Rx"
      },
      "outputs": [],
      "source": [
        "# cycle_gan_model = keras.models.load_model('./model_checkpoints/cyclegan_full',compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEthiP2-ZTAA"
      },
      "outputs": [],
      "source": [
        "# # Create cycle gan model\n",
        "# wandb.config = {\n",
        "#   \"learning_rate\": 1e-4,\n",
        "#   \"epochs\": 10,\n",
        "#   \"batch_size\": 16,\n",
        "#   \"loss_gen\": generator_loss_fn,\n",
        "#   \"loss_disc\": discriminator_loss_fn,\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "cycle_gan_model.compile(\n",
        "    gen_G_optimizer=keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5),\n",
        "    gen_F_optimizer=keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5),\n",
        "    disc_X_optimizer=keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5),\n",
        "    disc_Y_optimizer=keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5),\n",
        "    gen_loss_fn=generator_loss_fn,\n",
        "    disc_loss_fn=discriminator_loss_fn,\n",
        "    metrics = [{'psnr': psnr, 'ssim': ssim}]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6qEfcUe_VXk"
      },
      "outputs": [],
      "source": [
        "config_train = {\n",
        "      'img_size': 512,\n",
        "      'learning_rate': 1e-4,\n",
        "      'batch_size': 4,\n",
        "      'patch_size':PATCH_SIZE,\n",
        "      'input_shape': [PATCH_SIZE, PATCH_SIZE],\n",
        "      'epoch':100,\n",
        "      'lr_decay_factor':0.99,\n",
        "      'lr_decay_patience':5,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXgTEEPlVtfm",
        "outputId": "c732eda4-a39c-49be-b550-0584bad42466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init:  (10, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "data_gen = DataGenerator(\n",
        "    config_train['input_shape'],\n",
        "    config_train['batch_size'],\n",
        "    # transform_function=None)\n",
        "    transform_function='rotate_and_flip')\n",
        "tdata = data_gen.flow(*list(zip([X[:20],Y[:20]])))\n",
        "\n",
        "# Callbacks\n",
        "plotter = GANMonitor([expandLastDim(X_val[:10]), expandLastDim(Y_val[:10])], img_size = 512, patch_shape=PATCH_SIZE, slide=0)\n",
        "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"test\",\n",
        "    monitor='G_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    save_freq='epoch',)\n",
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='G_loss',verbose=True,factor=0.9,min_delta=0,patience=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqnqE4EPiGXK"
      },
      "outputs": [],
      "source": [
        "# Here we will train the model for just one epoch as each epoch takes around\n",
        "# 7 minutes on a single P100 backed machine.\n",
        "cycle_gan_model.fit(\n",
        "    x=tdata,\n",
        "    epochs=50,\n",
        "    callbacks=[plotter, model_checkpoint_callback, lr_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaIaBhlaUFJn"
      },
      "outputs": [],
      "source": [
        "def normalize(img):\n",
        "  # Normalizing images between 0 and 1 and preserving distribution\n",
        "  img_norm = (img - np.amin(img))/( np.amax(img)- np.amin(img))\n",
        "  return img_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8Ger7P9Kfv2"
      },
      "outputs": [],
      "source": [
        "x_val = expandLastDim((X_val))\n",
        "print(np.shape(x_val))\n",
        "y_val = expandLastDim((Y_val))\n",
        "img_size = 512\n",
        "patch_shape = PATCH_SIZE\n",
        "slide = 0\n",
        "start = 80\n",
        "end = start+ int((img_size-slide)/(patch_shape-slide))**2\n",
        "\n",
        "prediction = cycle_gan_model.gen_F(x_val[start:end]).numpy()\n",
        "x_merge = np.expand_dims(normalize(merge_patches(x_val[start:end,...,0], img_size , patch_shape, slide)), axis=-1)\n",
        "print(np.shape(x_merge))\n",
        "p_merge = np.expand_dims(normalize(merge_patches(prediction[0:end-start,...,0], img_size , patch_shape, slide)), axis=-1)\n",
        "y_merge = np.expand_dims(normalize(merge_patches(y_val[start:end,...,0], img_size , patch_shape, slide)), axis=-1)\n",
        "val_merged_ssim = ssim(tf.convert_to_tensor([y_merge,],dtype=tf.float32) ,tf.convert_to_tensor([p_merge,],dtype=tf.float32)).numpy()\n",
        "val_merged_psnr = psnr(tf.convert_to_tensor([y_merge,],dtype=tf.float32) ,tf.convert_to_tensor([p_merge,],dtype=tf.float32)).numpy()\n",
        "va_merged_mse = mse(y_merge,x_merge)\n",
        "print(\"| VALIDATION | val_merged_ssim: \"+str(val_merged_ssim)+\" | val_merged_psnr: \"+str(val_merged_psnr)+\" | va_merged_mse: \"+str(va_merged_mse))\n",
        "\n",
        "# plot and saving merged images\n",
        "_, ax = plt.subplots(1, 3, figsize=(30,30))\n",
        "ax[0].imshow(x_merge[...,0])\n",
        "ax[1].imshow(p_merge[...,0])\n",
        "ax[2].imshow(y_merge[...,0])\n",
        "ax[0].set_title(\"Input image\")\n",
        "ax[1].set_title(\"Translated image\")\n",
        "ax[2].set_title(\"GT image\")\n",
        "ax[0].axis(\"off\")\n",
        "ax[1].axis(\"off\")\n",
        "ax[2].axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mN9wWRxflQh"
      },
      "outputs": [],
      "source": [
        "print(np.shape(prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chKExEg3e-nA"
      },
      "outputs": [],
      "source": [
        "print(np.mean(p_merge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsFXjcF4bd4l"
      },
      "outputs": [],
      "source": [
        "cycle_gan_model.save('./model_checkpoints/cyclegan_full')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHR59yn4-WyX"
      },
      "source": [
        "##**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV_-UyAdMs1x"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def plot(img, label, hist=False, size=(10,10)):\n",
        "  plt.figure(figsize = size)\n",
        "  if hist==True:\n",
        "    plt.hist(img.flatten(), bins=120)\n",
        "  else: plt.imshow(img,cmap=\"gray\")\n",
        "  plt.title(label)\n",
        "\n",
        "def normalize_percentile(img, pmin=0.1, pmax=99.9, clip = True):\n",
        "  eps=1e-20 # avoid zero division\n",
        "  mi = np.percentile(img,pmin,axis=None,keepdims=True)\n",
        "  # mi = np.amin(img)\n",
        "  # print(\"mi\",mi)\n",
        "  ma = np.percentile(img,pmax,axis=None,keepdims=True)\n",
        "  if clip == True: return np.clip((img - mi) / ( ma - mi + eps ), 0, 1)\n",
        "  return (img - mi) / ( ma - mi + eps )\n",
        "\n",
        "# def ssim(y_true, y_pred):\n",
        "#     '''\n",
        "#     Computes the structural similarity index between two images. Note that the\n",
        "#     maximum signal value is assumed to be 1.\n",
        "#     '''\n",
        "\n",
        "#     return ssim2(y_true, y_pred,1,k2=0.05)\n",
        "# our SSIM loss\n",
        "from tensorflow.image import ssim as ssim2\n",
        "def ssim_our(y_true, y_pred):\n",
        "  return ssim2(y_true, y_pred,1,k2=0.05)\n",
        "\n",
        "def _get_gaussian_kernel(dim, size, sigma):\n",
        "    k = size // 2\n",
        "    normal = tfp.distributions.Normal(0.0, sigma)\n",
        "    p = normal.prob(tf.range(-k, size - k, dtype=tf.float32))\n",
        "\n",
        "    indices = [chr(i) for i in range(105, 105 + dim)]\n",
        "    eq = ','.join(indices) + '->' + ''.join(indices)\n",
        "    kernel = tf.einsum(eq, *([p] * dim))\n",
        "    kernel /= tf.reduce_sum(kernel)\n",
        "    kernel = kernel[..., tf.newaxis, tf.newaxis]\n",
        "\n",
        "    return kernel\n",
        "\n",
        "# RCAN ssim\n",
        "import keras.backend as K\n",
        "def ssim_rcan(y_true, y_pred):\n",
        "    '''\n",
        "    Computes the structural similarity index between two images. Note that the\n",
        "    maximum signal value is assumed to be 1.\n",
        "    References\n",
        "    ----------\n",
        "    Image Quality Assessment: From Error Visibility to Structural Similarity\n",
        "    https://doi.org/10.1109/TIP.2003.819861\n",
        "    '''\n",
        "\n",
        "    c1 = 0.01 ** 2\n",
        "    c2 = 0.03 ** 2\n",
        "\n",
        "    dim = K.ndim(y_pred) - 2\n",
        "    if dim not in (2, 3):\n",
        "        raise NotImplementedError(f'{dim}D SSIM is not suported')\n",
        "\n",
        "    num_channels = K.int_shape(y_pred)[-1]\n",
        "\n",
        "    kernel = _get_gaussian_kernel(dim, 11, 1.5)\n",
        "    conv = K.conv2d if dim == 2 else K.conv3d\n",
        "\n",
        "    def average(x):\n",
        "        # channel-wise weighted average using the Gaussian kernel\n",
        "        return tf.concat(\n",
        "            [conv(y, kernel) for y in tf.split(x, num_channels, axis=-1)],\n",
        "            axis=-1)\n",
        "\n",
        "    ux = average(y_true)\n",
        "    uy = average(y_pred)\n",
        "\n",
        "    a = ux * uy\n",
        "    b = K.square(ux) + K.square(uy)\n",
        "    c = average(y_true * y_pred)\n",
        "    d = average(K.square(y_true) + K.square(y_pred))\n",
        "\n",
        "    lum = (2 * a + c1) / (b + c1)\n",
        "    cs = (2 * (c - a) + c2) / (d - b + c2)\n",
        "\n",
        "    return K.mean(K.batch_flatten(lum * cs), axis=-1)\n",
        "\n",
        "def predictAndPlot(model,input,val, start, end, isPlot=True, merge=True, normalize=False, clip=False):\n",
        "  pred = model.gen_G(input[start:end])\n",
        "  print(\"shape of predicted image: \", np.shape(input))\n",
        "  if len(np.shape(input)) > 3:\n",
        "    if merge:\n",
        "      result = [merge_patches(input[start:end,:,:,0],512,256,128), merge_patches(pred[start:end,:,:,0],512,256,128), merge_patches(val[start:end,:,:,0],512,256,128)]\n",
        "    else: result = [input[start:end,:,:,0], pred[start:end,:,:,0],val[start:end,:,:,0]]\n",
        "  else:\n",
        "    if merge:\n",
        "      result = [merge_patches(input[start:end,:,:],512,256,128), merge_patches(pred[start:end,:,:,0],512,256,128), merge_patches(val[start:end,:,:],512,256,128)]\n",
        "    else: result = [input[start:end,:,:], pred[start:end,:,:,0], val[start:end,:,:]]\n",
        "  if normalize: result = [normalize_percentile(m) for m in result]\n",
        "  if clip: result = [np.clip(255 * m, 0, 255).astype('uint8') for m in result]\n",
        "  res_img =  np.concatenate((np.concatenate((result[0],result[1]),axis=1),result[2]),axis=1)\n",
        "  plot(res_img, \"input, restored, gt\", size=(30,30))\n",
        "  # plot(result[0],'input 25x')\n",
        "  # plot(result[1],'restored')\n",
        "  # plot(result[2],'gt 40x')\n",
        "  if isPlot:\n",
        "    plot(result[0],'hist input 25x',hist=True)\n",
        "    plot(result[1],'hist restored',hist=True)\n",
        "    plot(result[2],'hist gt 40x',hist=True)\n",
        "  return result\n",
        "\n",
        "def evalSSIM(result):\n",
        "  result1 = [np.expand_dims(m,-1) for m in result]\n",
        "  raw = tf.convert_to_tensor(np.expand_dims(result1[0],0),dtype=np.float32)\n",
        "  rest = tf.convert_to_tensor(np.expand_dims(result1[1],0),dtype=np.float32)\n",
        "  gt = tf.convert_to_tensor(np.expand_dims(result1[2],0),dtype=np.float32)\n",
        "  # Metric evaluation\n",
        "  print('raw vs gt=======================')\n",
        "  print(\"Our SSIM between raw and ground truth: \",ssim_our(gt,raw).numpy())\n",
        "  print(\"RCAN paper SSIM between raw and ground truth: \",ssim_rcan(gt, raw).numpy())\n",
        "  print('predicted vs gt=======================')\n",
        "  print(\"Our SSIM between predicted and ground truth: \",ssim_our(gt, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and ground truth: \",ssim_rcan(gt,rest).numpy())\n",
        "  print('predicted vs raw=======================')\n",
        "  print(\"Our SSIM between predicted and raw: \",ssim_our(raw, rest).numpy())\n",
        "  print(\"RCAN paper SSIM between predicted and raw: \",ssim_rcan(raw, rest).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_NvaC7PMuRC"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability as tfp\n",
        "import math\n",
        "result = predictAndPlot(cycle_gan_model,X_val, Y_val, 0, 9, isPlot=True, merge=True, normalize=False, clip=False)\n",
        "evalSSIM(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
